{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.10.4', ray_version='1.13.0', ray_commit='e4ce38d001dbbe09cd21c497fedd03d692b2be3e', address_info={'node_ip_address': '192.168.70.36', 'raylet_ip_address': '192.168.70.36', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-08-23_13-25-39_887980_42620/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-08-23_13-25-39_887980_42620/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2022-08-23_13-25-39_887980_42620', 'metrics_export_port': 40814, 'gcs_address': '192.168.70.36:50541', 'address': '192.168.70.36:50541', 'node_id': '16887aa97c726d06dc839a86f96224c4ed0bd78f5be01b1d93eff0fd'})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zet4/.virtualenvs/rayrl/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2.5\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame(data=[10, 5])\n",
    "data.std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5355339059327378"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow(12.5, exp=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_workers': 2,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'create_env_on_driver': False,\n",
       " 'rollout_fragment_length': 200,\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'gamma': 0.99,\n",
       " 'lr': 5e-05,\n",
       " 'train_batch_size': 4000,\n",
       " 'model': {'_use_default_native_models': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  'fcnet_hiddens': [256, 256],\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'post_fcnet_hiddens': [],\n",
       "  'post_fcnet_activation': 'relu',\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': False,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': False,\n",
       "  'lstm_use_prev_reward': False,\n",
       "  '_time_major': False,\n",
       "  'use_attention': False,\n",
       "  'attention_num_transformer_units': 1,\n",
       "  'attention_dim': 64,\n",
       "  'attention_num_heads': 1,\n",
       "  'attention_head_dim': 32,\n",
       "  'attention_memory_inference': 50,\n",
       "  'attention_memory_training': 50,\n",
       "  'attention_position_wise_mlp_dim': 32,\n",
       "  'attention_init_gru_gate_bias': 2.0,\n",
       "  'attention_use_n_prev_actions': 0,\n",
       "  'attention_use_n_prev_rewards': 0,\n",
       "  'framestack': True,\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'lstm_use_prev_action_reward': -1},\n",
       " 'optimizer': {},\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'env': None,\n",
       " 'observation_space': None,\n",
       " 'action_space': None,\n",
       " 'env_config': {},\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'env_task_fn': None,\n",
       " 'render_env': False,\n",
       " 'record_env': False,\n",
       " 'clip_rewards': None,\n",
       " 'normalize_actions': True,\n",
       " 'clip_actions': False,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'log_level': 'WARN',\n",
       " 'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       " 'ignore_worker_failures': False,\n",
       " 'recreate_failed_workers': False,\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'framework': 'tf',\n",
       " 'eager_tracing': False,\n",
       " 'eager_max_retraces': 20,\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'StochasticSampling'},\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_duration': 10,\n",
       " 'evaluation_duration_unit': 'episodes',\n",
       " 'evaluation_parallel_to_training': False,\n",
       " 'in_evaluation': False,\n",
       " 'evaluation_config': {},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'custom_eval_function': None,\n",
       " 'always_attach_evaluation_results': False,\n",
       " 'keep_per_episode_custom_metrics': False,\n",
       " 'sample_async': False,\n",
       " 'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'compress_observations': False,\n",
       " 'metrics_episode_collection_timeout_s': 180,\n",
       " 'metrics_num_episodes_for_smoothing': 100,\n",
       " 'min_time_s_per_reporting': None,\n",
       " 'min_train_timesteps_per_reporting': None,\n",
       " 'min_sample_timesteps_per_reporting': None,\n",
       " 'seed': None,\n",
       " 'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_gpus': 0,\n",
       " '_fake_gpus': False,\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'num_cpus_for_driver': 1,\n",
       " 'placement_strategy': 'PACK',\n",
       " 'input': 'sampler',\n",
       " 'input_config': {},\n",
       " 'actions_in_input_normalized': False,\n",
       " 'input_evaluation': ['is', 'wis'],\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_config': {},\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'multiagent': {'policies': {},\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_map_cache': None,\n",
       "  'policy_mapping_fn': None,\n",
       "  'policies_to_train': None,\n",
       "  'observation_fn': None,\n",
       "  'replay_mode': 'independent',\n",
       "  'count_steps_by': 'env_steps'},\n",
       " 'logger_config': None,\n",
       " '_tf_policy_handles_more_than_one_loss': False,\n",
       " '_disable_preprocessor_api': False,\n",
       " '_disable_action_flattening': False,\n",
       " '_disable_execution_plan_api': True,\n",
       " 'disable_env_checking': False,\n",
       " 'simple_optimizer': -1,\n",
       " 'monitor': -1,\n",
       " 'evaluation_num_episodes': -1,\n",
       " 'metrics_smoothing_episodes': -1,\n",
       " 'timesteps_per_iteration': 0,\n",
       " 'min_iter_time_s': -1,\n",
       " 'collect_metrics_timeout': -1,\n",
       " 'use_critic': True,\n",
       " 'use_gae': True,\n",
       " 'lambda': 1.0,\n",
       " 'kl_coeff': 0.2,\n",
       " 'sgd_minibatch_size': 128,\n",
       " 'shuffle_sequences': True,\n",
       " 'num_sgd_iter': 30,\n",
       " 'lr_schedule': None,\n",
       " 'vf_loss_coeff': 1.0,\n",
       " 'entropy_coeff': 0.0,\n",
       " 'entropy_coeff_schedule': None,\n",
       " 'clip_param': 0.3,\n",
       " 'vf_clip_param': 10.0,\n",
       " 'grad_clip': None,\n",
       " 'kl_target': 0.01}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-17 22:22:49,292\tWARNING deprecation.py:46 -- DeprecationWarning: `evaluation_num_episodes` has been deprecated. Use ``evaluation_duration` and `evaluation_duration_unit=episodes`` instead. This will raise an error in the future!\n",
      "2022-08-17 22:23:10,514\tWARNING deprecation.py:46 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-08-17 22:23:12,645\tINFO trainable.py:159 -- Trainable.setup took 23.359 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-08-17 22:23:12,646\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'bipedalwalker-v2/PPO/PPO_BipedalWalker-v2_0c4df_00000_0_2022-08-17_21-59-05.tune_metadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/zet4/Documents/finrl/basic_training.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zet4/Documents/finrl/basic_training.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mppo\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mppo\u001b[39;00m \u001b[39mimport\u001b[39;00m PPOTrainer\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zet4/Documents/finrl/basic_training.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m agent \u001b[39m=\u001b[39m PPOTrainer(config\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39menv\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mBipedalWalker-v2\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zet4/Documents/finrl/basic_training.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                             \u001b[39m'\u001b[39m\u001b[39mevaluation_interval\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m2\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zet4/Documents/finrl/basic_training.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                             \u001b[39m'\u001b[39m\u001b[39mevaluation_num_episodes\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m20\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zet4/Documents/finrl/basic_training.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                             }\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zet4/Documents/finrl/basic_training.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                     )\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/zet4/Documents/finrl/basic_training.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m agent\u001b[39m.\u001b[39;49mrestore(\u001b[39m'\u001b[39;49m\u001b[39mbipedalwalker-v2/PPO/PPO_BipedalWalker-v2_0c4df_00000_0_2022-08-17_21-59-05\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/.virtualenvs/rayrl/lib/python3.10/site-packages/ray/tune/trainable.py:569\u001b[0m, in \u001b[0;36mTrainable.restore\u001b[0;34m(self, checkpoint_path, checkpoint_node_ip)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[39mif\u001b[39;00m checkpoint:\n\u001b[1;32m    567\u001b[0m         checkpoint\u001b[39m.\u001b[39mto_directory(checkpoint_path)\n\u001b[0;32m--> 569\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(checkpoint_path \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.tune_metadata\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    570\u001b[0m     metadata \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m    571\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experiment_id \u001b[39m=\u001b[39m metadata[\u001b[39m\"\u001b[39m\u001b[39mexperiment_id\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bipedalwalker-v2/PPO/PPO_BipedalWalker-v2_0c4df_00000_0_2022-08-17_21-59-05.tune_metadata'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents.ppo.ppo import PPOTrainer\n",
    "\n",
    "agent = PPOTrainer(config={'env': 'BipedalWalker-v2',\n",
    "                            'evaluation_interval': 2,\n",
    "                            'evaluation_num_episodes': 20\n",
    "                            }\n",
    "                    )\n",
    "agent.restore('bipedalwalker-v2/PPO/PPO_BipedalWalker-v2_0c4df_00000_0_2022-08-17_21-59-05')\n",
    "# fail because of missing checkpoint_freq=2 during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zet4/.virtualenvs/rayrl/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=196922)\u001b[0m 2022-08-17 21:59:17,386\tINFO trainer.py:2332 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=196922)\u001b[0m 2022-08-17 21:59:17,388\tWARNING deprecation.py:46 -- DeprecationWarning: `evaluation_num_episodes` has been deprecated. Use ``evaluation_duration` and `evaluation_duration_unit=episodes`` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=196922)\u001b[0m 2022-08-17 21:59:17,388\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=196922)\u001b[0m 2022-08-17 21:59:17,388\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=196922)\u001b[0m 2022-08-17 21:59:39,014\tWARNING deprecation.py:46 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=196922)\u001b[0m 2022-08-17 21:59:42,637\tINFO trainable.py:159 -- Trainable.setup took 25.253 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=196922)\u001b[0m 2022-08-17 21:59:42,638\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-17 21:59:42 (running for 00:00:37.13)<br>Memory usage on this node: 9.2/15.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/5.36 GiB heap, 0.0/2.68 GiB objects<br>Result logdir: /home/zet4/Documents/finrl/bipedalwalker-v2/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                 </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v2_0c4df_00000</td><td>RUNNING </td><td>192.168.70.36:196922</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-17 21:59:47 (running for 00:00:42.20)<br>Memory usage on this node: 9.2/15.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/5.36 GiB heap, 0.0/2.68 GiB objects<br>Result logdir: /home/zet4/Documents/finrl/bipedalwalker-v2/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                 </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v2_0c4df_00000</td><td>RUNNING </td><td>192.168.70.36:196922</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-17 21:59:52 (running for 00:00:47.21)<br>Memory usage on this node: 9.2/15.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/5.36 GiB heap, 0.0/2.68 GiB objects<br>Result logdir: /home/zet4/Documents/finrl/bipedalwalker-v2/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                 </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v2_0c4df_00000</td><td>RUNNING </td><td>192.168.70.36:196922</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=196922)\u001b[0m 2022-08-17 21:59:56,512\tWARNING deprecation.py:46 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-17 21:59:57 (running for 00:00:52.23)<br>Memory usage on this node: 9.2/15.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/5.36 GiB heap, 0.0/2.68 GiB objects<br>Result logdir: /home/zet4/Documents/finrl/bipedalwalker-v2/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                 </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v2_0c4df_00000</td><td>RUNNING </td><td>192.168.70.36:196922</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-17 22:00:02 (running for 00:00:57.25)<br>Memory usage on this node: 9.3/15.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/5.36 GiB heap, 0.0/2.68 GiB objects<br>Result logdir: /home/zet4/Documents/finrl/bipedalwalker-v2/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                 </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v2_0c4df_00000</td><td>RUNNING </td><td>192.168.70.36:196922</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v2_0c4df_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-17_22-00-06\n",
      "  done: false\n",
      "  episode_len_mean: 585.8333333333334\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -99.79971977803237\n",
      "  episode_reward_mean: -106.50469161057941\n",
      "  episode_reward_min: -117.33686914746463\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 6\n",
      "  experiment_id: d96927190530496e871c8a5f3d9bd8ba\n",
      "  hostname: zet4\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.661588668823242\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016176734119653702\n",
      "          model: {}\n",
      "          policy_loss: -0.022964784875512123\n",
      "          total_loss: 5.220287799835205\n",
      "          vf_explained_var: 0.02869277447462082\n",
      "          vf_loss: 5.240016937255859\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_env_steps_sampled: 4000\n",
      "    num_env_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.70.36\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.65294117647059\n",
      "    ram_util_percent: 61.46176470588236\n",
      "  pid: 196922\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.5710142841939626\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.6611593476180613\n",
      "    mean_inference_ms: 3.9305938356581596\n",
      "    mean_raw_obs_processing_ms: 0.43669865764063154\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 585.8333333333334\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -99.79971977803237\n",
      "    episode_reward_mean: -106.50469161057941\n",
      "    episode_reward_min: -117.33686914746463\n",
      "    episodes_this_iter: 6\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 99\n",
      "      - 56\n",
      "      - 111\n",
      "      - 49\n",
      "      episode_reward:\n",
      "      - -99.79971977803237\n",
      "      - -102.16187234156027\n",
      "      - -107.19164499667909\n",
      "      - -107.19628921007924\n",
      "      - -105.34175418966078\n",
      "      - -117.33686914746463\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.5710142841939626\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 1.6611593476180613\n",
      "      mean_inference_ms: 3.9305938356581596\n",
      "      mean_raw_obs_processing_ms: 0.43669865764063154\n",
      "  time_since_restore: 23.386966943740845\n",
      "  time_this_iter_s: 23.386966943740845\n",
      "  time_total_s: 23.386966943740845\n",
      "  timers:\n",
      "    learn_throughput: 417.87\n",
      "    learn_time_ms: 9572.364\n",
      "    load_throughput: 13640013.008\n",
      "    load_time_ms: 0.293\n",
      "    training_iteration_time_ms: 23376.458\n",
      "    update_time_ms: 4.047\n",
      "  timestamp: 1660766406\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 0c4df_00000\n",
      "  warmup_time: 25.32832670211792\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-17 22:00:11 (running for 00:01:05.62)<br>Memory usage on this node: 9.2/15.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/5.36 GiB heap, 0.0/2.68 GiB objects<br>Result logdir: /home/zet4/Documents/finrl/bipedalwalker-v2/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v2_0c4df_00000</td><td>RUNNING </td><td>192.168.70.36:196922</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          23.387</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-106.505</td><td style=\"text-align: right;\">            -99.7997</td><td style=\"text-align: right;\">            -117.337</td><td style=\"text-align: right;\">           585.833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-17 22:00:16 (running for 00:01:10.62)<br>Memory usage on this node: 9.1/15.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/5.36 GiB heap, 0.0/2.68 GiB objects<br>Result logdir: /home/zet4/Documents/finrl/bipedalwalker-v2/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v2_0c4df_00000</td><td>RUNNING </td><td>192.168.70.36:196922</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          23.387</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-106.505</td><td style=\"text-align: right;\">            -99.7997</td><td style=\"text-align: right;\">            -117.337</td><td style=\"text-align: right;\">           585.833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-17 22:00:21 (running for 00:01:15.65)<br>Memory usage on this node: 9.0/15.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/5.36 GiB heap, 0.0/2.68 GiB objects<br>Result logdir: /home/zet4/Documents/finrl/bipedalwalker-v2/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v2_0c4df_00000</td><td>RUNNING </td><td>192.168.70.36:196922</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          23.387</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-106.505</td><td style=\"text-align: right;\">            -99.7997</td><td style=\"text-align: right;\">            -117.337</td><td style=\"text-align: right;\">           585.833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-17 22:00:26 (running for 00:01:20.66)<br>Memory usage on this node: 9.0/15.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/5.36 GiB heap, 0.0/2.68 GiB objects<br>Result logdir: /home/zet4/Documents/finrl/bipedalwalker-v2/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v2_0c4df_00000</td><td>RUNNING </td><td>192.168.70.36:196922</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          23.387</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-106.505</td><td style=\"text-align: right;\">            -99.7997</td><td style=\"text-align: right;\">            -117.337</td><td style=\"text-align: right;\">           585.833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-17 22:00:31 (running for 00:01:25.67)<br>Memory usage on this node: 9.0/15.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/5.36 GiB heap, 0.0/2.68 GiB objects<br>Result logdir: /home/zet4/Documents/finrl/bipedalwalker-v2/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v2_0c4df_00000</td><td>RUNNING </td><td>192.168.70.36:196922</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          23.387</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-106.505</td><td style=\"text-align: right;\">            -99.7997</td><td style=\"text-align: right;\">            -117.337</td><td style=\"text-align: right;\">           585.833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-17 22:00:36 (running for 00:01:30.67)<br>Memory usage on this node: 9.0/15.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/5.36 GiB heap, 0.0/2.68 GiB objects<br>Result logdir: /home/zet4/Documents/finrl/bipedalwalker-v2/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v2_0c4df_00000</td><td>RUNNING </td><td>192.168.70.36:196922</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          23.387</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-106.505</td><td style=\"text-align: right;\">            -99.7997</td><td style=\"text-align: right;\">            -117.337</td><td style=\"text-align: right;\">           585.833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-17 22:00:41 (running for 00:01:35.68)<br>Memory usage on this node: 9.0/15.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/5.36 GiB heap, 0.0/2.68 GiB objects<br>Result logdir: /home/zet4/Documents/finrl/bipedalwalker-v2/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v2_0c4df_00000</td><td>RUNNING </td><td>192.168.70.36:196922</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          23.387</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-106.505</td><td style=\"text-align: right;\">            -99.7997</td><td style=\"text-align: right;\">            -117.337</td><td style=\"text-align: right;\">           585.833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-17 22:00:46 (running for 00:01:40.69)<br>Memory usage on this node: 8.9/15.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/5.36 GiB heap, 0.0/2.68 GiB objects<br>Result logdir: /home/zet4/Documents/finrl/bipedalwalker-v2/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v2_0c4df_00000</td><td>RUNNING </td><td>192.168.70.36:196922</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          23.387</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-106.505</td><td style=\"text-align: right;\">            -99.7997</td><td style=\"text-align: right;\">            -117.337</td><td style=\"text-align: right;\">           585.833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-17 22:00:51 (running for 00:01:45.70)<br>Memory usage on this node: 8.9/15.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/5.36 GiB heap, 0.0/2.68 GiB objects<br>Result logdir: /home/zet4/Documents/finrl/bipedalwalker-v2/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v2_0c4df_00000</td><td>RUNNING </td><td>192.168.70.36:196922</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          23.387</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-106.505</td><td style=\"text-align: right;\">            -99.7997</td><td style=\"text-align: right;\">            -117.337</td><td style=\"text-align: right;\">           585.833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-17 22:00:56 (running for 00:01:50.70)<br>Memory usage on this node: 8.9/15.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/5.36 GiB heap, 0.0/2.68 GiB objects<br>Result logdir: /home/zet4/Documents/finrl/bipedalwalker-v2/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v2_0c4df_00000</td><td>RUNNING </td><td>192.168.70.36:196922</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          23.387</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-106.505</td><td style=\"text-align: right;\">            -99.7997</td><td style=\"text-align: right;\">            -117.337</td><td style=\"text-align: right;\">           585.833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v2_0c4df_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-17_22-00-56\n",
      "  done: false\n",
      "  episode_len_mean: 453.26666666666665\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -98.68418989205982\n",
      "  episode_reward_mean: -114.75445722197163\n",
      "  episode_reward_min: -181.9194918475101\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 15\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 729.45\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -95.50415235896092\n",
      "    episode_reward_mean: -110.62808270715689\n",
      "    episode_reward_min: -169.0044376938958\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 122\n",
      "      - 1600\n",
      "      - 86\n",
      "      - 1600\n",
      "      - 47\n",
      "      - 60\n",
      "      - 170\n",
      "      - 114\n",
      "      - 849\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 82\n",
      "      - 1600\n",
      "      - 80\n",
      "      - 1600\n",
      "      - 56\n",
      "      - 48\n",
      "      - 75\n",
      "      episode_reward:\n",
      "      - -113.18393443733822\n",
      "      - -107.32103914912781\n",
      "      - -108.19201902241508\n",
      "      - -110.36150741065212\n",
      "      - -103.88057249537856\n",
      "      - -106.05195751705705\n",
      "      - -114.58008724902074\n",
      "      - -111.75938023644748\n",
      "      - -110.00821137658507\n",
      "      - -110.16576654355912\n",
      "      - -169.0044376938958\n",
      "      - -95.50415235896092\n",
      "      - -114.6358436230346\n",
      "      - -103.23455162221069\n",
      "      - -107.13539265935927\n",
      "      - -100.6316845265627\n",
      "      - -106.52713380198377\n",
      "      - -108.5682614371311\n",
      "      - -112.35461062577677\n",
      "      - -99.46111035664131\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.23374462715969907\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.6019267911695306\n",
      "      mean_inference_ms: 1.2309196144036025\n",
      "      mean_raw_obs_processing_ms: 0.14466267076561592\n",
      "    timesteps_this_iter: 14589\n",
      "  experiment_id: d96927190530496e871c8a5f3d9bd8ba\n",
      "  hostname: zet4\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.792549133300781\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01686614751815796\n",
      "          model: {}\n",
      "          policy_loss: -0.027404403313994408\n",
      "          total_loss: 3.5537993907928467\n",
      "          vf_explained_var: 0.01469061616808176\n",
      "          vf_loss: 3.5778305530548096\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_env_steps_sampled: 8000\n",
      "    num_env_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.70.36\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 8000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.64999999999999\n",
      "    ram_util_percent: 59.98472222222222\n",
      "  pid: 196922\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4936700301261421\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.4242375623252106\n",
      "    mean_inference_ms: 3.284603011664004\n",
      "    mean_raw_obs_processing_ms: 0.3700135702986375\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 453.26666666666665\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -98.68418989205982\n",
      "    episode_reward_mean: -114.75445722197163\n",
      "    episode_reward_min: -181.9194918475101\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 99\n",
      "      - 56\n",
      "      - 111\n",
      "      - 49\n",
      "      - 1600\n",
      "      - 74\n",
      "      - 80\n",
      "      - 75\n",
      "      - 61\n",
      "      - 1118\n",
      "      - 130\n",
      "      - 83\n",
      "      - 63\n",
      "      episode_reward:\n",
      "      - -99.79971977803237\n",
      "      - -102.16187234156027\n",
      "      - -107.19164499667909\n",
      "      - -107.19628921007924\n",
      "      - -105.34175418966078\n",
      "      - -117.33686914746463\n",
      "      - -125.52484165832105\n",
      "      - -103.17798563046381\n",
      "      - -114.6847573180062\n",
      "      - -98.68418989205982\n",
      "      - -116.62866787917788\n",
      "      - -181.9194918475101\n",
      "      - -104.62345659262128\n",
      "      - -118.73149733226313\n",
      "      - -118.31382051567485\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.4936700301261421\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 1.4242375623252106\n",
      "      mean_inference_ms: 3.284603011664004\n",
      "      mean_raw_obs_processing_ms: 0.3700135702986375\n",
      "  time_since_restore: 73.9947578907013\n",
      "  time_this_iter_s: 50.60779094696045\n",
      "  time_total_s: 73.9947578907013\n",
      "  timers:\n",
      "    learn_throughput: 375.397\n",
      "    learn_time_ms: 10655.389\n",
      "    load_throughput: 11945330.011\n",
      "    load_time_ms: 0.335\n",
      "    training_iteration_time_ms: 20789.597\n",
      "    update_time_ms: 3.403\n",
      "  timestamp: 1660766456\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 0c4df_00000\n",
      "  warmup_time: 25.32832670211792\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-17 22:01:01 (running for 00:01:56.27)<br>Memory usage on this node: 8.9/15.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/5.36 GiB heap, 0.0/2.68 GiB objects<br>Result logdir: /home/zet4/Documents/finrl/bipedalwalker-v2/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v2_0c4df_00000</td><td>RUNNING </td><td>192.168.70.36:196922</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         73.9948</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-114.754</td><td style=\"text-align: right;\">            -98.6842</td><td style=\"text-align: right;\">            -181.919</td><td style=\"text-align: right;\">           453.267</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-17 22:01:06 (running for 00:02:01.28)<br>Memory usage on this node: 8.9/15.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/5.36 GiB heap, 0.0/2.68 GiB objects<br>Result logdir: /home/zet4/Documents/finrl/bipedalwalker-v2/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v2_0c4df_00000</td><td>RUNNING </td><td>192.168.70.36:196922</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         73.9948</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-114.754</td><td style=\"text-align: right;\">            -98.6842</td><td style=\"text-align: right;\">            -181.919</td><td style=\"text-align: right;\">           453.267</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_BipedalWalker-v2_0c4df_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-17_22-01-08\n",
      "  done: false\n",
      "  episode_len_mean: 479.1363636363636\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -96.24853127404363\n",
      "  episode_reward_mean: -112.69410780521979\n",
      "  episode_reward_min: -181.9194918475101\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 22\n",
      "  experiment_id: d96927190530496e871c8a5f3d9bd8ba\n",
      "  hostname: zet4\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 5.767385959625244\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019041338935494423\n",
      "          model: {}\n",
      "          policy_loss: -0.02940998785197735\n",
      "          total_loss: 2.328505277633667\n",
      "          vf_explained_var: 0.05189727991819382\n",
      "          vf_loss: 2.354106903076172\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_env_steps_sampled: 12000\n",
      "    num_env_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.70.36\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_sampled_this_iter: 4000\n",
      "  num_env_steps_trained: 12000\n",
      "  num_env_steps_trained_this_iter: 4000\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.71176470588236\n",
      "    ram_util_percent: 59.26470588235294\n",
      "  pid: 196922\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.4607294684977432\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 1.3201577330206797\n",
      "    mean_inference_ms: 3.0095809891866185\n",
      "    mean_raw_obs_processing_ms: 0.3438123227653408\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 479.1363636363636\n",
      "    episode_media: {}\n",
      "    episode_reward_max: -96.24853127404363\n",
      "    episode_reward_mean: -112.69410780521979\n",
      "    episode_reward_min: -181.9194918475101\n",
      "    episodes_this_iter: 7\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1600\n",
      "      - 1600\n",
      "      - 99\n",
      "      - 56\n",
      "      - 111\n",
      "      - 49\n",
      "      - 1600\n",
      "      - 74\n",
      "      - 80\n",
      "      - 75\n",
      "      - 61\n",
      "      - 1118\n",
      "      - 130\n",
      "      - 83\n",
      "      - 63\n",
      "      - 1600\n",
      "      - 83\n",
      "      - 1600\n",
      "      - 78\n",
      "      - 94\n",
      "      - 158\n",
      "      - 129\n",
      "      episode_reward:\n",
      "      - -99.79971977803237\n",
      "      - -102.16187234156027\n",
      "      - -107.19164499667909\n",
      "      - -107.19628921007924\n",
      "      - -105.34175418966078\n",
      "      - -117.33686914746463\n",
      "      - -125.52484165832105\n",
      "      - -103.17798563046381\n",
      "      - -114.6847573180062\n",
      "      - -98.68418989205982\n",
      "      - -116.62866787917788\n",
      "      - -181.9194918475101\n",
      "      - -104.62345659262128\n",
      "      - -118.73149733226313\n",
      "      - -118.31382051567485\n",
      "      - -96.24853127404363\n",
      "      - -117.53079932875559\n",
      "      - -112.14061056967638\n",
      "      - -104.88701473716274\n",
      "      - -99.10012732435204\n",
      "      - -105.7427436617886\n",
      "      - -122.30368648948149\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.4607294684977432\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 1.3201577330206797\n",
      "      mean_inference_ms: 3.0095809891866185\n",
      "      mean_raw_obs_processing_ms: 0.3438123227653408\n",
      "  time_since_restore: 85.8257999420166\n",
      "  time_this_iter_s: 11.831042051315308\n",
      "  time_total_s: 85.8257999420166\n",
      "  timers:\n",
      "    learn_throughput: 438.982\n",
      "    learn_time_ms: 9111.989\n",
      "    load_throughput: 13461259.16\n",
      "    load_time_ms: 0.297\n",
      "    training_iteration_time_ms: 17800.446\n",
      "    update_time_ms: 3.178\n",
      "  timestamp: 1660766468\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 0c4df_00000\n",
      "  warmup_time: 25.32832670211792\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-17 22:01:13 (running for 00:02:08.15)<br>Memory usage on this node: 8.9/15.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/5.36 GiB heap, 0.0/2.68 GiB objects<br>Result logdir: /home/zet4/Documents/finrl/bipedalwalker-v2/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v2_0c4df_00000</td><td>RUNNING </td><td>192.168.70.36:196922</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         85.8258</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-112.694</td><td style=\"text-align: right;\">            -96.2485</td><td style=\"text-align: right;\">            -181.919</td><td style=\"text-align: right;\">           479.136</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-17 22:01:18 (running for 00:02:13.16)<br>Memory usage on this node: 9.0/15.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/5.36 GiB heap, 0.0/2.68 GiB objects<br>Result logdir: /home/zet4/Documents/finrl/bipedalwalker-v2/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v2_0c4df_00000</td><td>RUNNING </td><td>192.168.70.36:196922</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         85.8258</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-112.694</td><td style=\"text-align: right;\">            -96.2485</td><td style=\"text-align: right;\">            -181.919</td><td style=\"text-align: right;\">           479.136</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-17 22:01:21,751\tWARNING tune.py:682 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-17 22:01:23 (running for 00:02:18.18)<br>Memory usage on this node: 9.0/15.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/5.36 GiB heap, 0.0/2.68 GiB objects<br>Result logdir: /home/zet4/Documents/finrl/bipedalwalker-v2/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v2_0c4df_00000</td><td>RUNNING </td><td>192.168.70.36:196922</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         85.8258</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-112.694</td><td style=\"text-align: right;\">            -96.2485</td><td style=\"text-align: right;\">            -181.919</td><td style=\"text-align: right;\">           479.136</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-17 22:01:23 (running for 00:02:18.19)<br>Memory usage on this node: 9.0/15.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/0 GPUs, 0.0/5.36 GiB heap, 0.0/2.68 GiB objects<br>Result logdir: /home/zet4/Documents/finrl/bipedalwalker-v2/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_BipedalWalker-v2_0c4df_00000</td><td>RUNNING </td><td>192.168.70.36:196922</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         85.8258</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-112.694</td><td style=\"text-align: right;\">            -96.2485</td><td style=\"text-align: right;\">            -181.919</td><td style=\"text-align: right;\">           479.136</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tune\n",
    "from ray import tune\n",
    "\n",
    "tune.run('PPO',\n",
    "        config={'env': 'CartPole-v1',\n",
    "                'evaluation_interval': 2,\n",
    "                'evaluation_num_episodes': 20,\n",
    "                'num_gpus': 0\n",
    "               },\n",
    "               local_dir='bipedalwalker-v2',\n",
    "               checkpoint_freq=2\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 14:57:28,683\tINFO worker.py:973 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'num_workers': 2,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'create_env_on_driver': False,\n",
       " 'rollout_fragment_length': 200,\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'gamma': 0.99,\n",
       " 'lr': 5e-05,\n",
       " 'train_batch_size': 4000,\n",
       " 'model': {'_use_default_native_models': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  'fcnet_hiddens': [256, 256],\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'post_fcnet_hiddens': [],\n",
       "  'post_fcnet_activation': 'relu',\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': False,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': False,\n",
       "  'lstm_use_prev_reward': False,\n",
       "  '_time_major': False,\n",
       "  'use_attention': False,\n",
       "  'attention_num_transformer_units': 1,\n",
       "  'attention_dim': 64,\n",
       "  'attention_num_heads': 1,\n",
       "  'attention_head_dim': 32,\n",
       "  'attention_memory_inference': 50,\n",
       "  'attention_memory_training': 50,\n",
       "  'attention_position_wise_mlp_dim': 32,\n",
       "  'attention_init_gru_gate_bias': 2.0,\n",
       "  'attention_use_n_prev_actions': 0,\n",
       "  'attention_use_n_prev_rewards': 0,\n",
       "  'framestack': True,\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'lstm_use_prev_action_reward': -1},\n",
       " 'optimizer': {},\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'env': None,\n",
       " 'observation_space': None,\n",
       " 'action_space': None,\n",
       " 'env_config': {},\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'env_task_fn': None,\n",
       " 'render_env': False,\n",
       " 'record_env': False,\n",
       " 'clip_rewards': None,\n",
       " 'normalize_actions': True,\n",
       " 'clip_actions': False,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'log_level': 'WARN',\n",
       " 'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       " 'ignore_worker_failures': False,\n",
       " 'recreate_failed_workers': False,\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'framework': 'tf',\n",
       " 'eager_tracing': False,\n",
       " 'eager_max_retraces': 20,\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'StochasticSampling'},\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_duration': 10,\n",
       " 'evaluation_duration_unit': 'episodes',\n",
       " 'evaluation_parallel_to_training': False,\n",
       " 'in_evaluation': False,\n",
       " 'evaluation_config': {},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'custom_eval_function': None,\n",
       " 'always_attach_evaluation_results': False,\n",
       " 'keep_per_episode_custom_metrics': False,\n",
       " 'sample_async': False,\n",
       " 'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'compress_observations': False,\n",
       " 'metrics_episode_collection_timeout_s': 180,\n",
       " 'metrics_num_episodes_for_smoothing': 100,\n",
       " 'min_time_s_per_reporting': None,\n",
       " 'min_train_timesteps_per_reporting': None,\n",
       " 'min_sample_timesteps_per_reporting': None,\n",
       " 'seed': None,\n",
       " 'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_gpus': 0,\n",
       " '_fake_gpus': False,\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'num_cpus_for_driver': 1,\n",
       " 'placement_strategy': 'PACK',\n",
       " 'input': 'sampler',\n",
       " 'input_config': {},\n",
       " 'actions_in_input_normalized': False,\n",
       " 'input_evaluation': ['is', 'wis'],\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_config': {},\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'multiagent': {'policies': {},\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_map_cache': None,\n",
       "  'policy_mapping_fn': None,\n",
       "  'policies_to_train': None,\n",
       "  'observation_fn': None,\n",
       "  'replay_mode': 'independent',\n",
       "  'count_steps_by': 'env_steps'},\n",
       " 'logger_config': None,\n",
       " '_tf_policy_handles_more_than_one_loss': False,\n",
       " '_disable_preprocessor_api': False,\n",
       " '_disable_action_flattening': False,\n",
       " '_disable_execution_plan_api': True,\n",
       " 'disable_env_checking': False,\n",
       " 'simple_optimizer': -1,\n",
       " 'monitor': -1,\n",
       " 'evaluation_num_episodes': -1,\n",
       " 'metrics_smoothing_episodes': -1,\n",
       " 'timesteps_per_iteration': 0,\n",
       " 'min_iter_time_s': -1,\n",
       " 'collect_metrics_timeout': -1,\n",
       " 'use_critic': True,\n",
       " 'use_gae': True,\n",
       " 'lambda': 1.0,\n",
       " 'kl_coeff': 0.2,\n",
       " 'sgd_minibatch_size': 128,\n",
       " 'shuffle_sequences': True,\n",
       " 'num_sgd_iter': 30,\n",
       " 'lr_schedule': None,\n",
       " 'vf_loss_coeff': 1.0,\n",
       " 'entropy_coeff': 0.0,\n",
       " 'entropy_coeff_schedule': None,\n",
       " 'clip_param': 0.3,\n",
       " 'vf_clip_param': 10.0,\n",
       " 'grad_clip': None,\n",
       " 'kl_target': 0.01}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(ignore_reinit_error=True)\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 13:45:11,463\tINFO worker.py:973 -- Calling ray.init() again after it has already been called.\n",
      "2022-08-15 13:45:18,123\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 4000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-45-28\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -875.0002263354793\n",
      "episode_reward_mean: -1252.985064301061\n",
      "episode_reward_min: -1800.6808078530426\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 20\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4228661060333252\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007937874644994736\n",
      "        model: {}\n",
      "        policy_loss: 0.005782076623290777\n",
      "        total_loss: 9.957915306091309\n",
      "        vf_explained_var: 0.00039263040525838733\n",
      "        vf_loss: 9.950546264648438\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "iterations_since_restore: 1\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 4000\n",
      "num_agent_steps_trained: 4000\n",
      "num_env_steps_sampled: 4000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 4000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.90666666666666\n",
      "  ram_util_percent: 63.13333333333335\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19262373909238756\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1450634336388132\n",
      "  mean_inference_ms: 0.9818450715118159\n",
      "  mean_raw_obs_processing_ms: 0.12091188781173845\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -875.0002263354793\n",
      "  episode_reward_mean: -1252.985064301061\n",
      "  episode_reward_min: -1800.6808078530426\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -988.1746980649485\n",
      "    - -927.4714507962681\n",
      "    - -1640.041365285305\n",
      "    - -887.8334620461424\n",
      "    - -1305.2165629284677\n",
      "    - -1589.9937479320918\n",
      "    - -1770.1283934899848\n",
      "    - -1004.5335373490047\n",
      "    - -1629.1742759167228\n",
      "    - -1075.6178252451514\n",
      "    - -1632.3497299475582\n",
      "    - -1327.8167557443962\n",
      "    - -1603.3345958170523\n",
      "    - -968.8433989952928\n",
      "    - -966.8347585460551\n",
      "    - -908.4835497652085\n",
      "    - -1800.6808078530426\n",
      "    - -941.9805922274545\n",
      "    - -875.0002263354793\n",
      "    - -1216.1915517355937\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19262373909238756\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1450634336388132\n",
      "    mean_inference_ms: 0.9818450715118159\n",
      "    mean_raw_obs_processing_ms: 0.12091188781173845\n",
      "time_since_restore: 10.474964141845703\n",
      "time_this_iter_s: 10.474964141845703\n",
      "time_total_s: 10.474964141845703\n",
      "timers:\n",
      "  learn_throughput: 868.972\n",
      "  learn_time_ms: 4603.142\n",
      "  load_throughput: 19949127.229\n",
      "  load_time_ms: 0.201\n",
      "  training_iteration_time_ms: 10468.896\n",
      "  update_time_ms: 2.27\n",
      "timestamp: 1660563928\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "Iteration  0\n",
      "checkpoint saved at  /home/zet4/ray_results/PPOTrainer_Pendulum-v0_2022-08-15_13-45-117w98j8mg/checkpoint_000001/checkpoint-1\n",
      "agent_timesteps_total: 8000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_trained: 8000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-45-38\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -875.0002263354793\n",
      "episode_reward_mean: -1191.842203204486\n",
      "episode_reward_min: -1800.6808078530426\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 40\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4067219495773315\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0041179358959198\n",
      "        model: {}\n",
      "        policy_loss: 0.008356698788702488\n",
      "        total_loss: 9.909406661987305\n",
      "        vf_explained_var: -0.0059492820873856544\n",
      "        vf_loss: 9.900225639343262\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_trained: 8000\n",
      "iterations_since_restore: 2\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 8000\n",
      "num_agent_steps_trained: 8000\n",
      "num_env_steps_sampled: 8000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 8000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.64285714285715\n",
      "  ram_util_percent: 63.14285714285716\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19101766802582873\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1451194168621121\n",
      "  mean_inference_ms: 0.9717224902545458\n",
      "  mean_raw_obs_processing_ms: 0.12053152571714554\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -875.0002263354793\n",
      "  episode_reward_mean: -1191.842203204486\n",
      "  episode_reward_min: -1800.6808078530426\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -988.1746980649485\n",
      "    - -927.4714507962681\n",
      "    - -1640.041365285305\n",
      "    - -887.8334620461424\n",
      "    - -1305.2165629284677\n",
      "    - -1589.9937479320918\n",
      "    - -1770.1283934899848\n",
      "    - -1004.5335373490047\n",
      "    - -1629.1742759167228\n",
      "    - -1075.6178252451514\n",
      "    - -1632.3497299475582\n",
      "    - -1327.8167557443962\n",
      "    - -1603.3345958170523\n",
      "    - -968.8433989952928\n",
      "    - -966.8347585460551\n",
      "    - -908.4835497652085\n",
      "    - -1800.6808078530426\n",
      "    - -941.9805922274545\n",
      "    - -875.0002263354793\n",
      "    - -1216.1915517355937\n",
      "    - -1509.210098260178\n",
      "    - -927.6196895430224\n",
      "    - -1378.5518734603986\n",
      "    - -1271.6916065271719\n",
      "    - -1012.4307347823609\n",
      "    - -1129.344541547314\n",
      "    - -1150.0627265556216\n",
      "    - -995.0663359752381\n",
      "    - -1007.0105059913651\n",
      "    - -1516.3233694067505\n",
      "    - -1003.6192447209478\n",
      "    - -886.0594974164645\n",
      "    - -1089.558855605612\n",
      "    - -1121.9506496013892\n",
      "    - -887.3680300269515\n",
      "    - -1468.065431460236\n",
      "    - -996.1855627651288\n",
      "    - -1051.3240000395147\n",
      "    - -1281.0778237825289\n",
      "    - -931.4662646900164\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19101766802582873\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1451194168621121\n",
      "    mean_inference_ms: 0.9717224902545458\n",
      "    mean_raw_obs_processing_ms: 0.12053152571714554\n",
      "time_since_restore: 20.220848560333252\n",
      "time_this_iter_s: 9.745884418487549\n",
      "time_total_s: 20.220848560333252\n",
      "timers:\n",
      "  learn_throughput: 919.473\n",
      "  learn_time_ms: 4350.318\n",
      "  load_throughput: 19703130.945\n",
      "  load_time_ms: 0.203\n",
      "  training_iteration_time_ms: 10105.85\n",
      "  update_time_ms: 2.156\n",
      "timestamp: 1660563938\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 8000\n",
      "training_iteration: 2\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 12000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_trained: 12000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-45-48\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -781.5649561606865\n",
      "episode_reward_mean: -1168.7884754596173\n",
      "episode_reward_min: -1800.6808078530426\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 60\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.398646354675293\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00655212951824069\n",
      "        model: {}\n",
      "        policy_loss: 0.007788806688040495\n",
      "        total_loss: 9.943934440612793\n",
      "        vf_explained_var: -0.0028309498447924852\n",
      "        vf_loss: 9.935492515563965\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_trained: 12000\n",
      "iterations_since_restore: 3\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 12000\n",
      "num_agent_steps_trained: 12000\n",
      "num_env_steps_sampled: 12000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 12000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.47857142857142\n",
      "  ram_util_percent: 63.23571428571431\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19009583838016442\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14467202139910926\n",
      "  mean_inference_ms: 0.9636127419675176\n",
      "  mean_raw_obs_processing_ms: 0.11983428318022143\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -781.5649561606865\n",
      "  episode_reward_mean: -1168.7884754596173\n",
      "  episode_reward_min: -1800.6808078530426\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -988.1746980649485\n",
      "    - -927.4714507962681\n",
      "    - -1640.041365285305\n",
      "    - -887.8334620461424\n",
      "    - -1305.2165629284677\n",
      "    - -1589.9937479320918\n",
      "    - -1770.1283934899848\n",
      "    - -1004.5335373490047\n",
      "    - -1629.1742759167228\n",
      "    - -1075.6178252451514\n",
      "    - -1632.3497299475582\n",
      "    - -1327.8167557443962\n",
      "    - -1603.3345958170523\n",
      "    - -968.8433989952928\n",
      "    - -966.8347585460551\n",
      "    - -908.4835497652085\n",
      "    - -1800.6808078530426\n",
      "    - -941.9805922274545\n",
      "    - -875.0002263354793\n",
      "    - -1216.1915517355937\n",
      "    - -1509.210098260178\n",
      "    - -927.6196895430224\n",
      "    - -1378.5518734603986\n",
      "    - -1271.6916065271719\n",
      "    - -1012.4307347823609\n",
      "    - -1129.344541547314\n",
      "    - -1150.0627265556216\n",
      "    - -995.0663359752381\n",
      "    - -1007.0105059913651\n",
      "    - -1516.3233694067505\n",
      "    - -1003.6192447209478\n",
      "    - -886.0594974164645\n",
      "    - -1089.558855605612\n",
      "    - -1121.9506496013892\n",
      "    - -887.3680300269515\n",
      "    - -1468.065431460236\n",
      "    - -996.1855627651288\n",
      "    - -1051.3240000395147\n",
      "    - -1281.0778237825289\n",
      "    - -931.4662646900164\n",
      "    - -1028.6210707992627\n",
      "    - -975.9864665502549\n",
      "    - -1592.38576653991\n",
      "    - -1000.6521673122053\n",
      "    - -1398.369801742319\n",
      "    - -1178.4371022745015\n",
      "    - -892.7744160404642\n",
      "    - -1454.5683407783858\n",
      "    - -1238.654877792119\n",
      "    - -1562.2754071965314\n",
      "    - -1060.7485993326652\n",
      "    - -969.3698243870065\n",
      "    - -881.8351650855735\n",
      "    - -1002.3608813149405\n",
      "    - -1195.511052349149\n",
      "    - -798.1522938989187\n",
      "    - -1092.1526659103322\n",
      "    - -1277.638138018075\n",
      "    - -781.5649561606865\n",
      "    - -1071.5614059143381\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19009583838016442\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14467202139910926\n",
      "    mean_inference_ms: 0.9636127419675176\n",
      "    mean_raw_obs_processing_ms: 0.11983428318022143\n",
      "time_since_restore: 29.83970880508423\n",
      "time_this_iter_s: 9.618860244750977\n",
      "time_total_s: 29.83970880508423\n",
      "timers:\n",
      "  learn_throughput: 936.935\n",
      "  learn_time_ms: 4269.238\n",
      "  load_throughput: 19614827.747\n",
      "  load_time_ms: 0.204\n",
      "  training_iteration_time_ms: 9941.773\n",
      "  update_time_ms: 2.258\n",
      "timestamp: 1660563948\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 12000\n",
      "training_iteration: 3\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 16000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_trained: 16000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-45-58\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -775.9027688776691\n",
      "episode_reward_mean: -1162.1755007031963\n",
      "episode_reward_min: -1800.6808078530426\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 80\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3960121870040894\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0044008707627654076\n",
      "        model: {}\n",
      "        policy_loss: 0.007687479257583618\n",
      "        total_loss: 9.926170349121094\n",
      "        vf_explained_var: -0.0031091845594346523\n",
      "        vf_loss: 9.91804313659668\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_trained: 16000\n",
      "iterations_since_restore: 4\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 16000\n",
      "num_agent_steps_trained: 16000\n",
      "num_env_steps_sampled: 16000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 16000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.14\n",
      "  ram_util_percent: 63.206666666666685\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.18960074067520769\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14450398849964122\n",
      "  mean_inference_ms: 0.9598136447429371\n",
      "  mean_raw_obs_processing_ms: 0.11957626992750052\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -775.9027688776691\n",
      "  episode_reward_mean: -1162.1755007031963\n",
      "  episode_reward_min: -1800.6808078530426\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -988.1746980649485\n",
      "    - -927.4714507962681\n",
      "    - -1640.041365285305\n",
      "    - -887.8334620461424\n",
      "    - -1305.2165629284677\n",
      "    - -1589.9937479320918\n",
      "    - -1770.1283934899848\n",
      "    - -1004.5335373490047\n",
      "    - -1629.1742759167228\n",
      "    - -1075.6178252451514\n",
      "    - -1632.3497299475582\n",
      "    - -1327.8167557443962\n",
      "    - -1603.3345958170523\n",
      "    - -968.8433989952928\n",
      "    - -966.8347585460551\n",
      "    - -908.4835497652085\n",
      "    - -1800.6808078530426\n",
      "    - -941.9805922274545\n",
      "    - -875.0002263354793\n",
      "    - -1216.1915517355937\n",
      "    - -1509.210098260178\n",
      "    - -927.6196895430224\n",
      "    - -1378.5518734603986\n",
      "    - -1271.6916065271719\n",
      "    - -1012.4307347823609\n",
      "    - -1129.344541547314\n",
      "    - -1150.0627265556216\n",
      "    - -995.0663359752381\n",
      "    - -1007.0105059913651\n",
      "    - -1516.3233694067505\n",
      "    - -1003.6192447209478\n",
      "    - -886.0594974164645\n",
      "    - -1089.558855605612\n",
      "    - -1121.9506496013892\n",
      "    - -887.3680300269515\n",
      "    - -1468.065431460236\n",
      "    - -996.1855627651288\n",
      "    - -1051.3240000395147\n",
      "    - -1281.0778237825289\n",
      "    - -931.4662646900164\n",
      "    - -1028.6210707992627\n",
      "    - -975.9864665502549\n",
      "    - -1592.38576653991\n",
      "    - -1000.6521673122053\n",
      "    - -1398.369801742319\n",
      "    - -1178.4371022745015\n",
      "    - -892.7744160404642\n",
      "    - -1454.5683407783858\n",
      "    - -1238.654877792119\n",
      "    - -1562.2754071965314\n",
      "    - -1060.7485993326652\n",
      "    - -969.3698243870065\n",
      "    - -881.8351650855735\n",
      "    - -1002.3608813149405\n",
      "    - -1195.511052349149\n",
      "    - -798.1522938989187\n",
      "    - -1092.1526659103322\n",
      "    - -1277.638138018075\n",
      "    - -781.5649561606865\n",
      "    - -1071.5614059143381\n",
      "    - -1257.0996234323723\n",
      "    - -1356.4152462264594\n",
      "    - -1458.231145700803\n",
      "    - -1232.3692828749433\n",
      "    - -1308.9407583153409\n",
      "    - -885.2728525599703\n",
      "    - -908.1068279385499\n",
      "    - -1082.685311025416\n",
      "    - -1207.5501598101648\n",
      "    - -873.5698972899754\n",
      "    - -1309.2148092521413\n",
      "    - -1556.6381611329396\n",
      "    - -1292.8461833213112\n",
      "    - -1272.4199178276228\n",
      "    - -1156.6894460739945\n",
      "    - -1042.9956653438996\n",
      "    - -892.5649503250196\n",
      "    - -898.9378676933726\n",
      "    - -775.9027688776691\n",
      "    - -1078.2806536566754\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18960074067520769\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14450398849964122\n",
      "    mean_inference_ms: 0.9598136447429371\n",
      "    mean_raw_obs_processing_ms: 0.11957626992750052\n",
      "time_since_restore: 39.84825134277344\n",
      "time_this_iter_s: 10.008542537689209\n",
      "time_total_s: 39.84825134277344\n",
      "timers:\n",
      "  learn_throughput: 934.35\n",
      "  learn_time_ms: 4281.053\n",
      "  load_throughput: 18882629.15\n",
      "  load_time_ms: 0.212\n",
      "  training_iteration_time_ms: 9955.865\n",
      "  update_time_ms: 2.502\n",
      "timestamp: 1660563958\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 16000\n",
      "training_iteration: 4\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 20000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 20000\n",
      "  num_agent_steps_trained: 20000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_trained: 20000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-46-10\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -770.0259497670975\n",
      "episode_reward_mean: -1146.9824810894993\n",
      "episode_reward_min: -1800.6808078530426\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 100\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.05000000074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.356757402420044\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021195439621806145\n",
      "        model: {}\n",
      "        policy_loss: 0.003322852309793234\n",
      "        total_loss: 9.914142608642578\n",
      "        vf_explained_var: -0.00299033778719604\n",
      "        vf_loss: 9.909759521484375\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 20000\n",
      "  num_agent_steps_trained: 20000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_trained: 20000\n",
      "iterations_since_restore: 5\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 20000\n",
      "num_agent_steps_trained: 20000\n",
      "num_env_steps_sampled: 20000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 20000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 67.48235294117647\n",
      "  ram_util_percent: 63.464705882352945\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19110132906101834\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14547626274746445\n",
      "  mean_inference_ms: 0.9662325367638079\n",
      "  mean_raw_obs_processing_ms: 0.12030791592122025\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -770.0259497670975\n",
      "  episode_reward_mean: -1146.9824810894993\n",
      "  episode_reward_min: -1800.6808078530426\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -988.1746980649485\n",
      "    - -927.4714507962681\n",
      "    - -1640.041365285305\n",
      "    - -887.8334620461424\n",
      "    - -1305.2165629284677\n",
      "    - -1589.9937479320918\n",
      "    - -1770.1283934899848\n",
      "    - -1004.5335373490047\n",
      "    - -1629.1742759167228\n",
      "    - -1075.6178252451514\n",
      "    - -1632.3497299475582\n",
      "    - -1327.8167557443962\n",
      "    - -1603.3345958170523\n",
      "    - -968.8433989952928\n",
      "    - -966.8347585460551\n",
      "    - -908.4835497652085\n",
      "    - -1800.6808078530426\n",
      "    - -941.9805922274545\n",
      "    - -875.0002263354793\n",
      "    - -1216.1915517355937\n",
      "    - -1509.210098260178\n",
      "    - -927.6196895430224\n",
      "    - -1378.5518734603986\n",
      "    - -1271.6916065271719\n",
      "    - -1012.4307347823609\n",
      "    - -1129.344541547314\n",
      "    - -1150.0627265556216\n",
      "    - -995.0663359752381\n",
      "    - -1007.0105059913651\n",
      "    - -1516.3233694067505\n",
      "    - -1003.6192447209478\n",
      "    - -886.0594974164645\n",
      "    - -1089.558855605612\n",
      "    - -1121.9506496013892\n",
      "    - -887.3680300269515\n",
      "    - -1468.065431460236\n",
      "    - -996.1855627651288\n",
      "    - -1051.3240000395147\n",
      "    - -1281.0778237825289\n",
      "    - -931.4662646900164\n",
      "    - -1028.6210707992627\n",
      "    - -975.9864665502549\n",
      "    - -1592.38576653991\n",
      "    - -1000.6521673122053\n",
      "    - -1398.369801742319\n",
      "    - -1178.4371022745015\n",
      "    - -892.7744160404642\n",
      "    - -1454.5683407783858\n",
      "    - -1238.654877792119\n",
      "    - -1562.2754071965314\n",
      "    - -1060.7485993326652\n",
      "    - -969.3698243870065\n",
      "    - -881.8351650855735\n",
      "    - -1002.3608813149405\n",
      "    - -1195.511052349149\n",
      "    - -798.1522938989187\n",
      "    - -1092.1526659103322\n",
      "    - -1277.638138018075\n",
      "    - -781.5649561606865\n",
      "    - -1071.5614059143381\n",
      "    - -1257.0996234323723\n",
      "    - -1356.4152462264594\n",
      "    - -1458.231145700803\n",
      "    - -1232.3692828749433\n",
      "    - -1308.9407583153409\n",
      "    - -885.2728525599703\n",
      "    - -908.1068279385499\n",
      "    - -1082.685311025416\n",
      "    - -1207.5501598101648\n",
      "    - -873.5698972899754\n",
      "    - -1309.2148092521413\n",
      "    - -1556.6381611329396\n",
      "    - -1292.8461833213112\n",
      "    - -1272.4199178276228\n",
      "    - -1156.6894460739945\n",
      "    - -1042.9956653438996\n",
      "    - -892.5649503250196\n",
      "    - -898.9378676933726\n",
      "    - -775.9027688776691\n",
      "    - -1078.2806536566754\n",
      "    - -1495.3604172442854\n",
      "    - -865.5588183476433\n",
      "    - -922.7502880317732\n",
      "    - -999.8250414718867\n",
      "    - -988.0248784278043\n",
      "    - -1164.705840665387\n",
      "    - -1289.4105097373827\n",
      "    - -1184.0923439431308\n",
      "    - -1197.0117560756391\n",
      "    - -960.0258334277987\n",
      "    - -1277.9900991310735\n",
      "    - -770.0259497670975\n",
      "    - -894.905023910079\n",
      "    - -1056.555437384123\n",
      "    - -897.9332044513404\n",
      "    - -990.2668378462043\n",
      "    - -1446.8169691519672\n",
      "    - -999.0207574175389\n",
      "    - -1386.250266741606\n",
      "    - -937.6777795204221\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19110132906101834\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14547626274746445\n",
      "    mean_inference_ms: 0.9662325367638079\n",
      "    mean_raw_obs_processing_ms: 0.12030791592122025\n",
      "time_since_restore: 52.38098502159119\n",
      "time_this_iter_s: 12.532733678817749\n",
      "time_total_s: 52.38098502159119\n",
      "timers:\n",
      "  learn_throughput: 879.765\n",
      "  learn_time_ms: 4546.67\n",
      "  load_throughput: 17461715.237\n",
      "  load_time_ms: 0.229\n",
      "  training_iteration_time_ms: 10470.021\n",
      "  update_time_ms: 2.432\n",
      "timestamp: 1660563970\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 20000\n",
      "training_iteration: 5\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 24000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_trained: 24000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-46-22\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -770.0259497670975\n",
      "episode_reward_mean: -1119.968363462947\n",
      "episode_reward_min: -1598.1026156944956\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 120\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3690345287322998\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008804181590676308\n",
      "        model: {}\n",
      "        policy_loss: 0.006628849543631077\n",
      "        total_loss: 9.880777359008789\n",
      "        vf_explained_var: -0.006343070417642593\n",
      "        vf_loss: 9.87348747253418\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_trained: 24000\n",
      "iterations_since_restore: 6\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 24000\n",
      "num_agent_steps_trained: 24000\n",
      "num_env_steps_sampled: 24000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 24000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 63.22941176470589\n",
      "  ram_util_percent: 63.35294117647059\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1924567569365651\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14669294193326812\n",
      "  mean_inference_ms: 0.9702418962451257\n",
      "  mean_raw_obs_processing_ms: 0.12104524374165707\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -770.0259497670975\n",
      "  episode_reward_mean: -1119.968363462947\n",
      "  episode_reward_min: -1598.1026156944956\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1509.210098260178\n",
      "    - -927.6196895430224\n",
      "    - -1378.5518734603986\n",
      "    - -1271.6916065271719\n",
      "    - -1012.4307347823609\n",
      "    - -1129.344541547314\n",
      "    - -1150.0627265556216\n",
      "    - -995.0663359752381\n",
      "    - -1007.0105059913651\n",
      "    - -1516.3233694067505\n",
      "    - -1003.6192447209478\n",
      "    - -886.0594974164645\n",
      "    - -1089.558855605612\n",
      "    - -1121.9506496013892\n",
      "    - -887.3680300269515\n",
      "    - -1468.065431460236\n",
      "    - -996.1855627651288\n",
      "    - -1051.3240000395147\n",
      "    - -1281.0778237825289\n",
      "    - -931.4662646900164\n",
      "    - -1028.6210707992627\n",
      "    - -975.9864665502549\n",
      "    - -1592.38576653991\n",
      "    - -1000.6521673122053\n",
      "    - -1398.369801742319\n",
      "    - -1178.4371022745015\n",
      "    - -892.7744160404642\n",
      "    - -1454.5683407783858\n",
      "    - -1238.654877792119\n",
      "    - -1562.2754071965314\n",
      "    - -1060.7485993326652\n",
      "    - -969.3698243870065\n",
      "    - -881.8351650855735\n",
      "    - -1002.3608813149405\n",
      "    - -1195.511052349149\n",
      "    - -798.1522938989187\n",
      "    - -1092.1526659103322\n",
      "    - -1277.638138018075\n",
      "    - -781.5649561606865\n",
      "    - -1071.5614059143381\n",
      "    - -1257.0996234323723\n",
      "    - -1356.4152462264594\n",
      "    - -1458.231145700803\n",
      "    - -1232.3692828749433\n",
      "    - -1308.9407583153409\n",
      "    - -885.2728525599703\n",
      "    - -908.1068279385499\n",
      "    - -1082.685311025416\n",
      "    - -1207.5501598101648\n",
      "    - -873.5698972899754\n",
      "    - -1309.2148092521413\n",
      "    - -1556.6381611329396\n",
      "    - -1292.8461833213112\n",
      "    - -1272.4199178276228\n",
      "    - -1156.6894460739945\n",
      "    - -1042.9956653438996\n",
      "    - -892.5649503250196\n",
      "    - -898.9378676933726\n",
      "    - -775.9027688776691\n",
      "    - -1078.2806536566754\n",
      "    - -1495.3604172442854\n",
      "    - -865.5588183476433\n",
      "    - -922.7502880317732\n",
      "    - -999.8250414718867\n",
      "    - -988.0248784278043\n",
      "    - -1164.705840665387\n",
      "    - -1289.4105097373827\n",
      "    - -1184.0923439431308\n",
      "    - -1197.0117560756391\n",
      "    - -960.0258334277987\n",
      "    - -1277.9900991310735\n",
      "    - -770.0259497670975\n",
      "    - -894.905023910079\n",
      "    - -1056.555437384123\n",
      "    - -897.9332044513404\n",
      "    - -990.2668378462043\n",
      "    - -1446.8169691519672\n",
      "    - -999.0207574175389\n",
      "    - -1386.250266741606\n",
      "    - -937.6777795204221\n",
      "    - -1229.6068771427101\n",
      "    - -895.6088541598078\n",
      "    - -857.3892252685897\n",
      "    - -1192.056298724483\n",
      "    - -1035.2598650781515\n",
      "    - -1051.0931928095551\n",
      "    - -901.816036033148\n",
      "    - -1458.4045404615767\n",
      "    - -979.8327291529246\n",
      "    - -1302.1247125613982\n",
      "    - -971.6791654823678\n",
      "    - -1064.5645324861684\n",
      "    - -1214.7280518691261\n",
      "    - -885.0765330141511\n",
      "    - -1390.6111981195913\n",
      "    - -990.7559407628036\n",
      "    - -880.7702603279499\n",
      "    - -981.4116662316677\n",
      "    - -1477.397227985362\n",
      "    - -1598.1026156944956\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1924567569365651\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14669294193326812\n",
      "    mean_inference_ms: 0.9702418962451257\n",
      "    mean_raw_obs_processing_ms: 0.12104524374165707\n",
      "time_since_restore: 63.94594883918762\n",
      "time_this_iter_s: 11.564963817596436\n",
      "time_total_s: 63.94594883918762\n",
      "timers:\n",
      "  learn_throughput: 857.035\n",
      "  learn_time_ms: 4667.254\n",
      "  load_throughput: 17604633.788\n",
      "  load_time_ms: 0.227\n",
      "  training_iteration_time_ms: 10651.422\n",
      "  update_time_ms: 2.655\n",
      "timestamp: 1660563982\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 24000\n",
      "training_iteration: 6\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 28000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 28000\n",
      "  num_agent_steps_trained: 28000\n",
      "  num_env_steps_sampled: 28000\n",
      "  num_env_steps_trained: 28000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-46-33\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -750.1708199632869\n",
      "episode_reward_mean: -1124.29678282887\n",
      "episode_reward_min: -1727.713081232677\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 140\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4034947156906128\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005638598930090666\n",
      "        model: {}\n",
      "        policy_loss: 0.007010703440755606\n",
      "        total_loss: 9.824644088745117\n",
      "        vf_explained_var: -0.0049313330091536045\n",
      "        vf_loss: 9.81721019744873\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 28000\n",
      "  num_agent_steps_trained: 28000\n",
      "  num_env_steps_sampled: 28000\n",
      "  num_env_steps_trained: 28000\n",
      "iterations_since_restore: 7\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 28000\n",
      "num_agent_steps_trained: 28000\n",
      "num_env_steps_sampled: 28000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 28000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.193749999999994\n",
      "  ram_util_percent: 63.125\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19445339460793362\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14792675450295142\n",
      "  mean_inference_ms: 0.9780957148649688\n",
      "  mean_raw_obs_processing_ms: 0.12194114427701827\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -750.1708199632869\n",
      "  episode_reward_mean: -1124.29678282887\n",
      "  episode_reward_min: -1727.713081232677\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1028.6210707992627\n",
      "    - -975.9864665502549\n",
      "    - -1592.38576653991\n",
      "    - -1000.6521673122053\n",
      "    - -1398.369801742319\n",
      "    - -1178.4371022745015\n",
      "    - -892.7744160404642\n",
      "    - -1454.5683407783858\n",
      "    - -1238.654877792119\n",
      "    - -1562.2754071965314\n",
      "    - -1060.7485993326652\n",
      "    - -969.3698243870065\n",
      "    - -881.8351650855735\n",
      "    - -1002.3608813149405\n",
      "    - -1195.511052349149\n",
      "    - -798.1522938989187\n",
      "    - -1092.1526659103322\n",
      "    - -1277.638138018075\n",
      "    - -781.5649561606865\n",
      "    - -1071.5614059143381\n",
      "    - -1257.0996234323723\n",
      "    - -1356.4152462264594\n",
      "    - -1458.231145700803\n",
      "    - -1232.3692828749433\n",
      "    - -1308.9407583153409\n",
      "    - -885.2728525599703\n",
      "    - -908.1068279385499\n",
      "    - -1082.685311025416\n",
      "    - -1207.5501598101648\n",
      "    - -873.5698972899754\n",
      "    - -1309.2148092521413\n",
      "    - -1556.6381611329396\n",
      "    - -1292.8461833213112\n",
      "    - -1272.4199178276228\n",
      "    - -1156.6894460739945\n",
      "    - -1042.9956653438996\n",
      "    - -892.5649503250196\n",
      "    - -898.9378676933726\n",
      "    - -775.9027688776691\n",
      "    - -1078.2806536566754\n",
      "    - -1495.3604172442854\n",
      "    - -865.5588183476433\n",
      "    - -922.7502880317732\n",
      "    - -999.8250414718867\n",
      "    - -988.0248784278043\n",
      "    - -1164.705840665387\n",
      "    - -1289.4105097373827\n",
      "    - -1184.0923439431308\n",
      "    - -1197.0117560756391\n",
      "    - -960.0258334277987\n",
      "    - -1277.9900991310735\n",
      "    - -770.0259497670975\n",
      "    - -894.905023910079\n",
      "    - -1056.555437384123\n",
      "    - -897.9332044513404\n",
      "    - -990.2668378462043\n",
      "    - -1446.8169691519672\n",
      "    - -999.0207574175389\n",
      "    - -1386.250266741606\n",
      "    - -937.6777795204221\n",
      "    - -1229.6068771427101\n",
      "    - -895.6088541598078\n",
      "    - -857.3892252685897\n",
      "    - -1192.056298724483\n",
      "    - -1035.2598650781515\n",
      "    - -1051.0931928095551\n",
      "    - -901.816036033148\n",
      "    - -1458.4045404615767\n",
      "    - -979.8327291529246\n",
      "    - -1302.1247125613982\n",
      "    - -971.6791654823678\n",
      "    - -1064.5645324861684\n",
      "    - -1214.7280518691261\n",
      "    - -885.0765330141511\n",
      "    - -1390.6111981195913\n",
      "    - -990.7559407628036\n",
      "    - -880.7702603279499\n",
      "    - -981.4116662316677\n",
      "    - -1477.397227985362\n",
      "    - -1598.1026156944956\n",
      "    - -951.7586921224059\n",
      "    - -1312.931152481941\n",
      "    - -1727.713081232677\n",
      "    - -750.1708199632869\n",
      "    - -975.0793036578964\n",
      "    - -865.610589297625\n",
      "    - -1028.74886652416\n",
      "    - -1166.19746871135\n",
      "    - -986.0734686511593\n",
      "    - -1365.5070890336976\n",
      "    - -1428.765456653926\n",
      "    - -1045.078554914389\n",
      "    - -1509.8999230166244\n",
      "    - -971.6531291153887\n",
      "    - -927.0184709814296\n",
      "    - -1348.038909407212\n",
      "    - -1354.0835462370426\n",
      "    - -947.774051411701\n",
      "    - -1089.514432354317\n",
      "    - -1295.2117729822621\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19445339460793362\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14792675450295142\n",
      "    mean_inference_ms: 0.9780957148649688\n",
      "    mean_raw_obs_processing_ms: 0.12194114427701827\n",
      "time_since_restore: 74.87002968788147\n",
      "time_this_iter_s: 10.924080848693848\n",
      "time_total_s: 74.87002968788147\n",
      "timers:\n",
      "  learn_throughput: 849.303\n",
      "  learn_time_ms: 4709.743\n",
      "  load_throughput: 17788626.477\n",
      "  load_time_ms: 0.225\n",
      "  training_iteration_time_ms: 10689.322\n",
      "  update_time_ms: 2.594\n",
      "timestamp: 1660563993\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 28000\n",
      "training_iteration: 7\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 32000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 32000\n",
      "  num_env_steps_trained: 32000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-46-44\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -750.1708199632869\n",
      "episode_reward_mean: -1125.998106279143\n",
      "episode_reward_min: -1727.713081232677\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 160\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3336611986160278\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005692181643098593\n",
      "        model: {}\n",
      "        policy_loss: 0.009319162927567959\n",
      "        total_loss: 9.92440128326416\n",
      "        vf_explained_var: -0.009362952783703804\n",
      "        vf_loss: 9.914654731750488\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 32000\n",
      "  num_env_steps_trained: 32000\n",
      "iterations_since_restore: 8\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 32000\n",
      "num_agent_steps_trained: 32000\n",
      "num_env_steps_sampled: 32000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 32000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 63.375\n",
      "  ram_util_percent: 63.15625000000001\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1973275281498612\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14987079720470894\n",
      "  mean_inference_ms: 0.9922326952666893\n",
      "  mean_raw_obs_processing_ms: 0.12356892549503826\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -750.1708199632869\n",
      "  episode_reward_mean: -1125.998106279143\n",
      "  episode_reward_min: -1727.713081232677\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1257.0996234323723\n",
      "    - -1356.4152462264594\n",
      "    - -1458.231145700803\n",
      "    - -1232.3692828749433\n",
      "    - -1308.9407583153409\n",
      "    - -885.2728525599703\n",
      "    - -908.1068279385499\n",
      "    - -1082.685311025416\n",
      "    - -1207.5501598101648\n",
      "    - -873.5698972899754\n",
      "    - -1309.2148092521413\n",
      "    - -1556.6381611329396\n",
      "    - -1292.8461833213112\n",
      "    - -1272.4199178276228\n",
      "    - -1156.6894460739945\n",
      "    - -1042.9956653438996\n",
      "    - -892.5649503250196\n",
      "    - -898.9378676933726\n",
      "    - -775.9027688776691\n",
      "    - -1078.2806536566754\n",
      "    - -1495.3604172442854\n",
      "    - -865.5588183476433\n",
      "    - -922.7502880317732\n",
      "    - -999.8250414718867\n",
      "    - -988.0248784278043\n",
      "    - -1164.705840665387\n",
      "    - -1289.4105097373827\n",
      "    - -1184.0923439431308\n",
      "    - -1197.0117560756391\n",
      "    - -960.0258334277987\n",
      "    - -1277.9900991310735\n",
      "    - -770.0259497670975\n",
      "    - -894.905023910079\n",
      "    - -1056.555437384123\n",
      "    - -897.9332044513404\n",
      "    - -990.2668378462043\n",
      "    - -1446.8169691519672\n",
      "    - -999.0207574175389\n",
      "    - -1386.250266741606\n",
      "    - -937.6777795204221\n",
      "    - -1229.6068771427101\n",
      "    - -895.6088541598078\n",
      "    - -857.3892252685897\n",
      "    - -1192.056298724483\n",
      "    - -1035.2598650781515\n",
      "    - -1051.0931928095551\n",
      "    - -901.816036033148\n",
      "    - -1458.4045404615767\n",
      "    - -979.8327291529246\n",
      "    - -1302.1247125613982\n",
      "    - -971.6791654823678\n",
      "    - -1064.5645324861684\n",
      "    - -1214.7280518691261\n",
      "    - -885.0765330141511\n",
      "    - -1390.6111981195913\n",
      "    - -990.7559407628036\n",
      "    - -880.7702603279499\n",
      "    - -981.4116662316677\n",
      "    - -1477.397227985362\n",
      "    - -1598.1026156944956\n",
      "    - -951.7586921224059\n",
      "    - -1312.931152481941\n",
      "    - -1727.713081232677\n",
      "    - -750.1708199632869\n",
      "    - -975.0793036578964\n",
      "    - -865.610589297625\n",
      "    - -1028.74886652416\n",
      "    - -1166.19746871135\n",
      "    - -986.0734686511593\n",
      "    - -1365.5070890336976\n",
      "    - -1428.765456653926\n",
      "    - -1045.078554914389\n",
      "    - -1509.8999230166244\n",
      "    - -971.6531291153887\n",
      "    - -927.0184709814296\n",
      "    - -1348.038909407212\n",
      "    - -1354.0835462370426\n",
      "    - -947.774051411701\n",
      "    - -1089.514432354317\n",
      "    - -1295.2117729822621\n",
      "    - -984.6656871843107\n",
      "    - -1298.4127893454752\n",
      "    - -1299.9335958071022\n",
      "    - -1067.8850391105705\n",
      "    - -1252.6252152600755\n",
      "    - -1084.5800324239776\n",
      "    - -871.4490429127055\n",
      "    - -960.1559312805481\n",
      "    - -752.2611448895939\n",
      "    - -1395.3101097935737\n",
      "    - -1170.515259649023\n",
      "    - -1271.5355007884198\n",
      "    - -1025.8186804975956\n",
      "    - -922.3071304559884\n",
      "    - -1263.8072452927831\n",
      "    - -1392.376828758359\n",
      "    - -1013.1881316607218\n",
      "    - -990.5046144452468\n",
      "    - -1252.085875911027\n",
      "    - -1354.3348889578583\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1973275281498612\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14987079720470894\n",
      "    mean_inference_ms: 0.9922326952666893\n",
      "    mean_raw_obs_processing_ms: 0.12356892549503826\n",
      "time_since_restore: 86.57013988494873\n",
      "time_this_iter_s: 11.70011019706726\n",
      "time_total_s: 86.57013988494873\n",
      "timers:\n",
      "  learn_throughput: 844.631\n",
      "  learn_time_ms: 4735.794\n",
      "  load_throughput: 16209870.531\n",
      "  load_time_ms: 0.247\n",
      "  training_iteration_time_ms: 10813.73\n",
      "  update_time_ms: 2.531\n",
      "timestamp: 1660564004\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 32000\n",
      "training_iteration: 8\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 36000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_env_steps_sampled: 36000\n",
      "  num_env_steps_trained: 36000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-46-55\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -750.1708199632869\n",
      "episode_reward_mean: -1120.079003277714\n",
      "episode_reward_min: -1727.713081232677\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 180\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2940475940704346\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0036828238517045975\n",
      "        model: {}\n",
      "        policy_loss: 0.008659635670483112\n",
      "        total_loss: 9.91256046295166\n",
      "        vf_explained_var: -0.0081065334379673\n",
      "        vf_loss: 9.90362548828125\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_env_steps_sampled: 36000\n",
      "  num_env_steps_trained: 36000\n",
      "iterations_since_restore: 9\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 36000\n",
      "num_agent_steps_trained: 36000\n",
      "num_env_steps_sampled: 36000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 36000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.29999999999998\n",
      "  ram_util_percent: 63.10000000000001\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19996099154448643\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15160738953082226\n",
      "  mean_inference_ms: 1.0046448838902597\n",
      "  mean_raw_obs_processing_ms: 0.12499906162424278\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -750.1708199632869\n",
      "  episode_reward_mean: -1120.079003277714\n",
      "  episode_reward_min: -1727.713081232677\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1495.3604172442854\n",
      "    - -865.5588183476433\n",
      "    - -922.7502880317732\n",
      "    - -999.8250414718867\n",
      "    - -988.0248784278043\n",
      "    - -1164.705840665387\n",
      "    - -1289.4105097373827\n",
      "    - -1184.0923439431308\n",
      "    - -1197.0117560756391\n",
      "    - -960.0258334277987\n",
      "    - -1277.9900991310735\n",
      "    - -770.0259497670975\n",
      "    - -894.905023910079\n",
      "    - -1056.555437384123\n",
      "    - -897.9332044513404\n",
      "    - -990.2668378462043\n",
      "    - -1446.8169691519672\n",
      "    - -999.0207574175389\n",
      "    - -1386.250266741606\n",
      "    - -937.6777795204221\n",
      "    - -1229.6068771427101\n",
      "    - -895.6088541598078\n",
      "    - -857.3892252685897\n",
      "    - -1192.056298724483\n",
      "    - -1035.2598650781515\n",
      "    - -1051.0931928095551\n",
      "    - -901.816036033148\n",
      "    - -1458.4045404615767\n",
      "    - -979.8327291529246\n",
      "    - -1302.1247125613982\n",
      "    - -971.6791654823678\n",
      "    - -1064.5645324861684\n",
      "    - -1214.7280518691261\n",
      "    - -885.0765330141511\n",
      "    - -1390.6111981195913\n",
      "    - -990.7559407628036\n",
      "    - -880.7702603279499\n",
      "    - -981.4116662316677\n",
      "    - -1477.397227985362\n",
      "    - -1598.1026156944956\n",
      "    - -951.7586921224059\n",
      "    - -1312.931152481941\n",
      "    - -1727.713081232677\n",
      "    - -750.1708199632869\n",
      "    - -975.0793036578964\n",
      "    - -865.610589297625\n",
      "    - -1028.74886652416\n",
      "    - -1166.19746871135\n",
      "    - -986.0734686511593\n",
      "    - -1365.5070890336976\n",
      "    - -1428.765456653926\n",
      "    - -1045.078554914389\n",
      "    - -1509.8999230166244\n",
      "    - -971.6531291153887\n",
      "    - -927.0184709814296\n",
      "    - -1348.038909407212\n",
      "    - -1354.0835462370426\n",
      "    - -947.774051411701\n",
      "    - -1089.514432354317\n",
      "    - -1295.2117729822621\n",
      "    - -984.6656871843107\n",
      "    - -1298.4127893454752\n",
      "    - -1299.9335958071022\n",
      "    - -1067.8850391105705\n",
      "    - -1252.6252152600755\n",
      "    - -1084.5800324239776\n",
      "    - -871.4490429127055\n",
      "    - -960.1559312805481\n",
      "    - -752.2611448895939\n",
      "    - -1395.3101097935737\n",
      "    - -1170.515259649023\n",
      "    - -1271.5355007884198\n",
      "    - -1025.8186804975956\n",
      "    - -922.3071304559884\n",
      "    - -1263.8072452927831\n",
      "    - -1392.376828758359\n",
      "    - -1013.1881316607218\n",
      "    - -990.5046144452468\n",
      "    - -1252.085875911027\n",
      "    - -1354.3348889578583\n",
      "    - -1004.0401374078295\n",
      "    - -986.6928740625842\n",
      "    - -961.7054570870514\n",
      "    - -1036.8587952672158\n",
      "    - -995.0528359223036\n",
      "    - -1071.9330487538177\n",
      "    - -1012.1795472932422\n",
      "    - -1218.4628959571355\n",
      "    - -1062.940764838377\n",
      "    - -1390.244596233668\n",
      "    - -1271.2996320169154\n",
      "    - -885.1450859950809\n",
      "    - -1186.8101866234103\n",
      "    - -1201.1202009022522\n",
      "    - -986.5244560666621\n",
      "    - -979.552875945923\n",
      "    - -1157.906669525594\n",
      "    - -1294.3962332939186\n",
      "    - -1071.068468631453\n",
      "    - -1480.8864667113053\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19996099154448643\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15160738953082226\n",
      "    mean_inference_ms: 1.0046448838902597\n",
      "    mean_raw_obs_processing_ms: 0.12499906162424278\n",
      "time_since_restore: 96.65355181694031\n",
      "time_this_iter_s: 10.083411931991577\n",
      "time_total_s: 96.65355181694031\n",
      "timers:\n",
      "  learn_throughput: 851.732\n",
      "  learn_time_ms: 4696.31\n",
      "  load_throughput: 15912629.782\n",
      "  load_time_ms: 0.251\n",
      "  training_iteration_time_ms: 10731.93\n",
      "  update_time_ms: 2.525\n",
      "timestamp: 1660564015\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 36000\n",
      "training_iteration: 9\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 40000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 40000\n",
      "  num_env_steps_trained: 40000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-47-05\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -750.1708199632869\n",
      "episode_reward_mean: -1145.360024117421\n",
      "episode_reward_min: -1727.713081232677\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 200\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.03750000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4376777410507202\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02034643478691578\n",
      "        model: {}\n",
      "        policy_loss: 0.005315931513905525\n",
      "        total_loss: 9.905220985412598\n",
      "        vf_explained_var: -0.006779614835977554\n",
      "        vf_loss: 9.899141311645508\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 40000\n",
      "  num_env_steps_trained: 40000\n",
      "iterations_since_restore: 10\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 40000\n",
      "num_agent_steps_trained: 40000\n",
      "num_env_steps_sampled: 40000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 40000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.371428571428574\n",
      "  ram_util_percent: 63.14285714285716\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.20050330838603617\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.152101472842437\n",
      "  mean_inference_ms: 1.006952649334518\n",
      "  mean_raw_obs_processing_ms: 0.12543044885333962\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -750.1708199632869\n",
      "  episode_reward_mean: -1145.360024117421\n",
      "  episode_reward_min: -1727.713081232677\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1229.6068771427101\n",
      "    - -895.6088541598078\n",
      "    - -857.3892252685897\n",
      "    - -1192.056298724483\n",
      "    - -1035.2598650781515\n",
      "    - -1051.0931928095551\n",
      "    - -901.816036033148\n",
      "    - -1458.4045404615767\n",
      "    - -979.8327291529246\n",
      "    - -1302.1247125613982\n",
      "    - -971.6791654823678\n",
      "    - -1064.5645324861684\n",
      "    - -1214.7280518691261\n",
      "    - -885.0765330141511\n",
      "    - -1390.6111981195913\n",
      "    - -990.7559407628036\n",
      "    - -880.7702603279499\n",
      "    - -981.4116662316677\n",
      "    - -1477.397227985362\n",
      "    - -1598.1026156944956\n",
      "    - -951.7586921224059\n",
      "    - -1312.931152481941\n",
      "    - -1727.713081232677\n",
      "    - -750.1708199632869\n",
      "    - -975.0793036578964\n",
      "    - -865.610589297625\n",
      "    - -1028.74886652416\n",
      "    - -1166.19746871135\n",
      "    - -986.0734686511593\n",
      "    - -1365.5070890336976\n",
      "    - -1428.765456653926\n",
      "    - -1045.078554914389\n",
      "    - -1509.8999230166244\n",
      "    - -971.6531291153887\n",
      "    - -927.0184709814296\n",
      "    - -1348.038909407212\n",
      "    - -1354.0835462370426\n",
      "    - -947.774051411701\n",
      "    - -1089.514432354317\n",
      "    - -1295.2117729822621\n",
      "    - -984.6656871843107\n",
      "    - -1298.4127893454752\n",
      "    - -1299.9335958071022\n",
      "    - -1067.8850391105705\n",
      "    - -1252.6252152600755\n",
      "    - -1084.5800324239776\n",
      "    - -871.4490429127055\n",
      "    - -960.1559312805481\n",
      "    - -752.2611448895939\n",
      "    - -1395.3101097935737\n",
      "    - -1170.515259649023\n",
      "    - -1271.5355007884198\n",
      "    - -1025.8186804975956\n",
      "    - -922.3071304559884\n",
      "    - -1263.8072452927831\n",
      "    - -1392.376828758359\n",
      "    - -1013.1881316607218\n",
      "    - -990.5046144452468\n",
      "    - -1252.085875911027\n",
      "    - -1354.3348889578583\n",
      "    - -1004.0401374078295\n",
      "    - -986.6928740625842\n",
      "    - -961.7054570870514\n",
      "    - -1036.8587952672158\n",
      "    - -995.0528359223036\n",
      "    - -1071.9330487538177\n",
      "    - -1012.1795472932422\n",
      "    - -1218.4628959571355\n",
      "    - -1062.940764838377\n",
      "    - -1390.244596233668\n",
      "    - -1271.2996320169154\n",
      "    - -885.1450859950809\n",
      "    - -1186.8101866234103\n",
      "    - -1201.1202009022522\n",
      "    - -986.5244560666621\n",
      "    - -979.552875945923\n",
      "    - -1157.906669525594\n",
      "    - -1294.3962332939186\n",
      "    - -1071.068468631453\n",
      "    - -1480.8864667113053\n",
      "    - -1583.8790800764514\n",
      "    - -883.453057234073\n",
      "    - -1428.4301020744792\n",
      "    - -1327.7487708334095\n",
      "    - -1052.9075831829693\n",
      "    - -966.4300359093063\n",
      "    - -1084.5512375546582\n",
      "    - -1217.4632161151994\n",
      "    - -1262.5852083645625\n",
      "    - -1129.280257103618\n",
      "    - -1154.7482553683692\n",
      "    - -1528.5516600500987\n",
      "    - -1168.7470445538497\n",
      "    - -1483.0095640136024\n",
      "    - -1005.1371412675821\n",
      "    - -1285.592127803261\n",
      "    - -881.6456204023156\n",
      "    - -1224.8492853892299\n",
      "    - -976.6641123577225\n",
      "    - -1606.636777010143\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20050330838603617\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.152101472842437\n",
      "    mean_inference_ms: 1.006952649334518\n",
      "    mean_raw_obs_processing_ms: 0.12543044885333962\n",
      "time_since_restore: 106.64702534675598\n",
      "time_this_iter_s: 9.993473529815674\n",
      "time_total_s: 106.64702534675598\n",
      "timers:\n",
      "  learn_throughput: 857.953\n",
      "  learn_time_ms: 4662.261\n",
      "  load_throughput: 16169252.12\n",
      "  load_time_ms: 0.247\n",
      "  training_iteration_time_ms: 10657.24\n",
      "  update_time_ms: 2.484\n",
      "timestamp: 1660564025\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 40000\n",
      "training_iteration: 10\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 44000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 44000\n",
      "  num_agent_steps_trained: 44000\n",
      "  num_env_steps_sampled: 44000\n",
      "  num_env_steps_trained: 44000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-47-15\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -750.1708199632869\n",
      "episode_reward_mean: -1142.1253368194139\n",
      "episode_reward_min: -1727.713081232677\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 220\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.05624999850988388\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2628276348114014\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005384389776736498\n",
      "        model: {}\n",
      "        policy_loss: 0.007397905457764864\n",
      "        total_loss: 9.868685722351074\n",
      "        vf_explained_var: -0.009961997158825397\n",
      "        vf_loss: 9.86098575592041\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 44000\n",
      "  num_agent_steps_trained: 44000\n",
      "  num_env_steps_sampled: 44000\n",
      "  num_env_steps_trained: 44000\n",
      "iterations_since_restore: 11\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 44000\n",
      "num_agent_steps_trained: 44000\n",
      "num_env_steps_sampled: 44000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 44000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.17857142857143\n",
      "  ram_util_percent: 63.24285714285714\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.20034177475309775\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15205681910118926\n",
      "  mean_inference_ms: 1.0058125473406028\n",
      "  mean_raw_obs_processing_ms: 0.12543286875062237\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -750.1708199632869\n",
      "  episode_reward_mean: -1142.1253368194139\n",
      "  episode_reward_min: -1727.713081232677\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -951.7586921224059\n",
      "    - -1312.931152481941\n",
      "    - -1727.713081232677\n",
      "    - -750.1708199632869\n",
      "    - -975.0793036578964\n",
      "    - -865.610589297625\n",
      "    - -1028.74886652416\n",
      "    - -1166.19746871135\n",
      "    - -986.0734686511593\n",
      "    - -1365.5070890336976\n",
      "    - -1428.765456653926\n",
      "    - -1045.078554914389\n",
      "    - -1509.8999230166244\n",
      "    - -971.6531291153887\n",
      "    - -927.0184709814296\n",
      "    - -1348.038909407212\n",
      "    - -1354.0835462370426\n",
      "    - -947.774051411701\n",
      "    - -1089.514432354317\n",
      "    - -1295.2117729822621\n",
      "    - -984.6656871843107\n",
      "    - -1298.4127893454752\n",
      "    - -1299.9335958071022\n",
      "    - -1067.8850391105705\n",
      "    - -1252.6252152600755\n",
      "    - -1084.5800324239776\n",
      "    - -871.4490429127055\n",
      "    - -960.1559312805481\n",
      "    - -752.2611448895939\n",
      "    - -1395.3101097935737\n",
      "    - -1170.515259649023\n",
      "    - -1271.5355007884198\n",
      "    - -1025.8186804975956\n",
      "    - -922.3071304559884\n",
      "    - -1263.8072452927831\n",
      "    - -1392.376828758359\n",
      "    - -1013.1881316607218\n",
      "    - -990.5046144452468\n",
      "    - -1252.085875911027\n",
      "    - -1354.3348889578583\n",
      "    - -1004.0401374078295\n",
      "    - -986.6928740625842\n",
      "    - -961.7054570870514\n",
      "    - -1036.8587952672158\n",
      "    - -995.0528359223036\n",
      "    - -1071.9330487538177\n",
      "    - -1012.1795472932422\n",
      "    - -1218.4628959571355\n",
      "    - -1062.940764838377\n",
      "    - -1390.244596233668\n",
      "    - -1271.2996320169154\n",
      "    - -885.1450859950809\n",
      "    - -1186.8101866234103\n",
      "    - -1201.1202009022522\n",
      "    - -986.5244560666621\n",
      "    - -979.552875945923\n",
      "    - -1157.906669525594\n",
      "    - -1294.3962332939186\n",
      "    - -1071.068468631453\n",
      "    - -1480.8864667113053\n",
      "    - -1583.8790800764514\n",
      "    - -883.453057234073\n",
      "    - -1428.4301020744792\n",
      "    - -1327.7487708334095\n",
      "    - -1052.9075831829693\n",
      "    - -966.4300359093063\n",
      "    - -1084.5512375546582\n",
      "    - -1217.4632161151994\n",
      "    - -1262.5852083645625\n",
      "    - -1129.280257103618\n",
      "    - -1154.7482553683692\n",
      "    - -1528.5516600500987\n",
      "    - -1168.7470445538497\n",
      "    - -1483.0095640136024\n",
      "    - -1005.1371412675821\n",
      "    - -1285.592127803261\n",
      "    - -881.6456204023156\n",
      "    - -1224.8492853892299\n",
      "    - -976.6641123577225\n",
      "    - -1606.636777010143\n",
      "    - -1320.5573959462363\n",
      "    - -1380.1945519506942\n",
      "    - -890.7492525449899\n",
      "    - -988.0387743656968\n",
      "    - -894.7609697588659\n",
      "    - -1074.381738229133\n",
      "    - -1093.2092547319978\n",
      "    - -911.8388521166117\n",
      "    - -1213.2438060117252\n",
      "    - -1496.9825406707344\n",
      "    - -1171.7846641255787\n",
      "    - -1002.4732652330164\n",
      "    - -996.91237358407\n",
      "    - -978.9853095171803\n",
      "    - -975.2730694075344\n",
      "    - -1167.0767286251826\n",
      "    - -1158.7258874531447\n",
      "    - -1286.7044581478426\n",
      "    - -1030.0650130764334\n",
      "    - -1002.8628880686103\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20034177475309775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15205681910118926\n",
      "    mean_inference_ms: 1.0058125473406028\n",
      "    mean_raw_obs_processing_ms: 0.12543286875062237\n",
      "time_since_restore: 116.55789232254028\n",
      "time_this_iter_s: 9.910866975784302\n",
      "time_total_s: 116.55789232254028\n",
      "timers:\n",
      "  learn_throughput: 862.341\n",
      "  learn_time_ms: 4638.535\n",
      "  load_throughput: 16220841.149\n",
      "  load_time_ms: 0.247\n",
      "  training_iteration_time_ms: 10600.624\n",
      "  update_time_ms: 2.469\n",
      "timestamp: 1660564035\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 44000\n",
      "training_iteration: 11\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 48000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 48000\n",
      "  num_env_steps_sampled: 48000\n",
      "  num_env_steps_trained: 48000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-47-25\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -752.2611448895939\n",
      "episode_reward_mean: -1130.8137696289918\n",
      "episode_reward_min: -1606.636777010143\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 240\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.05624999850988388\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2607417106628418\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005484833847731352\n",
      "        model: {}\n",
      "        policy_loss: 0.008179008960723877\n",
      "        total_loss: 9.929567337036133\n",
      "        vf_explained_var: -0.012722083367407322\n",
      "        vf_loss: 9.92108154296875\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 48000\n",
      "  num_env_steps_sampled: 48000\n",
      "  num_env_steps_trained: 48000\n",
      "iterations_since_restore: 12\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 48000\n",
      "num_agent_steps_trained: 48000\n",
      "num_env_steps_sampled: 48000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 48000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.20666666666666\n",
      "  ram_util_percent: 63.226666666666674\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.2000269566187943\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15187594887407038\n",
      "  mean_inference_ms: 1.0041018013478995\n",
      "  mean_raw_obs_processing_ms: 0.12538086672489912\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -752.2611448895939\n",
      "  episode_reward_mean: -1130.8137696289918\n",
      "  episode_reward_min: -1606.636777010143\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -984.6656871843107\n",
      "    - -1298.4127893454752\n",
      "    - -1299.9335958071022\n",
      "    - -1067.8850391105705\n",
      "    - -1252.6252152600755\n",
      "    - -1084.5800324239776\n",
      "    - -871.4490429127055\n",
      "    - -960.1559312805481\n",
      "    - -752.2611448895939\n",
      "    - -1395.3101097935737\n",
      "    - -1170.515259649023\n",
      "    - -1271.5355007884198\n",
      "    - -1025.8186804975956\n",
      "    - -922.3071304559884\n",
      "    - -1263.8072452927831\n",
      "    - -1392.376828758359\n",
      "    - -1013.1881316607218\n",
      "    - -990.5046144452468\n",
      "    - -1252.085875911027\n",
      "    - -1354.3348889578583\n",
      "    - -1004.0401374078295\n",
      "    - -986.6928740625842\n",
      "    - -961.7054570870514\n",
      "    - -1036.8587952672158\n",
      "    - -995.0528359223036\n",
      "    - -1071.9330487538177\n",
      "    - -1012.1795472932422\n",
      "    - -1218.4628959571355\n",
      "    - -1062.940764838377\n",
      "    - -1390.244596233668\n",
      "    - -1271.2996320169154\n",
      "    - -885.1450859950809\n",
      "    - -1186.8101866234103\n",
      "    - -1201.1202009022522\n",
      "    - -986.5244560666621\n",
      "    - -979.552875945923\n",
      "    - -1157.906669525594\n",
      "    - -1294.3962332939186\n",
      "    - -1071.068468631453\n",
      "    - -1480.8864667113053\n",
      "    - -1583.8790800764514\n",
      "    - -883.453057234073\n",
      "    - -1428.4301020744792\n",
      "    - -1327.7487708334095\n",
      "    - -1052.9075831829693\n",
      "    - -966.4300359093063\n",
      "    - -1084.5512375546582\n",
      "    - -1217.4632161151994\n",
      "    - -1262.5852083645625\n",
      "    - -1129.280257103618\n",
      "    - -1154.7482553683692\n",
      "    - -1528.5516600500987\n",
      "    - -1168.7470445538497\n",
      "    - -1483.0095640136024\n",
      "    - -1005.1371412675821\n",
      "    - -1285.592127803261\n",
      "    - -881.6456204023156\n",
      "    - -1224.8492853892299\n",
      "    - -976.6641123577225\n",
      "    - -1606.636777010143\n",
      "    - -1320.5573959462363\n",
      "    - -1380.1945519506942\n",
      "    - -890.7492525449899\n",
      "    - -988.0387743656968\n",
      "    - -894.7609697588659\n",
      "    - -1074.381738229133\n",
      "    - -1093.2092547319978\n",
      "    - -911.8388521166117\n",
      "    - -1213.2438060117252\n",
      "    - -1496.9825406707344\n",
      "    - -1171.7846641255787\n",
      "    - -1002.4732652330164\n",
      "    - -996.91237358407\n",
      "    - -978.9853095171803\n",
      "    - -975.2730694075344\n",
      "    - -1167.0767286251826\n",
      "    - -1158.7258874531447\n",
      "    - -1286.7044581478426\n",
      "    - -1030.0650130764334\n",
      "    - -1002.8628880686103\n",
      "    - -1361.2616013244947\n",
      "    - -1370.5264094386528\n",
      "    - -1094.7353051669206\n",
      "    - -979.2006266487707\n",
      "    - -881.4816214000566\n",
      "    - -1065.1044219662488\n",
      "    - -864.141525618561\n",
      "    - -1116.87415125483\n",
      "    - -862.4283517891099\n",
      "    - -884.7606744307536\n",
      "    - -1266.8758163812836\n",
      "    - -1107.1972223918276\n",
      "    - -971.1076123505752\n",
      "    - -1176.3504157595905\n",
      "    - -1164.518234182824\n",
      "    - -1159.7486893612027\n",
      "    - -1440.023654817603\n",
      "    - -1145.2002916578608\n",
      "    - -1137.773565081269\n",
      "    - -866.3618686858658\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2000269566187943\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15187594887407038\n",
      "    mean_inference_ms: 1.0041018013478995\n",
      "    mean_raw_obs_processing_ms: 0.12538086672489912\n",
      "time_since_restore: 126.5653612613678\n",
      "time_this_iter_s: 10.007468938827515\n",
      "time_total_s: 126.5653612613678\n",
      "timers:\n",
      "  learn_throughput: 858.344\n",
      "  learn_time_ms: 4660.139\n",
      "  load_throughput: 16225547.389\n",
      "  load_time_ms: 0.247\n",
      "  training_iteration_time_ms: 10626.366\n",
      "  update_time_ms: 2.498\n",
      "timestamp: 1660564045\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 48000\n",
      "training_iteration: 12\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 52000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 52000\n",
      "  num_agent_steps_trained: 52000\n",
      "  num_env_steps_sampled: 52000\n",
      "  num_env_steps_trained: 52000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-47-35\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -830.7435234268424\n",
      "episode_reward_mean: -1117.3600833302694\n",
      "episode_reward_min: -1606.636777010143\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 260\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.05624999850988388\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1440253257751465\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005859719589352608\n",
      "        model: {}\n",
      "        policy_loss: 0.008333350531756878\n",
      "        total_loss: 9.872330665588379\n",
      "        vf_explained_var: -0.009820525534451008\n",
      "        vf_loss: 9.863667488098145\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 52000\n",
      "  num_agent_steps_trained: 52000\n",
      "  num_env_steps_sampled: 52000\n",
      "  num_env_steps_trained: 52000\n",
      "iterations_since_restore: 13\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 52000\n",
      "num_agent_steps_trained: 52000\n",
      "num_env_steps_sampled: 52000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 52000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.107142857142854\n",
      "  ram_util_percent: 63.200000000000024\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19890663794780708\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15116563943335512\n",
      "  mean_inference_ms: 0.9982668692482031\n",
      "  mean_raw_obs_processing_ms: 0.12484891989085944\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -830.7435234268424\n",
      "  episode_reward_mean: -1117.3600833302694\n",
      "  episode_reward_min: -1606.636777010143\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1004.0401374078295\n",
      "    - -986.6928740625842\n",
      "    - -961.7054570870514\n",
      "    - -1036.8587952672158\n",
      "    - -995.0528359223036\n",
      "    - -1071.9330487538177\n",
      "    - -1012.1795472932422\n",
      "    - -1218.4628959571355\n",
      "    - -1062.940764838377\n",
      "    - -1390.244596233668\n",
      "    - -1271.2996320169154\n",
      "    - -885.1450859950809\n",
      "    - -1186.8101866234103\n",
      "    - -1201.1202009022522\n",
      "    - -986.5244560666621\n",
      "    - -979.552875945923\n",
      "    - -1157.906669525594\n",
      "    - -1294.3962332939186\n",
      "    - -1071.068468631453\n",
      "    - -1480.8864667113053\n",
      "    - -1583.8790800764514\n",
      "    - -883.453057234073\n",
      "    - -1428.4301020744792\n",
      "    - -1327.7487708334095\n",
      "    - -1052.9075831829693\n",
      "    - -966.4300359093063\n",
      "    - -1084.5512375546582\n",
      "    - -1217.4632161151994\n",
      "    - -1262.5852083645625\n",
      "    - -1129.280257103618\n",
      "    - -1154.7482553683692\n",
      "    - -1528.5516600500987\n",
      "    - -1168.7470445538497\n",
      "    - -1483.0095640136024\n",
      "    - -1005.1371412675821\n",
      "    - -1285.592127803261\n",
      "    - -881.6456204023156\n",
      "    - -1224.8492853892299\n",
      "    - -976.6641123577225\n",
      "    - -1606.636777010143\n",
      "    - -1320.5573959462363\n",
      "    - -1380.1945519506942\n",
      "    - -890.7492525449899\n",
      "    - -988.0387743656968\n",
      "    - -894.7609697588659\n",
      "    - -1074.381738229133\n",
      "    - -1093.2092547319978\n",
      "    - -911.8388521166117\n",
      "    - -1213.2438060117252\n",
      "    - -1496.9825406707344\n",
      "    - -1171.7846641255787\n",
      "    - -1002.4732652330164\n",
      "    - -996.91237358407\n",
      "    - -978.9853095171803\n",
      "    - -975.2730694075344\n",
      "    - -1167.0767286251826\n",
      "    - -1158.7258874531447\n",
      "    - -1286.7044581478426\n",
      "    - -1030.0650130764334\n",
      "    - -1002.8628880686103\n",
      "    - -1361.2616013244947\n",
      "    - -1370.5264094386528\n",
      "    - -1094.7353051669206\n",
      "    - -979.2006266487707\n",
      "    - -881.4816214000566\n",
      "    - -1065.1044219662488\n",
      "    - -864.141525618561\n",
      "    - -1116.87415125483\n",
      "    - -862.4283517891099\n",
      "    - -884.7606744307536\n",
      "    - -1266.8758163812836\n",
      "    - -1107.1972223918276\n",
      "    - -971.1076123505752\n",
      "    - -1176.3504157595905\n",
      "    - -1164.518234182824\n",
      "    - -1159.7486893612027\n",
      "    - -1440.023654817603\n",
      "    - -1145.2002916578608\n",
      "    - -1137.773565081269\n",
      "    - -866.3618686858658\n",
      "    - -1011.894380503066\n",
      "    - -962.5412405084082\n",
      "    - -1165.6572593115652\n",
      "    - -975.4390018695758\n",
      "    - -965.4752958027967\n",
      "    - -1078.1888097124415\n",
      "    - -1092.368586089567\n",
      "    - -1167.120189947391\n",
      "    - -1200.5171076610193\n",
      "    - -1065.7666977893566\n",
      "    - -1074.364829231947\n",
      "    - -1002.0809236912219\n",
      "    - -990.124408942424\n",
      "    - -876.7254206549981\n",
      "    - -830.7435234268424\n",
      "    - -888.0835594253689\n",
      "    - -1235.9515592330197\n",
      "    - -1272.386978848976\n",
      "    - -1145.716070977693\n",
      "    - -1277.2382709250398\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19890663794780708\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15116563943335512\n",
      "    mean_inference_ms: 0.9982668692482031\n",
      "    mean_raw_obs_processing_ms: 0.12484891989085944\n",
      "time_since_restore: 136.57794880867004\n",
      "time_this_iter_s: 10.012587547302246\n",
      "time_total_s: 136.57794880867004\n",
      "timers:\n",
      "  learn_throughput: 853.953\n",
      "  learn_time_ms: 4684.1\n",
      "  load_throughput: 16309143.579\n",
      "  load_time_ms: 0.245\n",
      "  training_iteration_time_ms: 10665.473\n",
      "  update_time_ms: 2.486\n",
      "timestamp: 1660564055\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 52000\n",
      "training_iteration: 13\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 56000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 56000\n",
      "  num_agent_steps_trained: 56000\n",
      "  num_env_steps_sampled: 56000\n",
      "  num_env_steps_trained: 56000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-47-45\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -830.7435234268424\n",
      "episode_reward_mean: -1127.6169751206883\n",
      "episode_reward_min: -1606.636777010143\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 280\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.05624999850988388\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3712373971939087\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01999252289533615\n",
      "        model: {}\n",
      "        policy_loss: 0.003635181812569499\n",
      "        total_loss: 9.921658515930176\n",
      "        vf_explained_var: -0.005383792798966169\n",
      "        vf_loss: 9.916899681091309\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 56000\n",
      "  num_agent_steps_trained: 56000\n",
      "  num_env_steps_sampled: 56000\n",
      "  num_env_steps_trained: 56000\n",
      "iterations_since_restore: 14\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 56000\n",
      "num_agent_steps_trained: 56000\n",
      "num_env_steps_sampled: 56000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 56000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.6\n",
      "  ram_util_percent: 63.200000000000024\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1979339380933644\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15056563412444343\n",
      "  mean_inference_ms: 0.9935741828834124\n",
      "  mean_raw_obs_processing_ms: 0.12440345980046494\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -830.7435234268424\n",
      "  episode_reward_mean: -1127.6169751206883\n",
      "  episode_reward_min: -1606.636777010143\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1583.8790800764514\n",
      "    - -883.453057234073\n",
      "    - -1428.4301020744792\n",
      "    - -1327.7487708334095\n",
      "    - -1052.9075831829693\n",
      "    - -966.4300359093063\n",
      "    - -1084.5512375546582\n",
      "    - -1217.4632161151994\n",
      "    - -1262.5852083645625\n",
      "    - -1129.280257103618\n",
      "    - -1154.7482553683692\n",
      "    - -1528.5516600500987\n",
      "    - -1168.7470445538497\n",
      "    - -1483.0095640136024\n",
      "    - -1005.1371412675821\n",
      "    - -1285.592127803261\n",
      "    - -881.6456204023156\n",
      "    - -1224.8492853892299\n",
      "    - -976.6641123577225\n",
      "    - -1606.636777010143\n",
      "    - -1320.5573959462363\n",
      "    - -1380.1945519506942\n",
      "    - -890.7492525449899\n",
      "    - -988.0387743656968\n",
      "    - -894.7609697588659\n",
      "    - -1074.381738229133\n",
      "    - -1093.2092547319978\n",
      "    - -911.8388521166117\n",
      "    - -1213.2438060117252\n",
      "    - -1496.9825406707344\n",
      "    - -1171.7846641255787\n",
      "    - -1002.4732652330164\n",
      "    - -996.91237358407\n",
      "    - -978.9853095171803\n",
      "    - -975.2730694075344\n",
      "    - -1167.0767286251826\n",
      "    - -1158.7258874531447\n",
      "    - -1286.7044581478426\n",
      "    - -1030.0650130764334\n",
      "    - -1002.8628880686103\n",
      "    - -1361.2616013244947\n",
      "    - -1370.5264094386528\n",
      "    - -1094.7353051669206\n",
      "    - -979.2006266487707\n",
      "    - -881.4816214000566\n",
      "    - -1065.1044219662488\n",
      "    - -864.141525618561\n",
      "    - -1116.87415125483\n",
      "    - -862.4283517891099\n",
      "    - -884.7606744307536\n",
      "    - -1266.8758163812836\n",
      "    - -1107.1972223918276\n",
      "    - -971.1076123505752\n",
      "    - -1176.3504157595905\n",
      "    - -1164.518234182824\n",
      "    - -1159.7486893612027\n",
      "    - -1440.023654817603\n",
      "    - -1145.2002916578608\n",
      "    - -1137.773565081269\n",
      "    - -866.3618686858658\n",
      "    - -1011.894380503066\n",
      "    - -962.5412405084082\n",
      "    - -1165.6572593115652\n",
      "    - -975.4390018695758\n",
      "    - -965.4752958027967\n",
      "    - -1078.1888097124415\n",
      "    - -1092.368586089567\n",
      "    - -1167.120189947391\n",
      "    - -1200.5171076610193\n",
      "    - -1065.7666977893566\n",
      "    - -1074.364829231947\n",
      "    - -1002.0809236912219\n",
      "    - -990.124408942424\n",
      "    - -876.7254206549981\n",
      "    - -830.7435234268424\n",
      "    - -888.0835594253689\n",
      "    - -1235.9515592330197\n",
      "    - -1272.386978848976\n",
      "    - -1145.716070977693\n",
      "    - -1277.2382709250398\n",
      "    - -1347.451978333189\n",
      "    - -1110.1113878368856\n",
      "    - -1102.2369240343567\n",
      "    - -1587.439736935298\n",
      "    - -881.2123158193687\n",
      "    - -951.2547567599943\n",
      "    - -962.3564574912177\n",
      "    - -1014.1714829157552\n",
      "    - -1504.117134531302\n",
      "    - -1521.0963114674678\n",
      "    - -1340.891746043265\n",
      "    - -883.5668364206782\n",
      "    - -1272.2336457356344\n",
      "    - -993.4355646892261\n",
      "    - -1079.5274494346038\n",
      "    - -979.1771198120581\n",
      "    - -1280.2591540284602\n",
      "    - -1080.8784572821403\n",
      "    - -969.383271580854\n",
      "    - -1419.708676425878\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1979339380933644\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15056563412444343\n",
      "    mean_inference_ms: 0.9935741828834124\n",
      "    mean_raw_obs_processing_ms: 0.12440345980046494\n",
      "time_since_restore: 146.69089031219482\n",
      "time_this_iter_s: 10.11294150352478\n",
      "time_total_s: 146.69089031219482\n",
      "timers:\n",
      "  learn_throughput: 853.001\n",
      "  learn_time_ms: 4689.328\n",
      "  load_throughput: 16348875.463\n",
      "  load_time_ms: 0.245\n",
      "  training_iteration_time_ms: 10676.234\n",
      "  update_time_ms: 2.373\n",
      "timestamp: 1660564065\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 56000\n",
      "training_iteration: 14\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 60000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 60000\n",
      "  num_agent_steps_trained: 60000\n",
      "  num_env_steps_sampled: 60000\n",
      "  num_env_steps_trained: 60000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-47-55\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -830.7435234268424\n",
      "episode_reward_mean: -1099.6374249287298\n",
      "episode_reward_min: -1587.439736935298\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 300\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.05624999850988388\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.167995572090149\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005592580884695053\n",
      "        model: {}\n",
      "        policy_loss: 0.009335950948297977\n",
      "        total_loss: 9.904916763305664\n",
      "        vf_explained_var: -0.008636416867375374\n",
      "        vf_loss: 9.895264625549316\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 60000\n",
      "  num_agent_steps_trained: 60000\n",
      "  num_env_steps_sampled: 60000\n",
      "  num_env_steps_trained: 60000\n",
      "iterations_since_restore: 15\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 60000\n",
      "num_agent_steps_trained: 60000\n",
      "num_env_steps_sampled: 60000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 60000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.94666666666667\n",
      "  ram_util_percent: 63.200000000000024\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19710986877218706\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15002799561739408\n",
      "  mean_inference_ms: 0.989459016761317\n",
      "  mean_raw_obs_processing_ms: 0.12397995596162388\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -830.7435234268424\n",
      "  episode_reward_mean: -1099.6374249287298\n",
      "  episode_reward_min: -1587.439736935298\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1320.5573959462363\n",
      "    - -1380.1945519506942\n",
      "    - -890.7492525449899\n",
      "    - -988.0387743656968\n",
      "    - -894.7609697588659\n",
      "    - -1074.381738229133\n",
      "    - -1093.2092547319978\n",
      "    - -911.8388521166117\n",
      "    - -1213.2438060117252\n",
      "    - -1496.9825406707344\n",
      "    - -1171.7846641255787\n",
      "    - -1002.4732652330164\n",
      "    - -996.91237358407\n",
      "    - -978.9853095171803\n",
      "    - -975.2730694075344\n",
      "    - -1167.0767286251826\n",
      "    - -1158.7258874531447\n",
      "    - -1286.7044581478426\n",
      "    - -1030.0650130764334\n",
      "    - -1002.8628880686103\n",
      "    - -1361.2616013244947\n",
      "    - -1370.5264094386528\n",
      "    - -1094.7353051669206\n",
      "    - -979.2006266487707\n",
      "    - -881.4816214000566\n",
      "    - -1065.1044219662488\n",
      "    - -864.141525618561\n",
      "    - -1116.87415125483\n",
      "    - -862.4283517891099\n",
      "    - -884.7606744307536\n",
      "    - -1266.8758163812836\n",
      "    - -1107.1972223918276\n",
      "    - -971.1076123505752\n",
      "    - -1176.3504157595905\n",
      "    - -1164.518234182824\n",
      "    - -1159.7486893612027\n",
      "    - -1440.023654817603\n",
      "    - -1145.2002916578608\n",
      "    - -1137.773565081269\n",
      "    - -866.3618686858658\n",
      "    - -1011.894380503066\n",
      "    - -962.5412405084082\n",
      "    - -1165.6572593115652\n",
      "    - -975.4390018695758\n",
      "    - -965.4752958027967\n",
      "    - -1078.1888097124415\n",
      "    - -1092.368586089567\n",
      "    - -1167.120189947391\n",
      "    - -1200.5171076610193\n",
      "    - -1065.7666977893566\n",
      "    - -1074.364829231947\n",
      "    - -1002.0809236912219\n",
      "    - -990.124408942424\n",
      "    - -876.7254206549981\n",
      "    - -830.7435234268424\n",
      "    - -888.0835594253689\n",
      "    - -1235.9515592330197\n",
      "    - -1272.386978848976\n",
      "    - -1145.716070977693\n",
      "    - -1277.2382709250398\n",
      "    - -1347.451978333189\n",
      "    - -1110.1113878368856\n",
      "    - -1102.2369240343567\n",
      "    - -1587.439736935298\n",
      "    - -881.2123158193687\n",
      "    - -951.2547567599943\n",
      "    - -962.3564574912177\n",
      "    - -1014.1714829157552\n",
      "    - -1504.117134531302\n",
      "    - -1521.0963114674678\n",
      "    - -1340.891746043265\n",
      "    - -883.5668364206782\n",
      "    - -1272.2336457356344\n",
      "    - -993.4355646892261\n",
      "    - -1079.5274494346038\n",
      "    - -979.1771198120581\n",
      "    - -1280.2591540284602\n",
      "    - -1080.8784572821403\n",
      "    - -969.383271580854\n",
      "    - -1419.708676425878\n",
      "    - -1076.1751256788643\n",
      "    - -1004.7517550754968\n",
      "    - -985.6094820031589\n",
      "    - -968.2827213933231\n",
      "    - -902.1706224365591\n",
      "    - -978.8048108215033\n",
      "    - -1456.9753433029198\n",
      "    - -1501.563701476143\n",
      "    - -1397.2092453843295\n",
      "    - -873.8295290982294\n",
      "    - -1164.7387688674353\n",
      "    - -899.0314864132923\n",
      "    - -912.4278992175357\n",
      "    - -1191.3077586123004\n",
      "    - -1307.2165117411982\n",
      "    - -1039.8281223024628\n",
      "    - -885.9697838907011\n",
      "    - -1088.6241211251058\n",
      "    - -962.6299366103474\n",
      "    - -857.208392018151\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19710986877218706\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15002799561739408\n",
      "    mean_inference_ms: 0.989459016761317\n",
      "    mean_raw_obs_processing_ms: 0.12397995596162388\n",
      "time_since_restore: 156.5883128643036\n",
      "time_this_iter_s: 9.897422552108765\n",
      "time_total_s: 156.5883128643036\n",
      "timers:\n",
      "  learn_throughput: 876.481\n",
      "  learn_time_ms: 4563.702\n",
      "  load_throughput: 17081262.472\n",
      "  load_time_ms: 0.234\n",
      "  training_iteration_time_ms: 10412.67\n",
      "  update_time_ms: 2.375\n",
      "timestamp: 1660564075\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 60000\n",
      "training_iteration: 15\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 64000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 64000\n",
      "  num_agent_steps_trained: 64000\n",
      "  num_env_steps_sampled: 64000\n",
      "  num_env_steps_trained: 64000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-48-05\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -830.7435234268424\n",
      "episode_reward_mean: -1099.171839345708\n",
      "episode_reward_min: -1587.439736935298\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 320\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.05624999850988388\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2392239570617676\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021830610930919647\n",
      "        model: {}\n",
      "        policy_loss: 0.002842274960130453\n",
      "        total_loss: 9.804220199584961\n",
      "        vf_explained_var: -0.0077694132924079895\n",
      "        vf_loss: 9.800150871276855\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 64000\n",
      "  num_agent_steps_trained: 64000\n",
      "  num_env_steps_sampled: 64000\n",
      "  num_env_steps_trained: 64000\n",
      "iterations_since_restore: 16\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 64000\n",
      "num_agent_steps_trained: 64000\n",
      "num_env_steps_sampled: 64000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 64000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.685714285714276\n",
      "  ram_util_percent: 63.200000000000024\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19642043565298123\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14961984808799592\n",
      "  mean_inference_ms: 0.9862838844533559\n",
      "  mean_raw_obs_processing_ms: 0.12367786200895363\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -830.7435234268424\n",
      "  episode_reward_mean: -1099.171839345708\n",
      "  episode_reward_min: -1587.439736935298\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1361.2616013244947\n",
      "    - -1370.5264094386528\n",
      "    - -1094.7353051669206\n",
      "    - -979.2006266487707\n",
      "    - -881.4816214000566\n",
      "    - -1065.1044219662488\n",
      "    - -864.141525618561\n",
      "    - -1116.87415125483\n",
      "    - -862.4283517891099\n",
      "    - -884.7606744307536\n",
      "    - -1266.8758163812836\n",
      "    - -1107.1972223918276\n",
      "    - -971.1076123505752\n",
      "    - -1176.3504157595905\n",
      "    - -1164.518234182824\n",
      "    - -1159.7486893612027\n",
      "    - -1440.023654817603\n",
      "    - -1145.2002916578608\n",
      "    - -1137.773565081269\n",
      "    - -866.3618686858658\n",
      "    - -1011.894380503066\n",
      "    - -962.5412405084082\n",
      "    - -1165.6572593115652\n",
      "    - -975.4390018695758\n",
      "    - -965.4752958027967\n",
      "    - -1078.1888097124415\n",
      "    - -1092.368586089567\n",
      "    - -1167.120189947391\n",
      "    - -1200.5171076610193\n",
      "    - -1065.7666977893566\n",
      "    - -1074.364829231947\n",
      "    - -1002.0809236912219\n",
      "    - -990.124408942424\n",
      "    - -876.7254206549981\n",
      "    - -830.7435234268424\n",
      "    - -888.0835594253689\n",
      "    - -1235.9515592330197\n",
      "    - -1272.386978848976\n",
      "    - -1145.716070977693\n",
      "    - -1277.2382709250398\n",
      "    - -1347.451978333189\n",
      "    - -1110.1113878368856\n",
      "    - -1102.2369240343567\n",
      "    - -1587.439736935298\n",
      "    - -881.2123158193687\n",
      "    - -951.2547567599943\n",
      "    - -962.3564574912177\n",
      "    - -1014.1714829157552\n",
      "    - -1504.117134531302\n",
      "    - -1521.0963114674678\n",
      "    - -1340.891746043265\n",
      "    - -883.5668364206782\n",
      "    - -1272.2336457356344\n",
      "    - -993.4355646892261\n",
      "    - -1079.5274494346038\n",
      "    - -979.1771198120581\n",
      "    - -1280.2591540284602\n",
      "    - -1080.8784572821403\n",
      "    - -969.383271580854\n",
      "    - -1419.708676425878\n",
      "    - -1076.1751256788643\n",
      "    - -1004.7517550754968\n",
      "    - -985.6094820031589\n",
      "    - -968.2827213933231\n",
      "    - -902.1706224365591\n",
      "    - -978.8048108215033\n",
      "    - -1456.9753433029198\n",
      "    - -1501.563701476143\n",
      "    - -1397.2092453843295\n",
      "    - -873.8295290982294\n",
      "    - -1164.7387688674353\n",
      "    - -899.0314864132923\n",
      "    - -912.4278992175357\n",
      "    - -1191.3077586123004\n",
      "    - -1307.2165117411982\n",
      "    - -1039.8281223024628\n",
      "    - -885.9697838907011\n",
      "    - -1088.6241211251058\n",
      "    - -962.6299366103474\n",
      "    - -857.208392018151\n",
      "    - -988.5719970362455\n",
      "    - -1286.8852636247786\n",
      "    - -1093.7825987051697\n",
      "    - -875.1851631670377\n",
      "    - -1003.2909317749776\n",
      "    - -861.3244529156381\n",
      "    - -1331.8443637620196\n",
      "    - -884.1750625376609\n",
      "    - -1193.8278794735993\n",
      "    - -981.3911873277757\n",
      "    - -1300.3402124311635\n",
      "    - -898.2819570535346\n",
      "    - -926.2689038901142\n",
      "    - -986.0884953075515\n",
      "    - -1011.5524254339776\n",
      "    - -1436.642381684932\n",
      "    - -977.283307323075\n",
      "    - -1415.6181518565563\n",
      "    - -1574.3675876831765\n",
      "    - -961.5399122741225\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19642043565298123\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14961984808799592\n",
      "    mean_inference_ms: 0.9862838844533559\n",
      "    mean_raw_obs_processing_ms: 0.12367786200895363\n",
      "time_since_restore: 166.53575658798218\n",
      "time_this_iter_s: 9.947443723678589\n",
      "time_total_s: 166.53575658798218\n",
      "timers:\n",
      "  learn_throughput: 895.554\n",
      "  learn_time_ms: 4466.509\n",
      "  load_throughput: 16921044.881\n",
      "  load_time_ms: 0.236\n",
      "  training_iteration_time_ms: 10250.946\n",
      "  update_time_ms: 2.217\n",
      "timestamp: 1660564085\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 64000\n",
      "training_iteration: 16\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 68000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 68000\n",
      "  num_agent_steps_trained: 68000\n",
      "  num_env_steps_sampled: 68000\n",
      "  num_env_steps_trained: 68000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-48-15\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -830.7435234268424\n",
      "episode_reward_mean: -1122.6540729871165\n",
      "episode_reward_min: -1702.7208569094626\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 340\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.08437500149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6272920370101929\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02950480952858925\n",
      "        model: {}\n",
      "        policy_loss: -0.0017396834446117282\n",
      "        total_loss: 9.908612251281738\n",
      "        vf_explained_var: -0.006523828953504562\n",
      "        vf_loss: 9.907862663269043\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 68000\n",
      "  num_agent_steps_trained: 68000\n",
      "  num_env_steps_sampled: 68000\n",
      "  num_env_steps_trained: 68000\n",
      "iterations_since_restore: 17\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 68000\n",
      "num_agent_steps_trained: 68000\n",
      "num_env_steps_sampled: 68000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 68000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.41428571428572\n",
      "  ram_util_percent: 63.26428571428569\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19579635922430086\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14924242492891673\n",
      "  mean_inference_ms: 0.9834746099842233\n",
      "  mean_raw_obs_processing_ms: 0.12336148603930079\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -830.7435234268424\n",
      "  episode_reward_mean: -1122.6540729871165\n",
      "  episode_reward_min: -1702.7208569094626\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1011.894380503066\n",
      "    - -962.5412405084082\n",
      "    - -1165.6572593115652\n",
      "    - -975.4390018695758\n",
      "    - -965.4752958027967\n",
      "    - -1078.1888097124415\n",
      "    - -1092.368586089567\n",
      "    - -1167.120189947391\n",
      "    - -1200.5171076610193\n",
      "    - -1065.7666977893566\n",
      "    - -1074.364829231947\n",
      "    - -1002.0809236912219\n",
      "    - -990.124408942424\n",
      "    - -876.7254206549981\n",
      "    - -830.7435234268424\n",
      "    - -888.0835594253689\n",
      "    - -1235.9515592330197\n",
      "    - -1272.386978848976\n",
      "    - -1145.716070977693\n",
      "    - -1277.2382709250398\n",
      "    - -1347.451978333189\n",
      "    - -1110.1113878368856\n",
      "    - -1102.2369240343567\n",
      "    - -1587.439736935298\n",
      "    - -881.2123158193687\n",
      "    - -951.2547567599943\n",
      "    - -962.3564574912177\n",
      "    - -1014.1714829157552\n",
      "    - -1504.117134531302\n",
      "    - -1521.0963114674678\n",
      "    - -1340.891746043265\n",
      "    - -883.5668364206782\n",
      "    - -1272.2336457356344\n",
      "    - -993.4355646892261\n",
      "    - -1079.5274494346038\n",
      "    - -979.1771198120581\n",
      "    - -1280.2591540284602\n",
      "    - -1080.8784572821403\n",
      "    - -969.383271580854\n",
      "    - -1419.708676425878\n",
      "    - -1076.1751256788643\n",
      "    - -1004.7517550754968\n",
      "    - -985.6094820031589\n",
      "    - -968.2827213933231\n",
      "    - -902.1706224365591\n",
      "    - -978.8048108215033\n",
      "    - -1456.9753433029198\n",
      "    - -1501.563701476143\n",
      "    - -1397.2092453843295\n",
      "    - -873.8295290982294\n",
      "    - -1164.7387688674353\n",
      "    - -899.0314864132923\n",
      "    - -912.4278992175357\n",
      "    - -1191.3077586123004\n",
      "    - -1307.2165117411982\n",
      "    - -1039.8281223024628\n",
      "    - -885.9697838907011\n",
      "    - -1088.6241211251058\n",
      "    - -962.6299366103474\n",
      "    - -857.208392018151\n",
      "    - -988.5719970362455\n",
      "    - -1286.8852636247786\n",
      "    - -1093.7825987051697\n",
      "    - -875.1851631670377\n",
      "    - -1003.2909317749776\n",
      "    - -861.3244529156381\n",
      "    - -1331.8443637620196\n",
      "    - -884.1750625376609\n",
      "    - -1193.8278794735993\n",
      "    - -981.3911873277757\n",
      "    - -1300.3402124311635\n",
      "    - -898.2819570535346\n",
      "    - -926.2689038901142\n",
      "    - -986.0884953075515\n",
      "    - -1011.5524254339776\n",
      "    - -1436.642381684932\n",
      "    - -977.283307323075\n",
      "    - -1415.6181518565563\n",
      "    - -1574.3675876831765\n",
      "    - -961.5399122741225\n",
      "    - -1587.179523428015\n",
      "    - -910.3598233336487\n",
      "    - -1702.7208569094626\n",
      "    - -925.3158968506144\n",
      "    - -986.7899533988259\n",
      "    - -974.1095790547112\n",
      "    - -1452.0109107454036\n",
      "    - -995.535974510868\n",
      "    - -968.0489209058104\n",
      "    - -1294.3156689426387\n",
      "    - -1413.8756314948298\n",
      "    - -1402.9400119473962\n",
      "    - -1081.787391634897\n",
      "    - -1378.802735814917\n",
      "    - -969.6279470944629\n",
      "    - -1222.773969523166\n",
      "    - -1550.6761067783793\n",
      "    - -1393.4813353353152\n",
      "    - -875.6963681313085\n",
      "    - -1177.8468180144737\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19579635922430086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14924242492891673\n",
      "    mean_inference_ms: 0.9834746099842233\n",
      "    mean_raw_obs_processing_ms: 0.12336148603930079\n",
      "time_since_restore: 176.59442114830017\n",
      "time_this_iter_s: 10.058664560317993\n",
      "time_total_s: 176.59442114830017\n",
      "timers:\n",
      "  learn_throughput: 906.773\n",
      "  learn_time_ms: 4411.247\n",
      "  load_throughput: 16921044.881\n",
      "  load_time_ms: 0.236\n",
      "  training_iteration_time_ms: 10164.571\n",
      "  update_time_ms: 2.239\n",
      "timestamp: 1660564095\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 68000\n",
      "training_iteration: 17\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 72000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 72000\n",
      "  num_agent_steps_trained: 72000\n",
      "  num_env_steps_sampled: 72000\n",
      "  num_env_steps_trained: 72000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-48-25\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -857.208392018151\n",
      "episode_reward_mean: -1141.4988374583038\n",
      "episode_reward_min: -1702.7208569094626\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 360\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.12656250596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.37397038936615\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004484601318836212\n",
      "        model: {}\n",
      "        policy_loss: 0.011951899155974388\n",
      "        total_loss: 9.945382118225098\n",
      "        vf_explained_var: -0.0091512780636549\n",
      "        vf_loss: 9.932862281799316\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 72000\n",
      "  num_agent_steps_trained: 72000\n",
      "  num_env_steps_sampled: 72000\n",
      "  num_env_steps_trained: 72000\n",
      "iterations_since_restore: 18\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 72000\n",
      "num_agent_steps_trained: 72000\n",
      "num_env_steps_sampled: 72000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 72000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 41.586666666666666\n",
      "  ram_util_percent: 62.46666666666666\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1952422863618911\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14891920833533714\n",
      "  mean_inference_ms: 0.9809566401050356\n",
      "  mean_raw_obs_processing_ms: 0.12310696341408271\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -857.208392018151\n",
      "  episode_reward_mean: -1141.4988374583038\n",
      "  episode_reward_min: -1702.7208569094626\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1347.451978333189\n",
      "    - -1110.1113878368856\n",
      "    - -1102.2369240343567\n",
      "    - -1587.439736935298\n",
      "    - -881.2123158193687\n",
      "    - -951.2547567599943\n",
      "    - -962.3564574912177\n",
      "    - -1014.1714829157552\n",
      "    - -1504.117134531302\n",
      "    - -1521.0963114674678\n",
      "    - -1340.891746043265\n",
      "    - -883.5668364206782\n",
      "    - -1272.2336457356344\n",
      "    - -993.4355646892261\n",
      "    - -1079.5274494346038\n",
      "    - -979.1771198120581\n",
      "    - -1280.2591540284602\n",
      "    - -1080.8784572821403\n",
      "    - -969.383271580854\n",
      "    - -1419.708676425878\n",
      "    - -1076.1751256788643\n",
      "    - -1004.7517550754968\n",
      "    - -985.6094820031589\n",
      "    - -968.2827213933231\n",
      "    - -902.1706224365591\n",
      "    - -978.8048108215033\n",
      "    - -1456.9753433029198\n",
      "    - -1501.563701476143\n",
      "    - -1397.2092453843295\n",
      "    - -873.8295290982294\n",
      "    - -1164.7387688674353\n",
      "    - -899.0314864132923\n",
      "    - -912.4278992175357\n",
      "    - -1191.3077586123004\n",
      "    - -1307.2165117411982\n",
      "    - -1039.8281223024628\n",
      "    - -885.9697838907011\n",
      "    - -1088.6241211251058\n",
      "    - -962.6299366103474\n",
      "    - -857.208392018151\n",
      "    - -988.5719970362455\n",
      "    - -1286.8852636247786\n",
      "    - -1093.7825987051697\n",
      "    - -875.1851631670377\n",
      "    - -1003.2909317749776\n",
      "    - -861.3244529156381\n",
      "    - -1331.8443637620196\n",
      "    - -884.1750625376609\n",
      "    - -1193.8278794735993\n",
      "    - -981.3911873277757\n",
      "    - -1300.3402124311635\n",
      "    - -898.2819570535346\n",
      "    - -926.2689038901142\n",
      "    - -986.0884953075515\n",
      "    - -1011.5524254339776\n",
      "    - -1436.642381684932\n",
      "    - -977.283307323075\n",
      "    - -1415.6181518565563\n",
      "    - -1574.3675876831765\n",
      "    - -961.5399122741225\n",
      "    - -1587.179523428015\n",
      "    - -910.3598233336487\n",
      "    - -1702.7208569094626\n",
      "    - -925.3158968506144\n",
      "    - -986.7899533988259\n",
      "    - -974.1095790547112\n",
      "    - -1452.0109107454036\n",
      "    - -995.535974510868\n",
      "    - -968.0489209058104\n",
      "    - -1294.3156689426387\n",
      "    - -1413.8756314948298\n",
      "    - -1402.9400119473962\n",
      "    - -1081.787391634897\n",
      "    - -1378.802735814917\n",
      "    - -969.6279470944629\n",
      "    - -1222.773969523166\n",
      "    - -1550.6761067783793\n",
      "    - -1393.4813353353152\n",
      "    - -875.6963681313085\n",
      "    - -1177.8468180144737\n",
      "    - -1540.2192178121234\n",
      "    - -1393.1278470329783\n",
      "    - -900.0141134200541\n",
      "    - -1044.800206888546\n",
      "    - -870.2246429031647\n",
      "    - -896.1349063179822\n",
      "    - -1242.2645444245577\n",
      "    - -1168.7875995959735\n",
      "    - -967.4802550456973\n",
      "    - -1318.1573675627453\n",
      "    - -1059.1553708768797\n",
      "    - -1330.4269300781355\n",
      "    - -1375.7456694478922\n",
      "    - -1139.2353207403935\n",
      "    - -1097.0159433576614\n",
      "    - -1225.5019858065334\n",
      "    - -950.589995115292\n",
      "    - -1420.1370714955037\n",
      "    - -1183.0990618389815\n",
      "    - -1040.7425119103636\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1952422863618911\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14891920833533714\n",
      "    mean_inference_ms: 0.9809566401050356\n",
      "    mean_raw_obs_processing_ms: 0.12310696341408271\n",
      "time_since_restore: 187.0902760028839\n",
      "time_this_iter_s: 10.49585485458374\n",
      "time_total_s: 187.0902760028839\n",
      "timers:\n",
      "  learn_throughput: 908.263\n",
      "  learn_time_ms: 4404.009\n",
      "  load_throughput: 18238086.749\n",
      "  load_time_ms: 0.219\n",
      "  training_iteration_time_ms: 10045.068\n",
      "  update_time_ms: 2.264\n",
      "timestamp: 1660564105\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 72000\n",
      "training_iteration: 18\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 76000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 76000\n",
      "  num_agent_steps_trained: 76000\n",
      "  num_env_steps_sampled: 76000\n",
      "  num_env_steps_trained: 76000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-48-36\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -857.208392018151\n",
      "episode_reward_mean: -1124.9145295608284\n",
      "episode_reward_min: -1702.7208569094626\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 380\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.06328125298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9891545176506042\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011995486915111542\n",
      "        model: {}\n",
      "        policy_loss: 0.006478386931121349\n",
      "        total_loss: 9.86843490600586\n",
      "        vf_explained_var: -0.00680443225428462\n",
      "        vf_loss: 9.861197471618652\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 76000\n",
      "  num_agent_steps_trained: 76000\n",
      "  num_env_steps_sampled: 76000\n",
      "  num_env_steps_trained: 76000\n",
      "iterations_since_restore: 19\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 76000\n",
      "num_agent_steps_trained: 76000\n",
      "num_env_steps_sampled: 76000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 76000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.333333333333334\n",
      "  ram_util_percent: 59.10000000000001\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19477855377167422\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14865202529509286\n",
      "  mean_inference_ms: 0.9788667907335307\n",
      "  mean_raw_obs_processing_ms: 0.12290214858242829\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -857.208392018151\n",
      "  episode_reward_mean: -1124.9145295608284\n",
      "  episode_reward_min: -1702.7208569094626\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1076.1751256788643\n",
      "    - -1004.7517550754968\n",
      "    - -985.6094820031589\n",
      "    - -968.2827213933231\n",
      "    - -902.1706224365591\n",
      "    - -978.8048108215033\n",
      "    - -1456.9753433029198\n",
      "    - -1501.563701476143\n",
      "    - -1397.2092453843295\n",
      "    - -873.8295290982294\n",
      "    - -1164.7387688674353\n",
      "    - -899.0314864132923\n",
      "    - -912.4278992175357\n",
      "    - -1191.3077586123004\n",
      "    - -1307.2165117411982\n",
      "    - -1039.8281223024628\n",
      "    - -885.9697838907011\n",
      "    - -1088.6241211251058\n",
      "    - -962.6299366103474\n",
      "    - -857.208392018151\n",
      "    - -988.5719970362455\n",
      "    - -1286.8852636247786\n",
      "    - -1093.7825987051697\n",
      "    - -875.1851631670377\n",
      "    - -1003.2909317749776\n",
      "    - -861.3244529156381\n",
      "    - -1331.8443637620196\n",
      "    - -884.1750625376609\n",
      "    - -1193.8278794735993\n",
      "    - -981.3911873277757\n",
      "    - -1300.3402124311635\n",
      "    - -898.2819570535346\n",
      "    - -926.2689038901142\n",
      "    - -986.0884953075515\n",
      "    - -1011.5524254339776\n",
      "    - -1436.642381684932\n",
      "    - -977.283307323075\n",
      "    - -1415.6181518565563\n",
      "    - -1574.3675876831765\n",
      "    - -961.5399122741225\n",
      "    - -1587.179523428015\n",
      "    - -910.3598233336487\n",
      "    - -1702.7208569094626\n",
      "    - -925.3158968506144\n",
      "    - -986.7899533988259\n",
      "    - -974.1095790547112\n",
      "    - -1452.0109107454036\n",
      "    - -995.535974510868\n",
      "    - -968.0489209058104\n",
      "    - -1294.3156689426387\n",
      "    - -1413.8756314948298\n",
      "    - -1402.9400119473962\n",
      "    - -1081.787391634897\n",
      "    - -1378.802735814917\n",
      "    - -969.6279470944629\n",
      "    - -1222.773969523166\n",
      "    - -1550.6761067783793\n",
      "    - -1393.4813353353152\n",
      "    - -875.6963681313085\n",
      "    - -1177.8468180144737\n",
      "    - -1540.2192178121234\n",
      "    - -1393.1278470329783\n",
      "    - -900.0141134200541\n",
      "    - -1044.800206888546\n",
      "    - -870.2246429031647\n",
      "    - -896.1349063179822\n",
      "    - -1242.2645444245577\n",
      "    - -1168.7875995959735\n",
      "    - -967.4802550456973\n",
      "    - -1318.1573675627453\n",
      "    - -1059.1553708768797\n",
      "    - -1330.4269300781355\n",
      "    - -1375.7456694478922\n",
      "    - -1139.2353207403935\n",
      "    - -1097.0159433576614\n",
      "    - -1225.5019858065334\n",
      "    - -950.589995115292\n",
      "    - -1420.1370714955037\n",
      "    - -1183.0990618389815\n",
      "    - -1040.7425119103636\n",
      "    - -1085.6998308659674\n",
      "    - -1342.7072938802805\n",
      "    - -957.9968468435666\n",
      "    - -1013.5823450989546\n",
      "    - -1006.5994101269533\n",
      "    - -1024.5041694309027\n",
      "    - -895.8959698640383\n",
      "    - -1425.8091743356\n",
      "    - -1163.6526843976787\n",
      "    - -1007.4507694549621\n",
      "    - -948.9556874418773\n",
      "    - -994.2559122881232\n",
      "    - -1070.2033727370329\n",
      "    - -987.6569794167912\n",
      "    - -1078.163590059835\n",
      "    - -1337.8445184256082\n",
      "    - -1377.0454063535092\n",
      "    - -893.3905970421138\n",
      "    - -992.9272985762752\n",
      "    - -1017.7377611899818\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19477855377167422\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14865202529509286\n",
      "    mean_inference_ms: 0.9788667907335307\n",
      "    mean_raw_obs_processing_ms: 0.12290214858242829\n",
      "time_since_restore: 197.41484832763672\n",
      "time_this_iter_s: 10.324572324752808\n",
      "time_total_s: 197.41484832763672\n",
      "timers:\n",
      "  learn_throughput: 906.384\n",
      "  learn_time_ms: 4413.139\n",
      "  load_throughput: 18242052.843\n",
      "  load_time_ms: 0.219\n",
      "  training_iteration_time_ms: 10069.084\n",
      "  update_time_ms: 2.247\n",
      "timestamp: 1660564116\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 76000\n",
      "training_iteration: 19\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 80000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 80000\n",
      "  num_agent_steps_trained: 80000\n",
      "  num_env_steps_sampled: 80000\n",
      "  num_env_steps_trained: 80000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-48-46\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -847.6478283662893\n",
      "episode_reward_mean: -1133.8559809519677\n",
      "episode_reward_min: -1702.7208569094626\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 400\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.06328125298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1838299036026\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014553495682775974\n",
      "        model: {}\n",
      "        policy_loss: 0.007607128005474806\n",
      "        total_loss: 9.95294189453125\n",
      "        vf_explained_var: -0.008986967615783215\n",
      "        vf_loss: 9.944413185119629\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 80000\n",
      "  num_agent_steps_trained: 80000\n",
      "  num_env_steps_sampled: 80000\n",
      "  num_env_steps_trained: 80000\n",
      "iterations_since_restore: 20\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 80000\n",
      "num_agent_steps_trained: 80000\n",
      "num_env_steps_sampled: 80000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 80000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 23.571428571428573\n",
      "  ram_util_percent: 59.200000000000024\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19437173407025482\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14841653973852556\n",
      "  mean_inference_ms: 0.9769887339326309\n",
      "  mean_raw_obs_processing_ms: 0.1227170155019158\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -847.6478283662893\n",
      "  episode_reward_mean: -1133.8559809519677\n",
      "  episode_reward_min: -1702.7208569094626\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -988.5719970362455\n",
      "    - -1286.8852636247786\n",
      "    - -1093.7825987051697\n",
      "    - -875.1851631670377\n",
      "    - -1003.2909317749776\n",
      "    - -861.3244529156381\n",
      "    - -1331.8443637620196\n",
      "    - -884.1750625376609\n",
      "    - -1193.8278794735993\n",
      "    - -981.3911873277757\n",
      "    - -1300.3402124311635\n",
      "    - -898.2819570535346\n",
      "    - -926.2689038901142\n",
      "    - -986.0884953075515\n",
      "    - -1011.5524254339776\n",
      "    - -1436.642381684932\n",
      "    - -977.283307323075\n",
      "    - -1415.6181518565563\n",
      "    - -1574.3675876831765\n",
      "    - -961.5399122741225\n",
      "    - -1587.179523428015\n",
      "    - -910.3598233336487\n",
      "    - -1702.7208569094626\n",
      "    - -925.3158968506144\n",
      "    - -986.7899533988259\n",
      "    - -974.1095790547112\n",
      "    - -1452.0109107454036\n",
      "    - -995.535974510868\n",
      "    - -968.0489209058104\n",
      "    - -1294.3156689426387\n",
      "    - -1413.8756314948298\n",
      "    - -1402.9400119473962\n",
      "    - -1081.787391634897\n",
      "    - -1378.802735814917\n",
      "    - -969.6279470944629\n",
      "    - -1222.773969523166\n",
      "    - -1550.6761067783793\n",
      "    - -1393.4813353353152\n",
      "    - -875.6963681313085\n",
      "    - -1177.8468180144737\n",
      "    - -1540.2192178121234\n",
      "    - -1393.1278470329783\n",
      "    - -900.0141134200541\n",
      "    - -1044.800206888546\n",
      "    - -870.2246429031647\n",
      "    - -896.1349063179822\n",
      "    - -1242.2645444245577\n",
      "    - -1168.7875995959735\n",
      "    - -967.4802550456973\n",
      "    - -1318.1573675627453\n",
      "    - -1059.1553708768797\n",
      "    - -1330.4269300781355\n",
      "    - -1375.7456694478922\n",
      "    - -1139.2353207403935\n",
      "    - -1097.0159433576614\n",
      "    - -1225.5019858065334\n",
      "    - -950.589995115292\n",
      "    - -1420.1370714955037\n",
      "    - -1183.0990618389815\n",
      "    - -1040.7425119103636\n",
      "    - -1085.6998308659674\n",
      "    - -1342.7072938802805\n",
      "    - -957.9968468435666\n",
      "    - -1013.5823450989546\n",
      "    - -1006.5994101269533\n",
      "    - -1024.5041694309027\n",
      "    - -895.8959698640383\n",
      "    - -1425.8091743356\n",
      "    - -1163.6526843976787\n",
      "    - -1007.4507694549621\n",
      "    - -948.9556874418773\n",
      "    - -994.2559122881232\n",
      "    - -1070.2033727370329\n",
      "    - -987.6569794167912\n",
      "    - -1078.163590059835\n",
      "    - -1337.8445184256082\n",
      "    - -1377.0454063535092\n",
      "    - -893.3905970421138\n",
      "    - -992.9272985762752\n",
      "    - -1017.7377611899818\n",
      "    - -1600.502647160509\n",
      "    - -920.1270531850598\n",
      "    - -1103.0537747909298\n",
      "    - -878.8043326100911\n",
      "    - -1097.0288976002496\n",
      "    - -1013.2250739095526\n",
      "    - -1458.6149498478703\n",
      "    - -979.9222402707394\n",
      "    - -1623.2463900511002\n",
      "    - -1101.803956944889\n",
      "    - -1254.8879206096879\n",
      "    - -1385.2225708429348\n",
      "    - -891.6004812968386\n",
      "    - -1326.6042997546083\n",
      "    - -847.6478283662893\n",
      "    - -1083.1180910694638\n",
      "    - -874.9535458639625\n",
      "    - -983.4215534031199\n",
      "    - -994.1611804481286\n",
      "    - -930.5534685569801\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19437173407025482\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14841653973852556\n",
      "    mean_inference_ms: 0.9769887339326309\n",
      "    mean_raw_obs_processing_ms: 0.1227170155019158\n",
      "time_since_restore: 207.3363654613495\n",
      "time_this_iter_s: 9.921517133712769\n",
      "time_total_s: 207.3363654613495\n",
      "timers:\n",
      "  learn_throughput: 904.979\n",
      "  learn_time_ms: 4419.994\n",
      "  load_throughput: 18112076.001\n",
      "  load_time_ms: 0.221\n",
      "  training_iteration_time_ms: 10062.076\n",
      "  update_time_ms: 2.271\n",
      "timestamp: 1660564126\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 80000\n",
      "training_iteration: 20\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 84000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 84000\n",
      "  num_agent_steps_trained: 84000\n",
      "  num_env_steps_sampled: 84000\n",
      "  num_env_steps_trained: 84000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-48-56\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -835.4590350691417\n",
      "episode_reward_mean: -1133.0989245187945\n",
      "episode_reward_min: -1702.7208569094626\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 420\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.06328125298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1510658264160156\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010732757858932018\n",
      "        model: {}\n",
      "        policy_loss: 0.0080257598310709\n",
      "        total_loss: 9.890702247619629\n",
      "        vf_explained_var: -0.01423894427716732\n",
      "        vf_loss: 9.881998062133789\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 84000\n",
      "  num_agent_steps_trained: 84000\n",
      "  num_env_steps_sampled: 84000\n",
      "  num_env_steps_trained: 84000\n",
      "iterations_since_restore: 21\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 84000\n",
      "num_agent_steps_trained: 84000\n",
      "num_env_steps_sampled: 84000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 84000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.714285714285715\n",
      "  ram_util_percent: 59.12142857142858\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19400455412139428\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1482099051458764\n",
      "  mean_inference_ms: 0.9753217185741014\n",
      "  mean_raw_obs_processing_ms: 0.1225332397740498\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -835.4590350691417\n",
      "  episode_reward_mean: -1133.0989245187945\n",
      "  episode_reward_min: -1702.7208569094626\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1587.179523428015\n",
      "    - -910.3598233336487\n",
      "    - -1702.7208569094626\n",
      "    - -925.3158968506144\n",
      "    - -986.7899533988259\n",
      "    - -974.1095790547112\n",
      "    - -1452.0109107454036\n",
      "    - -995.535974510868\n",
      "    - -968.0489209058104\n",
      "    - -1294.3156689426387\n",
      "    - -1413.8756314948298\n",
      "    - -1402.9400119473962\n",
      "    - -1081.787391634897\n",
      "    - -1378.802735814917\n",
      "    - -969.6279470944629\n",
      "    - -1222.773969523166\n",
      "    - -1550.6761067783793\n",
      "    - -1393.4813353353152\n",
      "    - -875.6963681313085\n",
      "    - -1177.8468180144737\n",
      "    - -1540.2192178121234\n",
      "    - -1393.1278470329783\n",
      "    - -900.0141134200541\n",
      "    - -1044.800206888546\n",
      "    - -870.2246429031647\n",
      "    - -896.1349063179822\n",
      "    - -1242.2645444245577\n",
      "    - -1168.7875995959735\n",
      "    - -967.4802550456973\n",
      "    - -1318.1573675627453\n",
      "    - -1059.1553708768797\n",
      "    - -1330.4269300781355\n",
      "    - -1375.7456694478922\n",
      "    - -1139.2353207403935\n",
      "    - -1097.0159433576614\n",
      "    - -1225.5019858065334\n",
      "    - -950.589995115292\n",
      "    - -1420.1370714955037\n",
      "    - -1183.0990618389815\n",
      "    - -1040.7425119103636\n",
      "    - -1085.6998308659674\n",
      "    - -1342.7072938802805\n",
      "    - -957.9968468435666\n",
      "    - -1013.5823450989546\n",
      "    - -1006.5994101269533\n",
      "    - -1024.5041694309027\n",
      "    - -895.8959698640383\n",
      "    - -1425.8091743356\n",
      "    - -1163.6526843976787\n",
      "    - -1007.4507694549621\n",
      "    - -948.9556874418773\n",
      "    - -994.2559122881232\n",
      "    - -1070.2033727370329\n",
      "    - -987.6569794167912\n",
      "    - -1078.163590059835\n",
      "    - -1337.8445184256082\n",
      "    - -1377.0454063535092\n",
      "    - -893.3905970421138\n",
      "    - -992.9272985762752\n",
      "    - -1017.7377611899818\n",
      "    - -1600.502647160509\n",
      "    - -920.1270531850598\n",
      "    - -1103.0537747909298\n",
      "    - -878.8043326100911\n",
      "    - -1097.0288976002496\n",
      "    - -1013.2250739095526\n",
      "    - -1458.6149498478703\n",
      "    - -979.9222402707394\n",
      "    - -1623.2463900511002\n",
      "    - -1101.803956944889\n",
      "    - -1254.8879206096879\n",
      "    - -1385.2225708429348\n",
      "    - -891.6004812968386\n",
      "    - -1326.6042997546083\n",
      "    - -847.6478283662893\n",
      "    - -1083.1180910694638\n",
      "    - -874.9535458639625\n",
      "    - -983.4215534031199\n",
      "    - -994.1611804481286\n",
      "    - -930.5534685569801\n",
      "    - -1029.4697108024798\n",
      "    - -865.609232212612\n",
      "    - -861.2879464994082\n",
      "    - -936.8239175026669\n",
      "    - -1086.8890148901255\n",
      "    - -989.8220735250467\n",
      "    - -1081.7949166953485\n",
      "    - -1078.406658998231\n",
      "    - -1058.881883439496\n",
      "    - -909.8838406921733\n",
      "    - -1683.207659791216\n",
      "    - -1203.123732561156\n",
      "    - -985.9335239124783\n",
      "    - -1611.5611903401925\n",
      "    - -1025.4794789566968\n",
      "    - -835.4590350691417\n",
      "    - -944.7282289313509\n",
      "    - -1395.7722609594748\n",
      "    - -878.76076010729\n",
      "    - -1449.6615260592146\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19400455412139428\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1482099051458764\n",
      "    mean_inference_ms: 0.9753217185741014\n",
      "    mean_raw_obs_processing_ms: 0.1225332397740498\n",
      "time_since_restore: 217.38887310028076\n",
      "time_this_iter_s: 10.052507638931274\n",
      "time_total_s: 217.38887310028076\n",
      "timers:\n",
      "  learn_throughput: 904.057\n",
      "  learn_time_ms: 4424.499\n",
      "  load_throughput: 17964681.443\n",
      "  load_time_ms: 0.223\n",
      "  training_iteration_time_ms: 10076.365\n",
      "  update_time_ms: 2.288\n",
      "timestamp: 1660564136\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 84000\n",
      "training_iteration: 21\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 88000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 88000\n",
      "  num_agent_steps_trained: 88000\n",
      "  num_env_steps_sampled: 88000\n",
      "  num_env_steps_trained: 88000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-49-06\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -789.5139118836734\n",
      "episode_reward_mean: -1123.228659017899\n",
      "episode_reward_min: -1687.8854545397692\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 440\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.06328125298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3444836139678955\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010176103562116623\n",
      "        model: {}\n",
      "        policy_loss: 0.009732332080602646\n",
      "        total_loss: 9.919742584228516\n",
      "        vf_explained_var: -0.011843913234770298\n",
      "        vf_loss: 9.9093656539917\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 88000\n",
      "  num_agent_steps_trained: 88000\n",
      "  num_env_steps_sampled: 88000\n",
      "  num_env_steps_trained: 88000\n",
      "iterations_since_restore: 22\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 88000\n",
      "num_agent_steps_trained: 88000\n",
      "num_env_steps_sampled: 88000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 88000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 66.07857142857144\n",
      "  ram_util_percent: 59.10000000000001\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19365965576884858\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1480040918297069\n",
      "  mean_inference_ms: 0.9735770800003312\n",
      "  mean_raw_obs_processing_ms: 0.12234151379774286\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -789.5139118836734\n",
      "  episode_reward_mean: -1123.228659017899\n",
      "  episode_reward_min: -1687.8854545397692\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1540.2192178121234\n",
      "    - -1393.1278470329783\n",
      "    - -900.0141134200541\n",
      "    - -1044.800206888546\n",
      "    - -870.2246429031647\n",
      "    - -896.1349063179822\n",
      "    - -1242.2645444245577\n",
      "    - -1168.7875995959735\n",
      "    - -967.4802550456973\n",
      "    - -1318.1573675627453\n",
      "    - -1059.1553708768797\n",
      "    - -1330.4269300781355\n",
      "    - -1375.7456694478922\n",
      "    - -1139.2353207403935\n",
      "    - -1097.0159433576614\n",
      "    - -1225.5019858065334\n",
      "    - -950.589995115292\n",
      "    - -1420.1370714955037\n",
      "    - -1183.0990618389815\n",
      "    - -1040.7425119103636\n",
      "    - -1085.6998308659674\n",
      "    - -1342.7072938802805\n",
      "    - -957.9968468435666\n",
      "    - -1013.5823450989546\n",
      "    - -1006.5994101269533\n",
      "    - -1024.5041694309027\n",
      "    - -895.8959698640383\n",
      "    - -1425.8091743356\n",
      "    - -1163.6526843976787\n",
      "    - -1007.4507694549621\n",
      "    - -948.9556874418773\n",
      "    - -994.2559122881232\n",
      "    - -1070.2033727370329\n",
      "    - -987.6569794167912\n",
      "    - -1078.163590059835\n",
      "    - -1337.8445184256082\n",
      "    - -1377.0454063535092\n",
      "    - -893.3905970421138\n",
      "    - -992.9272985762752\n",
      "    - -1017.7377611899818\n",
      "    - -1600.502647160509\n",
      "    - -920.1270531850598\n",
      "    - -1103.0537747909298\n",
      "    - -878.8043326100911\n",
      "    - -1097.0288976002496\n",
      "    - -1013.2250739095526\n",
      "    - -1458.6149498478703\n",
      "    - -979.9222402707394\n",
      "    - -1623.2463900511002\n",
      "    - -1101.803956944889\n",
      "    - -1254.8879206096879\n",
      "    - -1385.2225708429348\n",
      "    - -891.6004812968386\n",
      "    - -1326.6042997546083\n",
      "    - -847.6478283662893\n",
      "    - -1083.1180910694638\n",
      "    - -874.9535458639625\n",
      "    - -983.4215534031199\n",
      "    - -994.1611804481286\n",
      "    - -930.5534685569801\n",
      "    - -1029.4697108024798\n",
      "    - -865.609232212612\n",
      "    - -861.2879464994082\n",
      "    - -936.8239175026669\n",
      "    - -1086.8890148901255\n",
      "    - -989.8220735250467\n",
      "    - -1081.7949166953485\n",
      "    - -1078.406658998231\n",
      "    - -1058.881883439496\n",
      "    - -909.8838406921733\n",
      "    - -1683.207659791216\n",
      "    - -1203.123732561156\n",
      "    - -985.9335239124783\n",
      "    - -1611.5611903401925\n",
      "    - -1025.4794789566968\n",
      "    - -835.4590350691417\n",
      "    - -944.7282289313509\n",
      "    - -1395.7722609594748\n",
      "    - -878.76076010729\n",
      "    - -1449.6615260592146\n",
      "    - -1273.321867938642\n",
      "    - -1071.1221157267478\n",
      "    - -789.5139118836734\n",
      "    - -1097.8734545172265\n",
      "    - -1404.2832401346393\n",
      "    - -1204.8597792005685\n",
      "    - -978.2318940336871\n",
      "    - -994.3172845333993\n",
      "    - -982.1896686184165\n",
      "    - -1687.8854545397692\n",
      "    - -886.3330954775327\n",
      "    - -1636.6399183325518\n",
      "    - -1021.4123132247197\n",
      "    - -1623.0268584653475\n",
      "    - -1535.2963362944356\n",
      "    - -1002.6443501228522\n",
      "    - -1173.2969030109903\n",
      "    - -879.0835501262806\n",
      "    - -1011.7079471132308\n",
      "    - -1023.8289304648433\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19365965576884858\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1480040918297069\n",
      "    mean_inference_ms: 0.9735770800003312\n",
      "    mean_raw_obs_processing_ms: 0.12234151379774286\n",
      "time_since_restore: 227.2929174900055\n",
      "time_this_iter_s: 9.904044389724731\n",
      "time_total_s: 227.2929174900055\n",
      "timers:\n",
      "  learn_throughput: 902.413\n",
      "  learn_time_ms: 4432.558\n",
      "  load_throughput: 17385716.062\n",
      "  load_time_ms: 0.23\n",
      "  training_iteration_time_ms: 10066.029\n",
      "  update_time_ms: 2.291\n",
      "timestamp: 1660564146\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 88000\n",
      "training_iteration: 22\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 92000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 92000\n",
      "  num_agent_steps_trained: 92000\n",
      "  num_env_steps_sampled: 92000\n",
      "  num_env_steps_trained: 92000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-49-16\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -722.7609864641385\n",
      "episode_reward_mean: -1101.8555810341088\n",
      "episode_reward_min: -1687.8854545397692\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 460\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.06328125298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9077150821685791\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009503569453954697\n",
      "        model: {}\n",
      "        policy_loss: 0.008280178532004356\n",
      "        total_loss: 9.916242599487305\n",
      "        vf_explained_var: -0.011790464632213116\n",
      "        vf_loss: 9.907361030578613\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 92000\n",
      "  num_agent_steps_trained: 92000\n",
      "  num_env_steps_sampled: 92000\n",
      "  num_env_steps_trained: 92000\n",
      "iterations_since_restore: 23\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 92000\n",
      "num_agent_steps_trained: 92000\n",
      "num_env_steps_sampled: 92000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 92000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.406666666666666\n",
      "  ram_util_percent: 59.14666666666668\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19336163494169753\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14784489596648773\n",
      "  mean_inference_ms: 0.9720414893219368\n",
      "  mean_raw_obs_processing_ms: 0.12216680095185577\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -722.7609864641385\n",
      "  episode_reward_mean: -1101.8555810341088\n",
      "  episode_reward_min: -1687.8854545397692\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1085.6998308659674\n",
      "    - -1342.7072938802805\n",
      "    - -957.9968468435666\n",
      "    - -1013.5823450989546\n",
      "    - -1006.5994101269533\n",
      "    - -1024.5041694309027\n",
      "    - -895.8959698640383\n",
      "    - -1425.8091743356\n",
      "    - -1163.6526843976787\n",
      "    - -1007.4507694549621\n",
      "    - -948.9556874418773\n",
      "    - -994.2559122881232\n",
      "    - -1070.2033727370329\n",
      "    - -987.6569794167912\n",
      "    - -1078.163590059835\n",
      "    - -1337.8445184256082\n",
      "    - -1377.0454063535092\n",
      "    - -893.3905970421138\n",
      "    - -992.9272985762752\n",
      "    - -1017.7377611899818\n",
      "    - -1600.502647160509\n",
      "    - -920.1270531850598\n",
      "    - -1103.0537747909298\n",
      "    - -878.8043326100911\n",
      "    - -1097.0288976002496\n",
      "    - -1013.2250739095526\n",
      "    - -1458.6149498478703\n",
      "    - -979.9222402707394\n",
      "    - -1623.2463900511002\n",
      "    - -1101.803956944889\n",
      "    - -1254.8879206096879\n",
      "    - -1385.2225708429348\n",
      "    - -891.6004812968386\n",
      "    - -1326.6042997546083\n",
      "    - -847.6478283662893\n",
      "    - -1083.1180910694638\n",
      "    - -874.9535458639625\n",
      "    - -983.4215534031199\n",
      "    - -994.1611804481286\n",
      "    - -930.5534685569801\n",
      "    - -1029.4697108024798\n",
      "    - -865.609232212612\n",
      "    - -861.2879464994082\n",
      "    - -936.8239175026669\n",
      "    - -1086.8890148901255\n",
      "    - -989.8220735250467\n",
      "    - -1081.7949166953485\n",
      "    - -1078.406658998231\n",
      "    - -1058.881883439496\n",
      "    - -909.8838406921733\n",
      "    - -1683.207659791216\n",
      "    - -1203.123732561156\n",
      "    - -985.9335239124783\n",
      "    - -1611.5611903401925\n",
      "    - -1025.4794789566968\n",
      "    - -835.4590350691417\n",
      "    - -944.7282289313509\n",
      "    - -1395.7722609594748\n",
      "    - -878.76076010729\n",
      "    - -1449.6615260592146\n",
      "    - -1273.321867938642\n",
      "    - -1071.1221157267478\n",
      "    - -789.5139118836734\n",
      "    - -1097.8734545172265\n",
      "    - -1404.2832401346393\n",
      "    - -1204.8597792005685\n",
      "    - -978.2318940336871\n",
      "    - -994.3172845333993\n",
      "    - -982.1896686184165\n",
      "    - -1687.8854545397692\n",
      "    - -886.3330954775327\n",
      "    - -1636.6399183325518\n",
      "    - -1021.4123132247197\n",
      "    - -1623.0268584653475\n",
      "    - -1535.2963362944356\n",
      "    - -1002.6443501228522\n",
      "    - -1173.2969030109903\n",
      "    - -879.0835501262806\n",
      "    - -1011.7079471132308\n",
      "    - -1023.8289304648433\n",
      "    - -1471.4881250687483\n",
      "    - -1199.9725629437667\n",
      "    - -867.7463658775225\n",
      "    - -1286.4089654196428\n",
      "    - -745.4497335377312\n",
      "    - -1063.8468372379862\n",
      "    - -1211.0795913584082\n",
      "    - -986.6181619830743\n",
      "    - -1142.3215977472885\n",
      "    - -999.482997010423\n",
      "    - -722.7609864641385\n",
      "    - -1652.8892249230823\n",
      "    - -1057.9518172370758\n",
      "    - -974.3910082090122\n",
      "    - -1085.301615909929\n",
      "    - -790.4125661107563\n",
      "    - -888.4161092487634\n",
      "    - -959.200999888916\n",
      "    - -971.3356023006022\n",
      "    - -948.477894815595\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19336163494169753\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14784489596648773\n",
      "    mean_inference_ms: 0.9720414893219368\n",
      "    mean_raw_obs_processing_ms: 0.12216680095185577\n",
      "time_since_restore: 237.61114382743835\n",
      "time_this_iter_s: 10.318226337432861\n",
      "time_total_s: 237.61114382743835\n",
      "timers:\n",
      "  learn_throughput: 896.758\n",
      "  learn_time_ms: 4460.511\n",
      "  load_throughput: 17225067.762\n",
      "  load_time_ms: 0.232\n",
      "  training_iteration_time_ms: 10096.746\n",
      "  update_time_ms: 2.31\n",
      "timestamp: 1660564156\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 92000\n",
      "training_iteration: 23\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 96000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 96000\n",
      "  num_agent_steps_trained: 96000\n",
      "  num_env_steps_sampled: 96000\n",
      "  num_env_steps_trained: 96000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-49-26\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -722.7609864641385\n",
      "episode_reward_mean: -1116.1586615537265\n",
      "episode_reward_min: -1775.75626084291\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 480\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.06328125298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3293278217315674\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02590802311897278\n",
      "        model: {}\n",
      "        policy_loss: 0.004478825721889734\n",
      "        total_loss: 9.897958755493164\n",
      "        vf_explained_var: -0.00819016620516777\n",
      "        vf_loss: 9.891840934753418\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 96000\n",
      "  num_agent_steps_trained: 96000\n",
      "  num_env_steps_sampled: 96000\n",
      "  num_env_steps_trained: 96000\n",
      "iterations_since_restore: 24\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 96000\n",
      "num_agent_steps_trained: 96000\n",
      "num_env_steps_sampled: 96000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 96000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 18.046666666666667\n",
      "  ram_util_percent: 59.200000000000024\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19304759690961085\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14765447117641473\n",
      "  mean_inference_ms: 0.9701424054678938\n",
      "  mean_raw_obs_processing_ms: 0.12195066340878723\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -722.7609864641385\n",
      "  episode_reward_mean: -1116.1586615537265\n",
      "  episode_reward_min: -1775.75626084291\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1600.502647160509\n",
      "    - -920.1270531850598\n",
      "    - -1103.0537747909298\n",
      "    - -878.8043326100911\n",
      "    - -1097.0288976002496\n",
      "    - -1013.2250739095526\n",
      "    - -1458.6149498478703\n",
      "    - -979.9222402707394\n",
      "    - -1623.2463900511002\n",
      "    - -1101.803956944889\n",
      "    - -1254.8879206096879\n",
      "    - -1385.2225708429348\n",
      "    - -891.6004812968386\n",
      "    - -1326.6042997546083\n",
      "    - -847.6478283662893\n",
      "    - -1083.1180910694638\n",
      "    - -874.9535458639625\n",
      "    - -983.4215534031199\n",
      "    - -994.1611804481286\n",
      "    - -930.5534685569801\n",
      "    - -1029.4697108024798\n",
      "    - -865.609232212612\n",
      "    - -861.2879464994082\n",
      "    - -936.8239175026669\n",
      "    - -1086.8890148901255\n",
      "    - -989.8220735250467\n",
      "    - -1081.7949166953485\n",
      "    - -1078.406658998231\n",
      "    - -1058.881883439496\n",
      "    - -909.8838406921733\n",
      "    - -1683.207659791216\n",
      "    - -1203.123732561156\n",
      "    - -985.9335239124783\n",
      "    - -1611.5611903401925\n",
      "    - -1025.4794789566968\n",
      "    - -835.4590350691417\n",
      "    - -944.7282289313509\n",
      "    - -1395.7722609594748\n",
      "    - -878.76076010729\n",
      "    - -1449.6615260592146\n",
      "    - -1273.321867938642\n",
      "    - -1071.1221157267478\n",
      "    - -789.5139118836734\n",
      "    - -1097.8734545172265\n",
      "    - -1404.2832401346393\n",
      "    - -1204.8597792005685\n",
      "    - -978.2318940336871\n",
      "    - -994.3172845333993\n",
      "    - -982.1896686184165\n",
      "    - -1687.8854545397692\n",
      "    - -886.3330954775327\n",
      "    - -1636.6399183325518\n",
      "    - -1021.4123132247197\n",
      "    - -1623.0268584653475\n",
      "    - -1535.2963362944356\n",
      "    - -1002.6443501228522\n",
      "    - -1173.2969030109903\n",
      "    - -879.0835501262806\n",
      "    - -1011.7079471132308\n",
      "    - -1023.8289304648433\n",
      "    - -1471.4881250687483\n",
      "    - -1199.9725629437667\n",
      "    - -867.7463658775225\n",
      "    - -1286.4089654196428\n",
      "    - -745.4497335377312\n",
      "    - -1063.8468372379862\n",
      "    - -1211.0795913584082\n",
      "    - -986.6181619830743\n",
      "    - -1142.3215977472885\n",
      "    - -999.482997010423\n",
      "    - -722.7609864641385\n",
      "    - -1652.8892249230823\n",
      "    - -1057.9518172370758\n",
      "    - -974.3910082090122\n",
      "    - -1085.301615909929\n",
      "    - -790.4125661107563\n",
      "    - -888.4161092487634\n",
      "    - -959.200999888916\n",
      "    - -971.3356023006022\n",
      "    - -948.477894815595\n",
      "    - -1414.8116143861496\n",
      "    - -1775.75626084291\n",
      "    - -1393.5631074093053\n",
      "    - -1464.3296903501239\n",
      "    - -1017.093867846842\n",
      "    - -976.2945869254194\n",
      "    - -948.7525840045976\n",
      "    - -1056.2761165070317\n",
      "    - -976.2055845408391\n",
      "    - -1157.4556093895899\n",
      "    - -854.6656859776938\n",
      "    - -1177.9873375295842\n",
      "    - -890.7209970825677\n",
      "    - -1249.5959191186744\n",
      "    - -1178.751920635069\n",
      "    - -862.4155298728107\n",
      "    - -951.6246905899309\n",
      "    - -979.1251667110453\n",
      "    - -1630.7963422988867\n",
      "    - -1096.1650577727407\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19304759690961085\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14765447117641473\n",
      "    mean_inference_ms: 0.9701424054678938\n",
      "    mean_raw_obs_processing_ms: 0.12195066340878723\n",
      "time_since_restore: 247.6137034893036\n",
      "time_this_iter_s: 10.002559661865234\n",
      "time_total_s: 247.6137034893036\n",
      "timers:\n",
      "  learn_throughput: 895.967\n",
      "  learn_time_ms: 4464.45\n",
      "  load_throughput: 17400141.05\n",
      "  load_time_ms: 0.23\n",
      "  training_iteration_time_ms: 10085.926\n",
      "  update_time_ms: 2.329\n",
      "timestamp: 1660564166\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 96000\n",
      "training_iteration: 24\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 100000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 100000\n",
      "  num_agent_steps_trained: 100000\n",
      "  num_env_steps_sampled: 100000\n",
      "  num_env_steps_trained: 100000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-49-37\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -722.7609864641385\n",
      "episode_reward_mean: -1129.9266904303445\n",
      "episode_reward_min: -1775.75626084291\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 500\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.09492187201976776\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4387363195419312\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010714656673371792\n",
      "        model: {}\n",
      "        policy_loss: 0.008915944956243038\n",
      "        total_loss: 9.935941696166992\n",
      "        vf_explained_var: -0.007722143549472094\n",
      "        vf_loss: 9.926009178161621\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 100000\n",
      "  num_agent_steps_trained: 100000\n",
      "  num_env_steps_sampled: 100000\n",
      "  num_env_steps_trained: 100000\n",
      "iterations_since_restore: 25\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 100000\n",
      "num_agent_steps_trained: 100000\n",
      "num_env_steps_sampled: 100000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 100000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 35.95333333333334\n",
      "  ram_util_percent: 59.246666666666684\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1928541597504355\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14760079256069036\n",
      "  mean_inference_ms: 0.9692276131463109\n",
      "  mean_raw_obs_processing_ms: 0.12185154654461687\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -722.7609864641385\n",
      "  episode_reward_mean: -1129.9266904303445\n",
      "  episode_reward_min: -1775.75626084291\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1029.4697108024798\n",
      "    - -865.609232212612\n",
      "    - -861.2879464994082\n",
      "    - -936.8239175026669\n",
      "    - -1086.8890148901255\n",
      "    - -989.8220735250467\n",
      "    - -1081.7949166953485\n",
      "    - -1078.406658998231\n",
      "    - -1058.881883439496\n",
      "    - -909.8838406921733\n",
      "    - -1683.207659791216\n",
      "    - -1203.123732561156\n",
      "    - -985.9335239124783\n",
      "    - -1611.5611903401925\n",
      "    - -1025.4794789566968\n",
      "    - -835.4590350691417\n",
      "    - -944.7282289313509\n",
      "    - -1395.7722609594748\n",
      "    - -878.76076010729\n",
      "    - -1449.6615260592146\n",
      "    - -1273.321867938642\n",
      "    - -1071.1221157267478\n",
      "    - -789.5139118836734\n",
      "    - -1097.8734545172265\n",
      "    - -1404.2832401346393\n",
      "    - -1204.8597792005685\n",
      "    - -978.2318940336871\n",
      "    - -994.3172845333993\n",
      "    - -982.1896686184165\n",
      "    - -1687.8854545397692\n",
      "    - -886.3330954775327\n",
      "    - -1636.6399183325518\n",
      "    - -1021.4123132247197\n",
      "    - -1623.0268584653475\n",
      "    - -1535.2963362944356\n",
      "    - -1002.6443501228522\n",
      "    - -1173.2969030109903\n",
      "    - -879.0835501262806\n",
      "    - -1011.7079471132308\n",
      "    - -1023.8289304648433\n",
      "    - -1471.4881250687483\n",
      "    - -1199.9725629437667\n",
      "    - -867.7463658775225\n",
      "    - -1286.4089654196428\n",
      "    - -745.4497335377312\n",
      "    - -1063.8468372379862\n",
      "    - -1211.0795913584082\n",
      "    - -986.6181619830743\n",
      "    - -1142.3215977472885\n",
      "    - -999.482997010423\n",
      "    - -722.7609864641385\n",
      "    - -1652.8892249230823\n",
      "    - -1057.9518172370758\n",
      "    - -974.3910082090122\n",
      "    - -1085.301615909929\n",
      "    - -790.4125661107563\n",
      "    - -888.4161092487634\n",
      "    - -959.200999888916\n",
      "    - -971.3356023006022\n",
      "    - -948.477894815595\n",
      "    - -1414.8116143861496\n",
      "    - -1775.75626084291\n",
      "    - -1393.5631074093053\n",
      "    - -1464.3296903501239\n",
      "    - -1017.093867846842\n",
      "    - -976.2945869254194\n",
      "    - -948.7525840045976\n",
      "    - -1056.2761165070317\n",
      "    - -976.2055845408391\n",
      "    - -1157.4556093895899\n",
      "    - -854.6656859776938\n",
      "    - -1177.9873375295842\n",
      "    - -890.7209970825677\n",
      "    - -1249.5959191186744\n",
      "    - -1178.751920635069\n",
      "    - -862.4155298728107\n",
      "    - -951.6246905899309\n",
      "    - -979.1251667110453\n",
      "    - -1630.7963422988867\n",
      "    - -1096.1650577727407\n",
      "    - -1059.7576602277286\n",
      "    - -1066.7168992886268\n",
      "    - -876.0278703181986\n",
      "    - -990.6929558090583\n",
      "    - -1033.6636512528955\n",
      "    - -976.5126814525834\n",
      "    - -1172.6006159378792\n",
      "    - -1253.1090697004538\n",
      "    - -1313.63741741504\n",
      "    - -1563.208174628455\n",
      "    - -1344.417030015005\n",
      "    - -1170.0491070897237\n",
      "    - -1181.024027594345\n",
      "    - -1026.401911523309\n",
      "    - -1736.2699366752167\n",
      "    - -862.6793843272686\n",
      "    - -1343.236141206932\n",
      "    - -1245.0301177149756\n",
      "    - -1210.9432463338403\n",
      "    - -1299.325245733258\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1928541597504355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14760079256069036\n",
      "    mean_inference_ms: 0.9692276131463109\n",
      "    mean_raw_obs_processing_ms: 0.12185154654461687\n",
      "time_since_restore: 258.2862968444824\n",
      "time_this_iter_s: 10.672593355178833\n",
      "time_total_s: 258.2862968444824\n",
      "timers:\n",
      "  learn_throughput: 890.516\n",
      "  learn_time_ms: 4491.776\n",
      "  load_throughput: 16566817.419\n",
      "  load_time_ms: 0.241\n",
      "  training_iteration_time_ms: 10163.427\n",
      "  update_time_ms: 2.374\n",
      "timestamp: 1660564177\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 100000\n",
      "training_iteration: 25\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 104000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 104000\n",
      "  num_agent_steps_trained: 104000\n",
      "  num_env_steps_sampled: 104000\n",
      "  num_env_steps_trained: 104000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-49-50\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -722.7609864641385\n",
      "episode_reward_mean: -1149.265649128904\n",
      "episode_reward_min: -1775.75626084291\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 520\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.09492187201976776\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5578845739364624\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007741848006844521\n",
      "        model: {}\n",
      "        policy_loss: 0.009520132094621658\n",
      "        total_loss: 9.876502990722656\n",
      "        vf_explained_var: -0.006469302345067263\n",
      "        vf_loss: 9.86624813079834\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 104000\n",
      "  num_agent_steps_trained: 104000\n",
      "  num_env_steps_sampled: 104000\n",
      "  num_env_steps_trained: 104000\n",
      "iterations_since_restore: 26\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 104000\n",
      "num_agent_steps_trained: 104000\n",
      "num_env_steps_sampled: 104000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 104000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 32.294444444444444\n",
      "  ram_util_percent: 59.25555555555556\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19278248205548956\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14759887899995738\n",
      "  mean_inference_ms: 0.9687895745252065\n",
      "  mean_raw_obs_processing_ms: 0.1217938899394342\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -722.7609864641385\n",
      "  episode_reward_mean: -1149.265649128904\n",
      "  episode_reward_min: -1775.75626084291\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1273.321867938642\n",
      "    - -1071.1221157267478\n",
      "    - -789.5139118836734\n",
      "    - -1097.8734545172265\n",
      "    - -1404.2832401346393\n",
      "    - -1204.8597792005685\n",
      "    - -978.2318940336871\n",
      "    - -994.3172845333993\n",
      "    - -982.1896686184165\n",
      "    - -1687.8854545397692\n",
      "    - -886.3330954775327\n",
      "    - -1636.6399183325518\n",
      "    - -1021.4123132247197\n",
      "    - -1623.0268584653475\n",
      "    - -1535.2963362944356\n",
      "    - -1002.6443501228522\n",
      "    - -1173.2969030109903\n",
      "    - -879.0835501262806\n",
      "    - -1011.7079471132308\n",
      "    - -1023.8289304648433\n",
      "    - -1471.4881250687483\n",
      "    - -1199.9725629437667\n",
      "    - -867.7463658775225\n",
      "    - -1286.4089654196428\n",
      "    - -745.4497335377312\n",
      "    - -1063.8468372379862\n",
      "    - -1211.0795913584082\n",
      "    - -986.6181619830743\n",
      "    - -1142.3215977472885\n",
      "    - -999.482997010423\n",
      "    - -722.7609864641385\n",
      "    - -1652.8892249230823\n",
      "    - -1057.9518172370758\n",
      "    - -974.3910082090122\n",
      "    - -1085.301615909929\n",
      "    - -790.4125661107563\n",
      "    - -888.4161092487634\n",
      "    - -959.200999888916\n",
      "    - -971.3356023006022\n",
      "    - -948.477894815595\n",
      "    - -1414.8116143861496\n",
      "    - -1775.75626084291\n",
      "    - -1393.5631074093053\n",
      "    - -1464.3296903501239\n",
      "    - -1017.093867846842\n",
      "    - -976.2945869254194\n",
      "    - -948.7525840045976\n",
      "    - -1056.2761165070317\n",
      "    - -976.2055845408391\n",
      "    - -1157.4556093895899\n",
      "    - -854.6656859776938\n",
      "    - -1177.9873375295842\n",
      "    - -890.7209970825677\n",
      "    - -1249.5959191186744\n",
      "    - -1178.751920635069\n",
      "    - -862.4155298728107\n",
      "    - -951.6246905899309\n",
      "    - -979.1251667110453\n",
      "    - -1630.7963422988867\n",
      "    - -1096.1650577727407\n",
      "    - -1059.7576602277286\n",
      "    - -1066.7168992886268\n",
      "    - -876.0278703181986\n",
      "    - -990.6929558090583\n",
      "    - -1033.6636512528955\n",
      "    - -976.5126814525834\n",
      "    - -1172.6006159378792\n",
      "    - -1253.1090697004538\n",
      "    - -1313.63741741504\n",
      "    - -1563.208174628455\n",
      "    - -1344.417030015005\n",
      "    - -1170.0491070897237\n",
      "    - -1181.024027594345\n",
      "    - -1026.401911523309\n",
      "    - -1736.2699366752167\n",
      "    - -862.6793843272686\n",
      "    - -1343.236141206932\n",
      "    - -1245.0301177149756\n",
      "    - -1210.9432463338403\n",
      "    - -1299.325245733258\n",
      "    - -1199.4266842251518\n",
      "    - -1582.5626374490282\n",
      "    - -992.7568123734692\n",
      "    - -1570.9132364130658\n",
      "    - -920.3000072769992\n",
      "    - -1013.3855554863451\n",
      "    - -905.2920769475568\n",
      "    - -936.9955802532209\n",
      "    - -1294.8227834035845\n",
      "    - -1459.6062969621262\n",
      "    - -1178.1228182106386\n",
      "    - -1158.3883427440974\n",
      "    - -1611.5246704035549\n",
      "    - -1273.1990275364021\n",
      "    - -969.9624352822077\n",
      "    - -963.4657114870772\n",
      "    - -1281.9300880558178\n",
      "    - -859.4541660867643\n",
      "    - -1485.2704999409095\n",
      "    - -1189.07303126377\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19278248205548956\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14759887899995738\n",
      "    mean_inference_ms: 0.9687895745252065\n",
      "    mean_raw_obs_processing_ms: 0.1217938899394342\n",
      "time_since_restore: 270.9604113101959\n",
      "time_this_iter_s: 12.674114465713501\n",
      "time_total_s: 270.9604113101959\n",
      "timers:\n",
      "  learn_throughput: 844.715\n",
      "  learn_time_ms: 4735.325\n",
      "  load_throughput: 16017964.483\n",
      "  load_time_ms: 0.25\n",
      "  training_iteration_time_ms: 10435.585\n",
      "  update_time_ms: 2.465\n",
      "timestamp: 1660564190\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 104000\n",
      "training_iteration: 26\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 108000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 108000\n",
      "  num_agent_steps_trained: 108000\n",
      "  num_env_steps_sampled: 108000\n",
      "  num_env_steps_trained: 108000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-50-01\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -722.7609864641385\n",
      "episode_reward_mean: -1149.2785288445668\n",
      "episode_reward_min: -1775.75626084291\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 540\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.09492187201976776\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4020788669586182\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010332257486879826\n",
      "        model: {}\n",
      "        policy_loss: 0.009469042532145977\n",
      "        total_loss: 9.950544357299805\n",
      "        vf_explained_var: -0.010102660395205021\n",
      "        vf_loss: 9.940093994140625\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 108000\n",
      "  num_agent_steps_trained: 108000\n",
      "  num_env_steps_sampled: 108000\n",
      "  num_env_steps_trained: 108000\n",
      "iterations_since_restore: 27\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 108000\n",
      "num_agent_steps_trained: 108000\n",
      "num_env_steps_sampled: 108000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 108000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 40.243750000000006\n",
      "  ram_util_percent: 59.4\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1929613644742418\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14780970081985378\n",
      "  mean_inference_ms: 0.9698672886083475\n",
      "  mean_raw_obs_processing_ms: 0.12191853295409913\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -722.7609864641385\n",
      "  episode_reward_mean: -1149.2785288445668\n",
      "  episode_reward_min: -1775.75626084291\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1471.4881250687483\n",
      "    - -1199.9725629437667\n",
      "    - -867.7463658775225\n",
      "    - -1286.4089654196428\n",
      "    - -745.4497335377312\n",
      "    - -1063.8468372379862\n",
      "    - -1211.0795913584082\n",
      "    - -986.6181619830743\n",
      "    - -1142.3215977472885\n",
      "    - -999.482997010423\n",
      "    - -722.7609864641385\n",
      "    - -1652.8892249230823\n",
      "    - -1057.9518172370758\n",
      "    - -974.3910082090122\n",
      "    - -1085.301615909929\n",
      "    - -790.4125661107563\n",
      "    - -888.4161092487634\n",
      "    - -959.200999888916\n",
      "    - -971.3356023006022\n",
      "    - -948.477894815595\n",
      "    - -1414.8116143861496\n",
      "    - -1775.75626084291\n",
      "    - -1393.5631074093053\n",
      "    - -1464.3296903501239\n",
      "    - -1017.093867846842\n",
      "    - -976.2945869254194\n",
      "    - -948.7525840045976\n",
      "    - -1056.2761165070317\n",
      "    - -976.2055845408391\n",
      "    - -1157.4556093895899\n",
      "    - -854.6656859776938\n",
      "    - -1177.9873375295842\n",
      "    - -890.7209970825677\n",
      "    - -1249.5959191186744\n",
      "    - -1178.751920635069\n",
      "    - -862.4155298728107\n",
      "    - -951.6246905899309\n",
      "    - -979.1251667110453\n",
      "    - -1630.7963422988867\n",
      "    - -1096.1650577727407\n",
      "    - -1059.7576602277286\n",
      "    - -1066.7168992886268\n",
      "    - -876.0278703181986\n",
      "    - -990.6929558090583\n",
      "    - -1033.6636512528955\n",
      "    - -976.5126814525834\n",
      "    - -1172.6006159378792\n",
      "    - -1253.1090697004538\n",
      "    - -1313.63741741504\n",
      "    - -1563.208174628455\n",
      "    - -1344.417030015005\n",
      "    - -1170.0491070897237\n",
      "    - -1181.024027594345\n",
      "    - -1026.401911523309\n",
      "    - -1736.2699366752167\n",
      "    - -862.6793843272686\n",
      "    - -1343.236141206932\n",
      "    - -1245.0301177149756\n",
      "    - -1210.9432463338403\n",
      "    - -1299.325245733258\n",
      "    - -1199.4266842251518\n",
      "    - -1582.5626374490282\n",
      "    - -992.7568123734692\n",
      "    - -1570.9132364130658\n",
      "    - -920.3000072769992\n",
      "    - -1013.3855554863451\n",
      "    - -905.2920769475568\n",
      "    - -936.9955802532209\n",
      "    - -1294.8227834035845\n",
      "    - -1459.6062969621262\n",
      "    - -1178.1228182106386\n",
      "    - -1158.3883427440974\n",
      "    - -1611.5246704035549\n",
      "    - -1273.1990275364021\n",
      "    - -969.9624352822077\n",
      "    - -963.4657114870772\n",
      "    - -1281.9300880558178\n",
      "    - -859.4541660867643\n",
      "    - -1485.2704999409095\n",
      "    - -1189.07303126377\n",
      "    - -1005.267046227263\n",
      "    - -1242.1026524018562\n",
      "    - -1149.4103457646736\n",
      "    - -991.2248670992836\n",
      "    - -1648.742457142285\n",
      "    - -891.1116195390675\n",
      "    - -854.866389749384\n",
      "    - -1482.6758515709876\n",
      "    - -1687.9215179060898\n",
      "    - -1469.206048263104\n",
      "    - -1175.9721039896708\n",
      "    - -983.3761041787033\n",
      "    - -885.2963511490045\n",
      "    - -911.3273227721421\n",
      "    - -968.854068976063\n",
      "    - -1208.5556880969255\n",
      "    - -1524.4319446671611\n",
      "    - -1263.4605761796804\n",
      "    - -871.171197083261\n",
      "    - -1063.1826925692314\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1929613644742418\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14780970081985378\n",
      "    mean_inference_ms: 0.9698672886083475\n",
      "    mean_raw_obs_processing_ms: 0.12191853295409913\n",
      "time_since_restore: 282.22213554382324\n",
      "time_this_iter_s: 11.26172423362732\n",
      "time_total_s: 282.22213554382324\n",
      "timers:\n",
      "  learn_throughput: 840.413\n",
      "  learn_time_ms: 4759.567\n",
      "  load_throughput: 16047074.127\n",
      "  load_time_ms: 0.249\n",
      "  training_iteration_time_ms: 10555.722\n",
      "  update_time_ms: 2.444\n",
      "timestamp: 1660564201\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 108000\n",
      "training_iteration: 27\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 112000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 112000\n",
      "  num_agent_steps_trained: 112000\n",
      "  num_env_steps_sampled: 112000\n",
      "  num_env_steps_trained: 112000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-50-11\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -813.5860564384373\n",
      "episode_reward_mean: -1149.2301223276936\n",
      "episode_reward_min: -1775.75626084291\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 560\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.09492187201976776\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8813755512237549\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010139938443899155\n",
      "        model: {}\n",
      "        policy_loss: 0.011712913401424885\n",
      "        total_loss: 9.876497268676758\n",
      "        vf_explained_var: -0.01407498400658369\n",
      "        vf_loss: 9.863821983337402\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 112000\n",
      "  num_agent_steps_trained: 112000\n",
      "  num_env_steps_sampled: 112000\n",
      "  num_env_steps_trained: 112000\n",
      "iterations_since_restore: 28\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 112000\n",
      "num_agent_steps_trained: 112000\n",
      "num_env_steps_sampled: 112000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 112000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 42.126666666666665\n",
      "  ram_util_percent: 59.299999999999976\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19314801022674843\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14800169331156232\n",
      "  mean_inference_ms: 0.9710054297818412\n",
      "  mean_raw_obs_processing_ms: 0.12203295359483313\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -813.5860564384373\n",
      "  episode_reward_mean: -1149.2301223276936\n",
      "  episode_reward_min: -1775.75626084291\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1414.8116143861496\n",
      "    - -1775.75626084291\n",
      "    - -1393.5631074093053\n",
      "    - -1464.3296903501239\n",
      "    - -1017.093867846842\n",
      "    - -976.2945869254194\n",
      "    - -948.7525840045976\n",
      "    - -1056.2761165070317\n",
      "    - -976.2055845408391\n",
      "    - -1157.4556093895899\n",
      "    - -854.6656859776938\n",
      "    - -1177.9873375295842\n",
      "    - -890.7209970825677\n",
      "    - -1249.5959191186744\n",
      "    - -1178.751920635069\n",
      "    - -862.4155298728107\n",
      "    - -951.6246905899309\n",
      "    - -979.1251667110453\n",
      "    - -1630.7963422988867\n",
      "    - -1096.1650577727407\n",
      "    - -1059.7576602277286\n",
      "    - -1066.7168992886268\n",
      "    - -876.0278703181986\n",
      "    - -990.6929558090583\n",
      "    - -1033.6636512528955\n",
      "    - -976.5126814525834\n",
      "    - -1172.6006159378792\n",
      "    - -1253.1090697004538\n",
      "    - -1313.63741741504\n",
      "    - -1563.208174628455\n",
      "    - -1344.417030015005\n",
      "    - -1170.0491070897237\n",
      "    - -1181.024027594345\n",
      "    - -1026.401911523309\n",
      "    - -1736.2699366752167\n",
      "    - -862.6793843272686\n",
      "    - -1343.236141206932\n",
      "    - -1245.0301177149756\n",
      "    - -1210.9432463338403\n",
      "    - -1299.325245733258\n",
      "    - -1199.4266842251518\n",
      "    - -1582.5626374490282\n",
      "    - -992.7568123734692\n",
      "    - -1570.9132364130658\n",
      "    - -920.3000072769992\n",
      "    - -1013.3855554863451\n",
      "    - -905.2920769475568\n",
      "    - -936.9955802532209\n",
      "    - -1294.8227834035845\n",
      "    - -1459.6062969621262\n",
      "    - -1178.1228182106386\n",
      "    - -1158.3883427440974\n",
      "    - -1611.5246704035549\n",
      "    - -1273.1990275364021\n",
      "    - -969.9624352822077\n",
      "    - -963.4657114870772\n",
      "    - -1281.9300880558178\n",
      "    - -859.4541660867643\n",
      "    - -1485.2704999409095\n",
      "    - -1189.07303126377\n",
      "    - -1005.267046227263\n",
      "    - -1242.1026524018562\n",
      "    - -1149.4103457646736\n",
      "    - -991.2248670992836\n",
      "    - -1648.742457142285\n",
      "    - -891.1116195390675\n",
      "    - -854.866389749384\n",
      "    - -1482.6758515709876\n",
      "    - -1687.9215179060898\n",
      "    - -1469.206048263104\n",
      "    - -1175.9721039896708\n",
      "    - -983.3761041787033\n",
      "    - -885.2963511490045\n",
      "    - -911.3273227721421\n",
      "    - -968.854068976063\n",
      "    - -1208.5556880969255\n",
      "    - -1524.4319446671611\n",
      "    - -1263.4605761796804\n",
      "    - -871.171197083261\n",
      "    - -1063.1826925692314\n",
      "    - -813.5860564384373\n",
      "    - -1008.5513017182773\n",
      "    - -1324.1304562227847\n",
      "    - -971.3745597900913\n",
      "    - -881.7265697880521\n",
      "    - -860.8766713923246\n",
      "    - -826.8025600061319\n",
      "    - -1514.8126611517662\n",
      "    - -1005.9393195262371\n",
      "    - -1443.818245835437\n",
      "    - -1281.3153957735133\n",
      "    - -883.2423225221725\n",
      "    - -1645.420124987545\n",
      "    - -877.1532464665455\n",
      "    - -913.3883011828193\n",
      "    - -901.1818782897587\n",
      "    - -877.8090986151878\n",
      "    - -837.1235125515975\n",
      "    - -1294.9488747760322\n",
      "    - -857.5109545704239\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19314801022674843\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14800169331156232\n",
      "    mean_inference_ms: 0.9710054297818412\n",
      "    mean_raw_obs_processing_ms: 0.12203295359483313\n",
      "time_since_restore: 292.42175364494324\n",
      "time_this_iter_s: 10.199618101119995\n",
      "time_total_s: 292.42175364494324\n",
      "timers:\n",
      "  learn_throughput: 847.008\n",
      "  learn_time_ms: 4722.504\n",
      "  load_throughput: 15963098.002\n",
      "  load_time_ms: 0.251\n",
      "  training_iteration_time_ms: 10526.056\n",
      "  update_time_ms: 2.444\n",
      "timestamp: 1660564211\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 112000\n",
      "training_iteration: 28\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 116000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 116000\n",
      "  num_agent_steps_trained: 116000\n",
      "  num_env_steps_sampled: 116000\n",
      "  num_env_steps_trained: 116000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-50-22\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -813.5860564384373\n",
      "episode_reward_mean: -1145.4275043455414\n",
      "episode_reward_min: -1740.6539243572597\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 580\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.09492187201976776\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2076284885406494\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009616109542548656\n",
      "        model: {}\n",
      "        policy_loss: 0.009474470280110836\n",
      "        total_loss: 9.87926959991455\n",
      "        vf_explained_var: -0.013681533746421337\n",
      "        vf_loss: 9.868882179260254\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 116000\n",
      "  num_agent_steps_trained: 116000\n",
      "  num_env_steps_sampled: 116000\n",
      "  num_env_steps_trained: 116000\n",
      "iterations_since_restore: 29\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 116000\n",
      "num_agent_steps_trained: 116000\n",
      "num_env_steps_sampled: 116000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 116000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.919999999999995\n",
      "  ram_util_percent: 59.31999999999998\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19344049938956864\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14826659306561196\n",
      "  mean_inference_ms: 0.9728564228614686\n",
      "  mean_raw_obs_processing_ms: 0.12221308458475398\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -813.5860564384373\n",
      "  episode_reward_mean: -1145.4275043455414\n",
      "  episode_reward_min: -1740.6539243572597\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1059.7576602277286\n",
      "    - -1066.7168992886268\n",
      "    - -876.0278703181986\n",
      "    - -990.6929558090583\n",
      "    - -1033.6636512528955\n",
      "    - -976.5126814525834\n",
      "    - -1172.6006159378792\n",
      "    - -1253.1090697004538\n",
      "    - -1313.63741741504\n",
      "    - -1563.208174628455\n",
      "    - -1344.417030015005\n",
      "    - -1170.0491070897237\n",
      "    - -1181.024027594345\n",
      "    - -1026.401911523309\n",
      "    - -1736.2699366752167\n",
      "    - -862.6793843272686\n",
      "    - -1343.236141206932\n",
      "    - -1245.0301177149756\n",
      "    - -1210.9432463338403\n",
      "    - -1299.325245733258\n",
      "    - -1199.4266842251518\n",
      "    - -1582.5626374490282\n",
      "    - -992.7568123734692\n",
      "    - -1570.9132364130658\n",
      "    - -920.3000072769992\n",
      "    - -1013.3855554863451\n",
      "    - -905.2920769475568\n",
      "    - -936.9955802532209\n",
      "    - -1294.8227834035845\n",
      "    - -1459.6062969621262\n",
      "    - -1178.1228182106386\n",
      "    - -1158.3883427440974\n",
      "    - -1611.5246704035549\n",
      "    - -1273.1990275364021\n",
      "    - -969.9624352822077\n",
      "    - -963.4657114870772\n",
      "    - -1281.9300880558178\n",
      "    - -859.4541660867643\n",
      "    - -1485.2704999409095\n",
      "    - -1189.07303126377\n",
      "    - -1005.267046227263\n",
      "    - -1242.1026524018562\n",
      "    - -1149.4103457646736\n",
      "    - -991.2248670992836\n",
      "    - -1648.742457142285\n",
      "    - -891.1116195390675\n",
      "    - -854.866389749384\n",
      "    - -1482.6758515709876\n",
      "    - -1687.9215179060898\n",
      "    - -1469.206048263104\n",
      "    - -1175.9721039896708\n",
      "    - -983.3761041787033\n",
      "    - -885.2963511490045\n",
      "    - -911.3273227721421\n",
      "    - -968.854068976063\n",
      "    - -1208.5556880969255\n",
      "    - -1524.4319446671611\n",
      "    - -1263.4605761796804\n",
      "    - -871.171197083261\n",
      "    - -1063.1826925692314\n",
      "    - -813.5860564384373\n",
      "    - -1008.5513017182773\n",
      "    - -1324.1304562227847\n",
      "    - -971.3745597900913\n",
      "    - -881.7265697880521\n",
      "    - -860.8766713923246\n",
      "    - -826.8025600061319\n",
      "    - -1514.8126611517662\n",
      "    - -1005.9393195262371\n",
      "    - -1443.818245835437\n",
      "    - -1281.3153957735133\n",
      "    - -883.2423225221725\n",
      "    - -1645.420124987545\n",
      "    - -877.1532464665455\n",
      "    - -913.3883011828193\n",
      "    - -901.1818782897587\n",
      "    - -877.8090986151878\n",
      "    - -837.1235125515975\n",
      "    - -1294.9488747760322\n",
      "    - -857.5109545704239\n",
      "    - -1507.9295190385399\n",
      "    - -877.6105024634173\n",
      "    - -975.5409854498964\n",
      "    - -979.1880725875911\n",
      "    - -1442.591185358202\n",
      "    - -1006.5098669345406\n",
      "    - -958.3567695740425\n",
      "    - -1535.1935537754407\n",
      "    - -1004.695684595138\n",
      "    - -1740.6539243572597\n",
      "    - -1085.9988944059594\n",
      "    - -991.8617866925935\n",
      "    - -896.1781412524113\n",
      "    - -888.9722164021985\n",
      "    - -959.1036888336982\n",
      "    - -989.8826930135158\n",
      "    - -1510.3501422569025\n",
      "    - -995.2113677159336\n",
      "    - -888.7740044676282\n",
      "    - -1437.5228724016613\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19344049938956864\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14826659306561196\n",
      "    mean_inference_ms: 0.9728564228614686\n",
      "    mean_raw_obs_processing_ms: 0.12221308458475398\n",
      "time_since_restore: 303.03685212135315\n",
      "time_this_iter_s: 10.615098476409912\n",
      "time_total_s: 303.03685212135315\n",
      "timers:\n",
      "  learn_throughput: 847.127\n",
      "  learn_time_ms: 4721.842\n",
      "  load_throughput: 16388801.407\n",
      "  load_time_ms: 0.244\n",
      "  training_iteration_time_ms: 10554.9\n",
      "  update_time_ms: 2.431\n",
      "timestamp: 1660564222\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 116000\n",
      "training_iteration: 29\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 120000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 120000\n",
      "  num_agent_steps_trained: 120000\n",
      "  num_env_steps_sampled: 120000\n",
      "  num_env_steps_trained: 120000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-50-34\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -780.8317872287796\n",
      "episode_reward_mean: -1147.064641214198\n",
      "episode_reward_min: -1740.6539243572597\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 600\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.09492187201976776\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5102208852767944\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.031625423580408096\n",
      "        model: {}\n",
      "        policy_loss: 0.001137687941081822\n",
      "        total_loss: 9.904989242553711\n",
      "        vf_explained_var: -0.007266613654792309\n",
      "        vf_loss: 9.900848388671875\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 120000\n",
      "  num_agent_steps_trained: 120000\n",
      "  num_env_steps_sampled: 120000\n",
      "  num_env_steps_trained: 120000\n",
      "iterations_since_restore: 30\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 120000\n",
      "num_agent_steps_trained: 120000\n",
      "num_env_steps_sampled: 120000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 120000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 63.98235294117647\n",
      "  ram_util_percent: 59.26470588235294\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19385157022212213\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1485754236544979\n",
      "  mean_inference_ms: 0.9752610779002657\n",
      "  mean_raw_obs_processing_ms: 0.12242798671178212\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -780.8317872287796\n",
      "  episode_reward_mean: -1147.064641214198\n",
      "  episode_reward_min: -1740.6539243572597\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1199.4266842251518\n",
      "    - -1582.5626374490282\n",
      "    - -992.7568123734692\n",
      "    - -1570.9132364130658\n",
      "    - -920.3000072769992\n",
      "    - -1013.3855554863451\n",
      "    - -905.2920769475568\n",
      "    - -936.9955802532209\n",
      "    - -1294.8227834035845\n",
      "    - -1459.6062969621262\n",
      "    - -1178.1228182106386\n",
      "    - -1158.3883427440974\n",
      "    - -1611.5246704035549\n",
      "    - -1273.1990275364021\n",
      "    - -969.9624352822077\n",
      "    - -963.4657114870772\n",
      "    - -1281.9300880558178\n",
      "    - -859.4541660867643\n",
      "    - -1485.2704999409095\n",
      "    - -1189.07303126377\n",
      "    - -1005.267046227263\n",
      "    - -1242.1026524018562\n",
      "    - -1149.4103457646736\n",
      "    - -991.2248670992836\n",
      "    - -1648.742457142285\n",
      "    - -891.1116195390675\n",
      "    - -854.866389749384\n",
      "    - -1482.6758515709876\n",
      "    - -1687.9215179060898\n",
      "    - -1469.206048263104\n",
      "    - -1175.9721039896708\n",
      "    - -983.3761041787033\n",
      "    - -885.2963511490045\n",
      "    - -911.3273227721421\n",
      "    - -968.854068976063\n",
      "    - -1208.5556880969255\n",
      "    - -1524.4319446671611\n",
      "    - -1263.4605761796804\n",
      "    - -871.171197083261\n",
      "    - -1063.1826925692314\n",
      "    - -813.5860564384373\n",
      "    - -1008.5513017182773\n",
      "    - -1324.1304562227847\n",
      "    - -971.3745597900913\n",
      "    - -881.7265697880521\n",
      "    - -860.8766713923246\n",
      "    - -826.8025600061319\n",
      "    - -1514.8126611517662\n",
      "    - -1005.9393195262371\n",
      "    - -1443.818245835437\n",
      "    - -1281.3153957735133\n",
      "    - -883.2423225221725\n",
      "    - -1645.420124987545\n",
      "    - -877.1532464665455\n",
      "    - -913.3883011828193\n",
      "    - -901.1818782897587\n",
      "    - -877.8090986151878\n",
      "    - -837.1235125515975\n",
      "    - -1294.9488747760322\n",
      "    - -857.5109545704239\n",
      "    - -1507.9295190385399\n",
      "    - -877.6105024634173\n",
      "    - -975.5409854498964\n",
      "    - -979.1880725875911\n",
      "    - -1442.591185358202\n",
      "    - -1006.5098669345406\n",
      "    - -958.3567695740425\n",
      "    - -1535.1935537754407\n",
      "    - -1004.695684595138\n",
      "    - -1740.6539243572597\n",
      "    - -1085.9988944059594\n",
      "    - -991.8617866925935\n",
      "    - -896.1781412524113\n",
      "    - -888.9722164021985\n",
      "    - -959.1036888336982\n",
      "    - -989.8826930135158\n",
      "    - -1510.3501422569025\n",
      "    - -995.2113677159336\n",
      "    - -888.7740044676282\n",
      "    - -1437.5228724016613\n",
      "    - -996.619414712036\n",
      "    - -1073.3756067262889\n",
      "    - -780.8317872287796\n",
      "    - -1011.6735859803536\n",
      "    - -1452.11741612302\n",
      "    - -1445.4078666672324\n",
      "    - -1439.739848567501\n",
      "    - -1631.0024966107394\n",
      "    - -1024.0264008504678\n",
      "    - -1394.6509380706027\n",
      "    - -882.9155398280967\n",
      "    - -1183.806444632708\n",
      "    - -908.2384347758588\n",
      "    - -995.6517663896725\n",
      "    - -1082.2847270373445\n",
      "    - -1627.977115809104\n",
      "    - -1373.4706340403195\n",
      "    - -883.2125552500944\n",
      "    - -1099.8823838731566\n",
      "    - -1602.1318679370927\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19385157022212213\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1485754236544979\n",
      "    mean_inference_ms: 0.9752610779002657\n",
      "    mean_raw_obs_processing_ms: 0.12242798671178212\n",
      "time_since_restore: 315.19039154052734\n",
      "time_this_iter_s: 12.153539419174194\n",
      "time_total_s: 315.19039154052734\n",
      "timers:\n",
      "  learn_throughput: 827.393\n",
      "  learn_time_ms: 4834.464\n",
      "  load_throughput: 16153683.805\n",
      "  load_time_ms: 0.248\n",
      "  training_iteration_time_ms: 10778.304\n",
      "  update_time_ms: 2.43\n",
      "timestamp: 1660564234\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 120000\n",
      "training_iteration: 30\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 124000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 124000\n",
      "  num_agent_steps_trained: 124000\n",
      "  num_env_steps_sampled: 124000\n",
      "  num_env_steps_trained: 124000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-50-46\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -780.8317872287796\n",
      "episode_reward_mean: -1123.6727291325428\n",
      "episode_reward_min: -1740.6539243572597\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 620\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.14238281548023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7927539348602295\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.034342680126428604\n",
      "        model: {}\n",
      "        policy_loss: 0.0037515058647841215\n",
      "        total_loss: 9.937199592590332\n",
      "        vf_explained_var: -0.007687270175665617\n",
      "        vf_loss: 9.928559303283691\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 124000\n",
      "  num_agent_steps_trained: 124000\n",
      "  num_env_steps_sampled: 124000\n",
      "  num_env_steps_trained: 124000\n",
      "iterations_since_restore: 31\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 124000\n",
      "num_agent_steps_trained: 124000\n",
      "num_env_steps_sampled: 124000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 124000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.84117647058823\n",
      "  ram_util_percent: 59.12941176470588\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19431121427805537\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14893910096764126\n",
      "  mean_inference_ms: 0.9780772022303647\n",
      "  mean_raw_obs_processing_ms: 0.12269614064409408\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -780.8317872287796\n",
      "  episode_reward_mean: -1123.6727291325428\n",
      "  episode_reward_min: -1740.6539243572597\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1005.267046227263\n",
      "    - -1242.1026524018562\n",
      "    - -1149.4103457646736\n",
      "    - -991.2248670992836\n",
      "    - -1648.742457142285\n",
      "    - -891.1116195390675\n",
      "    - -854.866389749384\n",
      "    - -1482.6758515709876\n",
      "    - -1687.9215179060898\n",
      "    - -1469.206048263104\n",
      "    - -1175.9721039896708\n",
      "    - -983.3761041787033\n",
      "    - -885.2963511490045\n",
      "    - -911.3273227721421\n",
      "    - -968.854068976063\n",
      "    - -1208.5556880969255\n",
      "    - -1524.4319446671611\n",
      "    - -1263.4605761796804\n",
      "    - -871.171197083261\n",
      "    - -1063.1826925692314\n",
      "    - -813.5860564384373\n",
      "    - -1008.5513017182773\n",
      "    - -1324.1304562227847\n",
      "    - -971.3745597900913\n",
      "    - -881.7265697880521\n",
      "    - -860.8766713923246\n",
      "    - -826.8025600061319\n",
      "    - -1514.8126611517662\n",
      "    - -1005.9393195262371\n",
      "    - -1443.818245835437\n",
      "    - -1281.3153957735133\n",
      "    - -883.2423225221725\n",
      "    - -1645.420124987545\n",
      "    - -877.1532464665455\n",
      "    - -913.3883011828193\n",
      "    - -901.1818782897587\n",
      "    - -877.8090986151878\n",
      "    - -837.1235125515975\n",
      "    - -1294.9488747760322\n",
      "    - -857.5109545704239\n",
      "    - -1507.9295190385399\n",
      "    - -877.6105024634173\n",
      "    - -975.5409854498964\n",
      "    - -979.1880725875911\n",
      "    - -1442.591185358202\n",
      "    - -1006.5098669345406\n",
      "    - -958.3567695740425\n",
      "    - -1535.1935537754407\n",
      "    - -1004.695684595138\n",
      "    - -1740.6539243572597\n",
      "    - -1085.9988944059594\n",
      "    - -991.8617866925935\n",
      "    - -896.1781412524113\n",
      "    - -888.9722164021985\n",
      "    - -959.1036888336982\n",
      "    - -989.8826930135158\n",
      "    - -1510.3501422569025\n",
      "    - -995.2113677159336\n",
      "    - -888.7740044676282\n",
      "    - -1437.5228724016613\n",
      "    - -996.619414712036\n",
      "    - -1073.3756067262889\n",
      "    - -780.8317872287796\n",
      "    - -1011.6735859803536\n",
      "    - -1452.11741612302\n",
      "    - -1445.4078666672324\n",
      "    - -1439.739848567501\n",
      "    - -1631.0024966107394\n",
      "    - -1024.0264008504678\n",
      "    - -1394.6509380706027\n",
      "    - -882.9155398280967\n",
      "    - -1183.806444632708\n",
      "    - -908.2384347758588\n",
      "    - -995.6517663896725\n",
      "    - -1082.2847270373445\n",
      "    - -1627.977115809104\n",
      "    - -1373.4706340403195\n",
      "    - -883.2125552500944\n",
      "    - -1099.8823838731566\n",
      "    - -1602.1318679370927\n",
      "    - -885.430195764021\n",
      "    - -1029.2019471557664\n",
      "    - -861.1104334150384\n",
      "    - -1057.0944652214646\n",
      "    - -1725.0637737831676\n",
      "    - -976.6657897467871\n",
      "    - -1188.6695797274854\n",
      "    - -1012.6586079202972\n",
      "    - -984.5787557384594\n",
      "    - -906.7307953175992\n",
      "    - -913.431278122273\n",
      "    - -1595.08096421102\n",
      "    - -1164.0437794466686\n",
      "    - -1030.4367375287425\n",
      "    - -901.7622882913153\n",
      "    - -1089.1967710064346\n",
      "    - -972.244728220468\n",
      "    - -1194.3507102972926\n",
      "    - -1012.1315560499941\n",
      "    - -1007.378096671978\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19431121427805537\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14893910096764126\n",
      "    mean_inference_ms: 0.9780772022303647\n",
      "    mean_raw_obs_processing_ms: 0.12269614064409408\n",
      "time_since_restore: 326.75623655319214\n",
      "time_this_iter_s: 11.565845012664795\n",
      "time_total_s: 326.75623655319214\n",
      "timers:\n",
      "  learn_throughput: 814.446\n",
      "  learn_time_ms: 4911.311\n",
      "  load_throughput: 15238161.671\n",
      "  load_time_ms: 0.262\n",
      "  training_iteration_time_ms: 10929.983\n",
      "  update_time_ms: 2.422\n",
      "timestamp: 1660564246\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 124000\n",
      "training_iteration: 31\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 128000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 128000\n",
      "  num_agent_steps_trained: 128000\n",
      "  num_env_steps_sampled: 128000\n",
      "  num_env_steps_trained: 128000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-50-56\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -780.8317872287796\n",
      "episode_reward_mean: -1113.1034355362353\n",
      "episode_reward_min: -1740.6539243572597\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 640\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.21357421576976776\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1024818420410156\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02355579473078251\n",
      "        model: {}\n",
      "        policy_loss: 0.016218770295381546\n",
      "        total_loss: 9.918216705322266\n",
      "        vf_explained_var: -0.008605560287833214\n",
      "        vf_loss: 9.896965980529785\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 128000\n",
      "  num_agent_steps_trained: 128000\n",
      "  num_env_steps_sampled: 128000\n",
      "  num_env_steps_trained: 128000\n",
      "iterations_since_restore: 32\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 128000\n",
      "num_agent_steps_trained: 128000\n",
      "num_env_steps_sampled: 128000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 128000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 21.42\n",
      "  ram_util_percent: 59.0\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1946388532870874\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14918430502594676\n",
      "  mean_inference_ms: 0.9800746296222641\n",
      "  mean_raw_obs_processing_ms: 0.12286644711365366\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -780.8317872287796\n",
      "  episode_reward_mean: -1113.1034355362353\n",
      "  episode_reward_min: -1740.6539243572597\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -813.5860564384373\n",
      "    - -1008.5513017182773\n",
      "    - -1324.1304562227847\n",
      "    - -971.3745597900913\n",
      "    - -881.7265697880521\n",
      "    - -860.8766713923246\n",
      "    - -826.8025600061319\n",
      "    - -1514.8126611517662\n",
      "    - -1005.9393195262371\n",
      "    - -1443.818245835437\n",
      "    - -1281.3153957735133\n",
      "    - -883.2423225221725\n",
      "    - -1645.420124987545\n",
      "    - -877.1532464665455\n",
      "    - -913.3883011828193\n",
      "    - -901.1818782897587\n",
      "    - -877.8090986151878\n",
      "    - -837.1235125515975\n",
      "    - -1294.9488747760322\n",
      "    - -857.5109545704239\n",
      "    - -1507.9295190385399\n",
      "    - -877.6105024634173\n",
      "    - -975.5409854498964\n",
      "    - -979.1880725875911\n",
      "    - -1442.591185358202\n",
      "    - -1006.5098669345406\n",
      "    - -958.3567695740425\n",
      "    - -1535.1935537754407\n",
      "    - -1004.695684595138\n",
      "    - -1740.6539243572597\n",
      "    - -1085.9988944059594\n",
      "    - -991.8617866925935\n",
      "    - -896.1781412524113\n",
      "    - -888.9722164021985\n",
      "    - -959.1036888336982\n",
      "    - -989.8826930135158\n",
      "    - -1510.3501422569025\n",
      "    - -995.2113677159336\n",
      "    - -888.7740044676282\n",
      "    - -1437.5228724016613\n",
      "    - -996.619414712036\n",
      "    - -1073.3756067262889\n",
      "    - -780.8317872287796\n",
      "    - -1011.6735859803536\n",
      "    - -1452.11741612302\n",
      "    - -1445.4078666672324\n",
      "    - -1439.739848567501\n",
      "    - -1631.0024966107394\n",
      "    - -1024.0264008504678\n",
      "    - -1394.6509380706027\n",
      "    - -882.9155398280967\n",
      "    - -1183.806444632708\n",
      "    - -908.2384347758588\n",
      "    - -995.6517663896725\n",
      "    - -1082.2847270373445\n",
      "    - -1627.977115809104\n",
      "    - -1373.4706340403195\n",
      "    - -883.2125552500944\n",
      "    - -1099.8823838731566\n",
      "    - -1602.1318679370927\n",
      "    - -885.430195764021\n",
      "    - -1029.2019471557664\n",
      "    - -861.1104334150384\n",
      "    - -1057.0944652214646\n",
      "    - -1725.0637737831676\n",
      "    - -976.6657897467871\n",
      "    - -1188.6695797274854\n",
      "    - -1012.6586079202972\n",
      "    - -984.5787557384594\n",
      "    - -906.7307953175992\n",
      "    - -913.431278122273\n",
      "    - -1595.08096421102\n",
      "    - -1164.0437794466686\n",
      "    - -1030.4367375287425\n",
      "    - -901.7622882913153\n",
      "    - -1089.1967710064346\n",
      "    - -972.244728220468\n",
      "    - -1194.3507102972926\n",
      "    - -1012.1315560499941\n",
      "    - -1007.378096671978\n",
      "    - -1505.9291487798714\n",
      "    - -872.1708058156445\n",
      "    - -1006.0535205587216\n",
      "    - -909.3832038527361\n",
      "    - -886.6697304208279\n",
      "    - -881.0933428364148\n",
      "    - -881.7919967171007\n",
      "    - -1529.2182678244624\n",
      "    - -1278.591378978881\n",
      "    - -1267.5591280246954\n",
      "    - -1029.9371419556255\n",
      "    - -1002.4549455957566\n",
      "    - -1668.682901510566\n",
      "    - -986.2605532288942\n",
      "    - -1016.0080884408824\n",
      "    - -1334.979016669282\n",
      "    - -1017.2763794828124\n",
      "    - -1368.1043553643547\n",
      "    - -798.3965353889316\n",
      "    - -980.6670442486266\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1946388532870874\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14918430502594676\n",
      "    mean_inference_ms: 0.9800746296222641\n",
      "    mean_raw_obs_processing_ms: 0.12286644711365366\n",
      "time_since_restore: 337.5177249908447\n",
      "time_this_iter_s: 10.761488437652588\n",
      "time_total_s: 337.5177249908447\n",
      "timers:\n",
      "  learn_throughput: 809.67\n",
      "  learn_time_ms: 4940.283\n",
      "  load_throughput: 15413152.044\n",
      "  load_time_ms: 0.26\n",
      "  training_iteration_time_ms: 11015.859\n",
      "  update_time_ms: 2.39\n",
      "timestamp: 1660564256\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 128000\n",
      "training_iteration: 32\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 132000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 132000\n",
      "  num_agent_steps_trained: 132000\n",
      "  num_env_steps_sampled: 132000\n",
      "  num_env_steps_trained: 132000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-51-07\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -780.8317872287796\n",
      "episode_reward_mean: -1125.6384668622582\n",
      "episode_reward_min: -1765.0628201154793\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 660\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0849779844284058\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007374154403805733\n",
      "        model: {}\n",
      "        policy_loss: 0.009705545380711555\n",
      "        total_loss: 9.859086990356445\n",
      "        vf_explained_var: -0.01691172830760479\n",
      "        vf_loss: 9.847020149230957\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 132000\n",
      "  num_agent_steps_trained: 132000\n",
      "  num_env_steps_sampled: 132000\n",
      "  num_env_steps_trained: 132000\n",
      "iterations_since_restore: 33\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 132000\n",
      "num_agent_steps_trained: 132000\n",
      "num_env_steps_sampled: 132000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 132000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.20625\n",
      "  ram_util_percent: 59.0625\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19497503194248617\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1494337505503274\n",
      "  mean_inference_ms: 0.9821528271740503\n",
      "  mean_raw_obs_processing_ms: 0.12305550629610548\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -780.8317872287796\n",
      "  episode_reward_mean: -1125.6384668622582\n",
      "  episode_reward_min: -1765.0628201154793\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1507.9295190385399\n",
      "    - -877.6105024634173\n",
      "    - -975.5409854498964\n",
      "    - -979.1880725875911\n",
      "    - -1442.591185358202\n",
      "    - -1006.5098669345406\n",
      "    - -958.3567695740425\n",
      "    - -1535.1935537754407\n",
      "    - -1004.695684595138\n",
      "    - -1740.6539243572597\n",
      "    - -1085.9988944059594\n",
      "    - -991.8617866925935\n",
      "    - -896.1781412524113\n",
      "    - -888.9722164021985\n",
      "    - -959.1036888336982\n",
      "    - -989.8826930135158\n",
      "    - -1510.3501422569025\n",
      "    - -995.2113677159336\n",
      "    - -888.7740044676282\n",
      "    - -1437.5228724016613\n",
      "    - -996.619414712036\n",
      "    - -1073.3756067262889\n",
      "    - -780.8317872287796\n",
      "    - -1011.6735859803536\n",
      "    - -1452.11741612302\n",
      "    - -1445.4078666672324\n",
      "    - -1439.739848567501\n",
      "    - -1631.0024966107394\n",
      "    - -1024.0264008504678\n",
      "    - -1394.6509380706027\n",
      "    - -882.9155398280967\n",
      "    - -1183.806444632708\n",
      "    - -908.2384347758588\n",
      "    - -995.6517663896725\n",
      "    - -1082.2847270373445\n",
      "    - -1627.977115809104\n",
      "    - -1373.4706340403195\n",
      "    - -883.2125552500944\n",
      "    - -1099.8823838731566\n",
      "    - -1602.1318679370927\n",
      "    - -885.430195764021\n",
      "    - -1029.2019471557664\n",
      "    - -861.1104334150384\n",
      "    - -1057.0944652214646\n",
      "    - -1725.0637737831676\n",
      "    - -976.6657897467871\n",
      "    - -1188.6695797274854\n",
      "    - -1012.6586079202972\n",
      "    - -984.5787557384594\n",
      "    - -906.7307953175992\n",
      "    - -913.431278122273\n",
      "    - -1595.08096421102\n",
      "    - -1164.0437794466686\n",
      "    - -1030.4367375287425\n",
      "    - -901.7622882913153\n",
      "    - -1089.1967710064346\n",
      "    - -972.244728220468\n",
      "    - -1194.3507102972926\n",
      "    - -1012.1315560499941\n",
      "    - -1007.378096671978\n",
      "    - -1505.9291487798714\n",
      "    - -872.1708058156445\n",
      "    - -1006.0535205587216\n",
      "    - -909.3832038527361\n",
      "    - -886.6697304208279\n",
      "    - -881.0933428364148\n",
      "    - -881.7919967171007\n",
      "    - -1529.2182678244624\n",
      "    - -1278.591378978881\n",
      "    - -1267.5591280246954\n",
      "    - -1029.9371419556255\n",
      "    - -1002.4549455957566\n",
      "    - -1668.682901510566\n",
      "    - -986.2605532288942\n",
      "    - -1016.0080884408824\n",
      "    - -1334.979016669282\n",
      "    - -1017.2763794828124\n",
      "    - -1368.1043553643547\n",
      "    - -798.3965353889316\n",
      "    - -980.6670442486266\n",
      "    - -893.3578122465531\n",
      "    - -1306.9624691227436\n",
      "    - -1725.780514371527\n",
      "    - -893.8688536579447\n",
      "    - -1541.4533675677828\n",
      "    - -983.8763778369893\n",
      "    - -905.6528718252496\n",
      "    - -1265.1545264870463\n",
      "    - -952.6007384760917\n",
      "    - -1006.5932901741991\n",
      "    - -938.0654904089603\n",
      "    - -895.2851823192104\n",
      "    - -910.5812295234608\n",
      "    - -1184.2795133751324\n",
      "    - -955.8082999759598\n",
      "    - -1187.6247147018246\n",
      "    - -1765.0628201154793\n",
      "    - -880.2844384992947\n",
      "    - -983.5728074998909\n",
      "    - -1098.3499260220779\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19497503194248617\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1494337505503274\n",
      "    mean_inference_ms: 0.9821528271740503\n",
      "    mean_raw_obs_processing_ms: 0.12305550629610548\n",
      "time_since_restore: 348.3097219467163\n",
      "time_this_iter_s: 10.791996955871582\n",
      "time_total_s: 348.3097219467163\n",
      "timers:\n",
      "  learn_throughput: 804.236\n",
      "  learn_time_ms: 4973.667\n",
      "  load_throughput: 15068453.386\n",
      "  load_time_ms: 0.265\n",
      "  training_iteration_time_ms: 11063.278\n",
      "  update_time_ms: 2.35\n",
      "timestamp: 1660564267\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 132000\n",
      "training_iteration: 33\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 136000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 136000\n",
      "  num_agent_steps_trained: 136000\n",
      "  num_env_steps_sampled: 136000\n",
      "  num_env_steps_trained: 136000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-51-17\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -780.8317872287796\n",
      "episode_reward_mean: -1147.626958071576\n",
      "episode_reward_min: -1765.0628201154793\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 680\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.905149221420288\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021893305703997612\n",
      "        model: {}\n",
      "        policy_loss: 0.016946373507380486\n",
      "        total_loss: 9.888867378234863\n",
      "        vf_explained_var: -0.005538145080208778\n",
      "        vf_loss: 9.864906311035156\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 136000\n",
      "  num_agent_steps_trained: 136000\n",
      "  num_env_steps_sampled: 136000\n",
      "  num_env_steps_trained: 136000\n",
      "iterations_since_restore: 34\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 136000\n",
      "num_agent_steps_trained: 136000\n",
      "num_env_steps_sampled: 136000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 136000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 32.142857142857146\n",
      "  ram_util_percent: 59.035714285714285\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19517360812390258\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14957553325099082\n",
      "  mean_inference_ms: 0.983438644214455\n",
      "  mean_raw_obs_processing_ms: 0.12315239796885644\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -780.8317872287796\n",
      "  episode_reward_mean: -1147.626958071576\n",
      "  episode_reward_min: -1765.0628201154793\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -996.619414712036\n",
      "    - -1073.3756067262889\n",
      "    - -780.8317872287796\n",
      "    - -1011.6735859803536\n",
      "    - -1452.11741612302\n",
      "    - -1445.4078666672324\n",
      "    - -1439.739848567501\n",
      "    - -1631.0024966107394\n",
      "    - -1024.0264008504678\n",
      "    - -1394.6509380706027\n",
      "    - -882.9155398280967\n",
      "    - -1183.806444632708\n",
      "    - -908.2384347758588\n",
      "    - -995.6517663896725\n",
      "    - -1082.2847270373445\n",
      "    - -1627.977115809104\n",
      "    - -1373.4706340403195\n",
      "    - -883.2125552500944\n",
      "    - -1099.8823838731566\n",
      "    - -1602.1318679370927\n",
      "    - -885.430195764021\n",
      "    - -1029.2019471557664\n",
      "    - -861.1104334150384\n",
      "    - -1057.0944652214646\n",
      "    - -1725.0637737831676\n",
      "    - -976.6657897467871\n",
      "    - -1188.6695797274854\n",
      "    - -1012.6586079202972\n",
      "    - -984.5787557384594\n",
      "    - -906.7307953175992\n",
      "    - -913.431278122273\n",
      "    - -1595.08096421102\n",
      "    - -1164.0437794466686\n",
      "    - -1030.4367375287425\n",
      "    - -901.7622882913153\n",
      "    - -1089.1967710064346\n",
      "    - -972.244728220468\n",
      "    - -1194.3507102972926\n",
      "    - -1012.1315560499941\n",
      "    - -1007.378096671978\n",
      "    - -1505.9291487798714\n",
      "    - -872.1708058156445\n",
      "    - -1006.0535205587216\n",
      "    - -909.3832038527361\n",
      "    - -886.6697304208279\n",
      "    - -881.0933428364148\n",
      "    - -881.7919967171007\n",
      "    - -1529.2182678244624\n",
      "    - -1278.591378978881\n",
      "    - -1267.5591280246954\n",
      "    - -1029.9371419556255\n",
      "    - -1002.4549455957566\n",
      "    - -1668.682901510566\n",
      "    - -986.2605532288942\n",
      "    - -1016.0080884408824\n",
      "    - -1334.979016669282\n",
      "    - -1017.2763794828124\n",
      "    - -1368.1043553643547\n",
      "    - -798.3965353889316\n",
      "    - -980.6670442486266\n",
      "    - -893.3578122465531\n",
      "    - -1306.9624691227436\n",
      "    - -1725.780514371527\n",
      "    - -893.8688536579447\n",
      "    - -1541.4533675677828\n",
      "    - -983.8763778369893\n",
      "    - -905.6528718252496\n",
      "    - -1265.1545264870463\n",
      "    - -952.6007384760917\n",
      "    - -1006.5932901741991\n",
      "    - -938.0654904089603\n",
      "    - -895.2851823192104\n",
      "    - -910.5812295234608\n",
      "    - -1184.2795133751324\n",
      "    - -955.8082999759598\n",
      "    - -1187.6247147018246\n",
      "    - -1765.0628201154793\n",
      "    - -880.2844384992947\n",
      "    - -983.5728074998909\n",
      "    - -1098.3499260220779\n",
      "    - -1618.7386446580447\n",
      "    - -1587.8237445591355\n",
      "    - -1491.7197131161004\n",
      "    - -894.2916165205366\n",
      "    - -904.7479471849475\n",
      "    - -873.9184601611355\n",
      "    - -1048.4934820941453\n",
      "    - -1713.110280775326\n",
      "    - -967.5054738521139\n",
      "    - -1015.454334381352\n",
      "    - -914.3689930668883\n",
      "    - -1559.2619123087995\n",
      "    - -1161.4609222762176\n",
      "    - -1009.7776534685086\n",
      "    - -1613.393282471002\n",
      "    - -1623.1927932952758\n",
      "    - -1154.3933220749357\n",
      "    - -1286.052663946972\n",
      "    - -1085.8502899786956\n",
      "    - -1347.4194623182088\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19517360812390258\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14957553325099082\n",
      "    mean_inference_ms: 0.983438644214455\n",
      "    mean_raw_obs_processing_ms: 0.12315239796885644\n",
      "time_since_restore: 358.1716637611389\n",
      "time_this_iter_s: 9.861941814422607\n",
      "time_total_s: 358.1716637611389\n",
      "timers:\n",
      "  learn_throughput: 804.762\n",
      "  learn_time_ms: 4970.415\n",
      "  load_throughput: 14925020.906\n",
      "  load_time_ms: 0.268\n",
      "  training_iteration_time_ms: 11049.108\n",
      "  update_time_ms: 2.338\n",
      "timestamp: 1660564277\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 136000\n",
      "training_iteration: 34\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 140000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 140000\n",
      "  num_agent_steps_trained: 140000\n",
      "  num_env_steps_sampled: 140000\n",
      "  num_env_steps_trained: 140000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-51-27\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -763.8439210480203\n",
      "episode_reward_mean: -1134.7942770767704\n",
      "episode_reward_min: -1765.0628201154793\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 700\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.48054200410842896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1522226333618164\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012571119703352451\n",
      "        model: {}\n",
      "        policy_loss: 0.004053638782352209\n",
      "        total_loss: 9.896564483642578\n",
      "        vf_explained_var: -0.021335095167160034\n",
      "        vf_loss: 9.886468887329102\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 140000\n",
      "  num_agent_steps_trained: 140000\n",
      "  num_env_steps_sampled: 140000\n",
      "  num_env_steps_trained: 140000\n",
      "iterations_since_restore: 35\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 140000\n",
      "num_agent_steps_trained: 140000\n",
      "num_env_steps_sampled: 140000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 140000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.353846153846156\n",
      "  ram_util_percent: 59.00769230769231\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19511507802038136\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14955079687468814\n",
      "  mean_inference_ms: 0.9833637733294751\n",
      "  mean_raw_obs_processing_ms: 0.12311309322080007\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -763.8439210480203\n",
      "  episode_reward_mean: -1134.7942770767704\n",
      "  episode_reward_min: -1765.0628201154793\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -885.430195764021\n",
      "    - -1029.2019471557664\n",
      "    - -861.1104334150384\n",
      "    - -1057.0944652214646\n",
      "    - -1725.0637737831676\n",
      "    - -976.6657897467871\n",
      "    - -1188.6695797274854\n",
      "    - -1012.6586079202972\n",
      "    - -984.5787557384594\n",
      "    - -906.7307953175992\n",
      "    - -913.431278122273\n",
      "    - -1595.08096421102\n",
      "    - -1164.0437794466686\n",
      "    - -1030.4367375287425\n",
      "    - -901.7622882913153\n",
      "    - -1089.1967710064346\n",
      "    - -972.244728220468\n",
      "    - -1194.3507102972926\n",
      "    - -1012.1315560499941\n",
      "    - -1007.378096671978\n",
      "    - -1505.9291487798714\n",
      "    - -872.1708058156445\n",
      "    - -1006.0535205587216\n",
      "    - -909.3832038527361\n",
      "    - -886.6697304208279\n",
      "    - -881.0933428364148\n",
      "    - -881.7919967171007\n",
      "    - -1529.2182678244624\n",
      "    - -1278.591378978881\n",
      "    - -1267.5591280246954\n",
      "    - -1029.9371419556255\n",
      "    - -1002.4549455957566\n",
      "    - -1668.682901510566\n",
      "    - -986.2605532288942\n",
      "    - -1016.0080884408824\n",
      "    - -1334.979016669282\n",
      "    - -1017.2763794828124\n",
      "    - -1368.1043553643547\n",
      "    - -798.3965353889316\n",
      "    - -980.6670442486266\n",
      "    - -893.3578122465531\n",
      "    - -1306.9624691227436\n",
      "    - -1725.780514371527\n",
      "    - -893.8688536579447\n",
      "    - -1541.4533675677828\n",
      "    - -983.8763778369893\n",
      "    - -905.6528718252496\n",
      "    - -1265.1545264870463\n",
      "    - -952.6007384760917\n",
      "    - -1006.5932901741991\n",
      "    - -938.0654904089603\n",
      "    - -895.2851823192104\n",
      "    - -910.5812295234608\n",
      "    - -1184.2795133751324\n",
      "    - -955.8082999759598\n",
      "    - -1187.6247147018246\n",
      "    - -1765.0628201154793\n",
      "    - -880.2844384992947\n",
      "    - -983.5728074998909\n",
      "    - -1098.3499260220779\n",
      "    - -1618.7386446580447\n",
      "    - -1587.8237445591355\n",
      "    - -1491.7197131161004\n",
      "    - -894.2916165205366\n",
      "    - -904.7479471849475\n",
      "    - -873.9184601611355\n",
      "    - -1048.4934820941453\n",
      "    - -1713.110280775326\n",
      "    - -967.5054738521139\n",
      "    - -1015.454334381352\n",
      "    - -914.3689930668883\n",
      "    - -1559.2619123087995\n",
      "    - -1161.4609222762176\n",
      "    - -1009.7776534685086\n",
      "    - -1613.393282471002\n",
      "    - -1623.1927932952758\n",
      "    - -1154.3933220749357\n",
      "    - -1286.052663946972\n",
      "    - -1085.8502899786956\n",
      "    - -1347.4194623182088\n",
      "    - -996.4739988223664\n",
      "    - -894.2875924591823\n",
      "    - -882.938594292758\n",
      "    - -1431.0036790442432\n",
      "    - -1257.9783318284917\n",
      "    - -1339.6426842407516\n",
      "    - -900.5417984109486\n",
      "    - -1627.0248712060143\n",
      "    - -890.2728595353485\n",
      "    - -1003.8138094262573\n",
      "    - -1098.2667614235884\n",
      "    - -1023.2617747255795\n",
      "    - -763.8439210480203\n",
      "    - -1243.0352953729914\n",
      "    - -1008.6855761946198\n",
      "    - -911.5451914475241\n",
      "    - -1106.566559838001\n",
      "    - -1228.6165514912386\n",
      "    - -1440.5957478072664\n",
      "    - -1557.3531330147268\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19511507802038136\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14955079687468814\n",
      "    mean_inference_ms: 0.9833637733294751\n",
      "    mean_raw_obs_processing_ms: 0.12311309322080007\n",
      "time_since_restore: 367.57223534584045\n",
      "time_this_iter_s: 9.400571584701538\n",
      "time_total_s: 367.57223534584045\n",
      "timers:\n",
      "  learn_throughput: 815.168\n",
      "  learn_time_ms: 4906.962\n",
      "  load_throughput: 15512913.546\n",
      "  load_time_ms: 0.258\n",
      "  training_iteration_time_ms: 10921.978\n",
      "  update_time_ms: 2.32\n",
      "timestamp: 1660564287\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 140000\n",
      "training_iteration: 35\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 144000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 144000\n",
      "  num_agent_steps_trained: 144000\n",
      "  num_env_steps_sampled: 144000\n",
      "  num_env_steps_trained: 144000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-51-36\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -763.8439210480203\n",
      "episode_reward_mean: -1141.890951271069\n",
      "episode_reward_min: -1765.0628201154793\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 720\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.48054200410842896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9219083189964294\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004923403728753328\n",
      "        model: {}\n",
      "        policy_loss: 0.010603779926896095\n",
      "        total_loss: 9.886655807495117\n",
      "        vf_explained_var: -0.02355620451271534\n",
      "        vf_loss: 9.873686790466309\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 144000\n",
      "  num_agent_steps_trained: 144000\n",
      "  num_env_steps_sampled: 144000\n",
      "  num_env_steps_trained: 144000\n",
      "iterations_since_restore: 36\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 144000\n",
      "num_agent_steps_trained: 144000\n",
      "num_env_steps_sampled: 144000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 144000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.528571428571425\n",
      "  ram_util_percent: 58.95714285714285\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19485219449685684\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14937323047203466\n",
      "  mean_inference_ms: 0.9821904307529001\n",
      "  mean_raw_obs_processing_ms: 0.1229455618081781\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -763.8439210480203\n",
      "  episode_reward_mean: -1141.890951271069\n",
      "  episode_reward_min: -1765.0628201154793\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1505.9291487798714\n",
      "    - -872.1708058156445\n",
      "    - -1006.0535205587216\n",
      "    - -909.3832038527361\n",
      "    - -886.6697304208279\n",
      "    - -881.0933428364148\n",
      "    - -881.7919967171007\n",
      "    - -1529.2182678244624\n",
      "    - -1278.591378978881\n",
      "    - -1267.5591280246954\n",
      "    - -1029.9371419556255\n",
      "    - -1002.4549455957566\n",
      "    - -1668.682901510566\n",
      "    - -986.2605532288942\n",
      "    - -1016.0080884408824\n",
      "    - -1334.979016669282\n",
      "    - -1017.2763794828124\n",
      "    - -1368.1043553643547\n",
      "    - -798.3965353889316\n",
      "    - -980.6670442486266\n",
      "    - -893.3578122465531\n",
      "    - -1306.9624691227436\n",
      "    - -1725.780514371527\n",
      "    - -893.8688536579447\n",
      "    - -1541.4533675677828\n",
      "    - -983.8763778369893\n",
      "    - -905.6528718252496\n",
      "    - -1265.1545264870463\n",
      "    - -952.6007384760917\n",
      "    - -1006.5932901741991\n",
      "    - -938.0654904089603\n",
      "    - -895.2851823192104\n",
      "    - -910.5812295234608\n",
      "    - -1184.2795133751324\n",
      "    - -955.8082999759598\n",
      "    - -1187.6247147018246\n",
      "    - -1765.0628201154793\n",
      "    - -880.2844384992947\n",
      "    - -983.5728074998909\n",
      "    - -1098.3499260220779\n",
      "    - -1618.7386446580447\n",
      "    - -1587.8237445591355\n",
      "    - -1491.7197131161004\n",
      "    - -894.2916165205366\n",
      "    - -904.7479471849475\n",
      "    - -873.9184601611355\n",
      "    - -1048.4934820941453\n",
      "    - -1713.110280775326\n",
      "    - -967.5054738521139\n",
      "    - -1015.454334381352\n",
      "    - -914.3689930668883\n",
      "    - -1559.2619123087995\n",
      "    - -1161.4609222762176\n",
      "    - -1009.7776534685086\n",
      "    - -1613.393282471002\n",
      "    - -1623.1927932952758\n",
      "    - -1154.3933220749357\n",
      "    - -1286.052663946972\n",
      "    - -1085.8502899786956\n",
      "    - -1347.4194623182088\n",
      "    - -996.4739988223664\n",
      "    - -894.2875924591823\n",
      "    - -882.938594292758\n",
      "    - -1431.0036790442432\n",
      "    - -1257.9783318284917\n",
      "    - -1339.6426842407516\n",
      "    - -900.5417984109486\n",
      "    - -1627.0248712060143\n",
      "    - -890.2728595353485\n",
      "    - -1003.8138094262573\n",
      "    - -1098.2667614235884\n",
      "    - -1023.2617747255795\n",
      "    - -763.8439210480203\n",
      "    - -1243.0352953729914\n",
      "    - -1008.6855761946198\n",
      "    - -911.5451914475241\n",
      "    - -1106.566559838001\n",
      "    - -1228.6165514912386\n",
      "    - -1440.5957478072664\n",
      "    - -1557.3531330147268\n",
      "    - -1399.4276665572406\n",
      "    - -1469.5907787791573\n",
      "    - -936.4966445935714\n",
      "    - -985.930161647759\n",
      "    - -1013.6478188412814\n",
      "    - -993.3068066495853\n",
      "    - -1118.324432553549\n",
      "    - -933.1434825821538\n",
      "    - -1180.6232001545432\n",
      "    - -898.4671003596978\n",
      "    - -1075.021782840154\n",
      "    - -1239.5698363434487\n",
      "    - -1105.7634598378738\n",
      "    - -1025.7862729720312\n",
      "    - -1065.520721906349\n",
      "    - -1121.7330937903273\n",
      "    - -904.5490482102094\n",
      "    - -1275.3195666335027\n",
      "    - -897.3407994037107\n",
      "    - -1577.3659984099722\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19485219449685684\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14937323047203466\n",
      "    mean_inference_ms: 0.9821904307529001\n",
      "    mean_raw_obs_processing_ms: 0.1229455618081781\n",
      "time_since_restore: 377.0101566314697\n",
      "time_this_iter_s: 9.437921285629272\n",
      "time_total_s: 377.0101566314697\n",
      "timers:\n",
      "  learn_throughput: 860.983\n",
      "  learn_time_ms: 4645.85\n",
      "  load_throughput: 15731097.984\n",
      "  load_time_ms: 0.254\n",
      "  training_iteration_time_ms: 10598.937\n",
      "  update_time_ms: 2.233\n",
      "timestamp: 1660564296\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 144000\n",
      "training_iteration: 36\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 148000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 148000\n",
      "  num_agent_steps_trained: 148000\n",
      "  num_env_steps_sampled: 148000\n",
      "  num_env_steps_trained: 148000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-51-47\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -763.8439210480203\n",
      "episode_reward_mean: -1130.531100014952\n",
      "episode_reward_min: -1765.0628201154793\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 740\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.24027100205421448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7528976798057556\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01237628422677517\n",
      "        model: {}\n",
      "        policy_loss: 0.013169866055250168\n",
      "        total_loss: 9.861966133117676\n",
      "        vf_explained_var: -0.014819128438830376\n",
      "        vf_loss: 9.845823287963867\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 148000\n",
      "  num_agent_steps_trained: 148000\n",
      "  num_env_steps_sampled: 148000\n",
      "  num_env_steps_trained: 148000\n",
      "iterations_since_restore: 37\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 148000\n",
      "num_agent_steps_trained: 148000\n",
      "num_env_steps_sampled: 148000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 148000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.04666666666667\n",
      "  ram_util_percent: 58.96\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19444300988621507\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14911238869269197\n",
      "  mean_inference_ms: 0.9803989467890717\n",
      "  mean_raw_obs_processing_ms: 0.12271508309698037\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -763.8439210480203\n",
      "  episode_reward_mean: -1130.531100014952\n",
      "  episode_reward_min: -1765.0628201154793\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -893.3578122465531\n",
      "    - -1306.9624691227436\n",
      "    - -1725.780514371527\n",
      "    - -893.8688536579447\n",
      "    - -1541.4533675677828\n",
      "    - -983.8763778369893\n",
      "    - -905.6528718252496\n",
      "    - -1265.1545264870463\n",
      "    - -952.6007384760917\n",
      "    - -1006.5932901741991\n",
      "    - -938.0654904089603\n",
      "    - -895.2851823192104\n",
      "    - -910.5812295234608\n",
      "    - -1184.2795133751324\n",
      "    - -955.8082999759598\n",
      "    - -1187.6247147018246\n",
      "    - -1765.0628201154793\n",
      "    - -880.2844384992947\n",
      "    - -983.5728074998909\n",
      "    - -1098.3499260220779\n",
      "    - -1618.7386446580447\n",
      "    - -1587.8237445591355\n",
      "    - -1491.7197131161004\n",
      "    - -894.2916165205366\n",
      "    - -904.7479471849475\n",
      "    - -873.9184601611355\n",
      "    - -1048.4934820941453\n",
      "    - -1713.110280775326\n",
      "    - -967.5054738521139\n",
      "    - -1015.454334381352\n",
      "    - -914.3689930668883\n",
      "    - -1559.2619123087995\n",
      "    - -1161.4609222762176\n",
      "    - -1009.7776534685086\n",
      "    - -1613.393282471002\n",
      "    - -1623.1927932952758\n",
      "    - -1154.3933220749357\n",
      "    - -1286.052663946972\n",
      "    - -1085.8502899786956\n",
      "    - -1347.4194623182088\n",
      "    - -996.4739988223664\n",
      "    - -894.2875924591823\n",
      "    - -882.938594292758\n",
      "    - -1431.0036790442432\n",
      "    - -1257.9783318284917\n",
      "    - -1339.6426842407516\n",
      "    - -900.5417984109486\n",
      "    - -1627.0248712060143\n",
      "    - -890.2728595353485\n",
      "    - -1003.8138094262573\n",
      "    - -1098.2667614235884\n",
      "    - -1023.2617747255795\n",
      "    - -763.8439210480203\n",
      "    - -1243.0352953729914\n",
      "    - -1008.6855761946198\n",
      "    - -911.5451914475241\n",
      "    - -1106.566559838001\n",
      "    - -1228.6165514912386\n",
      "    - -1440.5957478072664\n",
      "    - -1557.3531330147268\n",
      "    - -1399.4276665572406\n",
      "    - -1469.5907787791573\n",
      "    - -936.4966445935714\n",
      "    - -985.930161647759\n",
      "    - -1013.6478188412814\n",
      "    - -993.3068066495853\n",
      "    - -1118.324432553549\n",
      "    - -933.1434825821538\n",
      "    - -1180.6232001545432\n",
      "    - -898.4671003596978\n",
      "    - -1075.021782840154\n",
      "    - -1239.5698363434487\n",
      "    - -1105.7634598378738\n",
      "    - -1025.7862729720312\n",
      "    - -1065.520721906349\n",
      "    - -1121.7330937903273\n",
      "    - -904.5490482102094\n",
      "    - -1275.3195666335027\n",
      "    - -897.3407994037107\n",
      "    - -1577.3659984099722\n",
      "    - -1164.0103803593731\n",
      "    - -894.6997327899509\n",
      "    - -1071.6690748761177\n",
      "    - -1076.2683794464774\n",
      "    - -906.638693790309\n",
      "    - -897.5419167158091\n",
      "    - -899.7781737712398\n",
      "    - -890.2891384266278\n",
      "    - -985.1732029307428\n",
      "    - -1160.8702122915945\n",
      "    - -903.646852927247\n",
      "    - -1616.9784088652302\n",
      "    - -1097.766342735619\n",
      "    - -1181.2202994482655\n",
      "    - -924.993382442306\n",
      "    - -1425.8690269839299\n",
      "    - -1080.0229815488997\n",
      "    - -987.7502613965568\n",
      "    - -957.0868904012423\n",
      "    - -962.9690079358534\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19444300988621507\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14911238869269197\n",
      "    mean_inference_ms: 0.9803989467890717\n",
      "    mean_raw_obs_processing_ms: 0.12271508309698037\n",
      "time_since_restore: 387.5582585334778\n",
      "time_this_iter_s: 10.548101902008057\n",
      "time_total_s: 387.5582585334778\n",
      "timers:\n",
      "  learn_throughput: 851.892\n",
      "  learn_time_ms: 4695.43\n",
      "  load_throughput: 15222952.545\n",
      "  load_time_ms: 0.263\n",
      "  training_iteration_time_ms: 10527.66\n",
      "  update_time_ms: 2.221\n",
      "timestamp: 1660564307\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 148000\n",
      "training_iteration: 37\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 152000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 152000\n",
      "  num_agent_steps_trained: 152000\n",
      "  num_env_steps_sampled: 152000\n",
      "  num_env_steps_trained: 152000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-51-58\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -763.8439210480203\n",
      "episode_reward_mean: -1141.782140667973\n",
      "episode_reward_min: -1827.2597656488967\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 760\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.24027100205421448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3507282733917236\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012994818389415741\n",
      "        model: {}\n",
      "        policy_loss: 0.006089654751121998\n",
      "        total_loss: 9.959325790405273\n",
      "        vf_explained_var: -0.00912464875727892\n",
      "        vf_loss: 9.950114250183105\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 152000\n",
      "  num_agent_steps_trained: 152000\n",
      "  num_env_steps_sampled: 152000\n",
      "  num_env_steps_trained: 152000\n",
      "iterations_since_restore: 38\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 152000\n",
      "num_agent_steps_trained: 152000\n",
      "num_env_steps_sampled: 152000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 152000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 58.8235294117647\n",
      "  ram_util_percent: 59.194117647058825\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1942515691575163\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14900573203780432\n",
      "  mean_inference_ms: 0.9797145655129711\n",
      "  mean_raw_obs_processing_ms: 0.12258903676314986\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -763.8439210480203\n",
      "  episode_reward_mean: -1141.782140667973\n",
      "  episode_reward_min: -1827.2597656488967\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1618.7386446580447\n",
      "    - -1587.8237445591355\n",
      "    - -1491.7197131161004\n",
      "    - -894.2916165205366\n",
      "    - -904.7479471849475\n",
      "    - -873.9184601611355\n",
      "    - -1048.4934820941453\n",
      "    - -1713.110280775326\n",
      "    - -967.5054738521139\n",
      "    - -1015.454334381352\n",
      "    - -914.3689930668883\n",
      "    - -1559.2619123087995\n",
      "    - -1161.4609222762176\n",
      "    - -1009.7776534685086\n",
      "    - -1613.393282471002\n",
      "    - -1623.1927932952758\n",
      "    - -1154.3933220749357\n",
      "    - -1286.052663946972\n",
      "    - -1085.8502899786956\n",
      "    - -1347.4194623182088\n",
      "    - -996.4739988223664\n",
      "    - -894.2875924591823\n",
      "    - -882.938594292758\n",
      "    - -1431.0036790442432\n",
      "    - -1257.9783318284917\n",
      "    - -1339.6426842407516\n",
      "    - -900.5417984109486\n",
      "    - -1627.0248712060143\n",
      "    - -890.2728595353485\n",
      "    - -1003.8138094262573\n",
      "    - -1098.2667614235884\n",
      "    - -1023.2617747255795\n",
      "    - -763.8439210480203\n",
      "    - -1243.0352953729914\n",
      "    - -1008.6855761946198\n",
      "    - -911.5451914475241\n",
      "    - -1106.566559838001\n",
      "    - -1228.6165514912386\n",
      "    - -1440.5957478072664\n",
      "    - -1557.3531330147268\n",
      "    - -1399.4276665572406\n",
      "    - -1469.5907787791573\n",
      "    - -936.4966445935714\n",
      "    - -985.930161647759\n",
      "    - -1013.6478188412814\n",
      "    - -993.3068066495853\n",
      "    - -1118.324432553549\n",
      "    - -933.1434825821538\n",
      "    - -1180.6232001545432\n",
      "    - -898.4671003596978\n",
      "    - -1075.021782840154\n",
      "    - -1239.5698363434487\n",
      "    - -1105.7634598378738\n",
      "    - -1025.7862729720312\n",
      "    - -1065.520721906349\n",
      "    - -1121.7330937903273\n",
      "    - -904.5490482102094\n",
      "    - -1275.3195666335027\n",
      "    - -897.3407994037107\n",
      "    - -1577.3659984099722\n",
      "    - -1164.0103803593731\n",
      "    - -894.6997327899509\n",
      "    - -1071.6690748761177\n",
      "    - -1076.2683794464774\n",
      "    - -906.638693790309\n",
      "    - -897.5419167158091\n",
      "    - -899.7781737712398\n",
      "    - -890.2891384266278\n",
      "    - -985.1732029307428\n",
      "    - -1160.8702122915945\n",
      "    - -903.646852927247\n",
      "    - -1616.9784088652302\n",
      "    - -1097.766342735619\n",
      "    - -1181.2202994482655\n",
      "    - -924.993382442306\n",
      "    - -1425.8690269839299\n",
      "    - -1080.0229815488997\n",
      "    - -987.7502613965568\n",
      "    - -957.0868904012423\n",
      "    - -962.9690079358534\n",
      "    - -1346.0245074043467\n",
      "    - -932.0599277190797\n",
      "    - -887.2015991692954\n",
      "    - -890.7423909279318\n",
      "    - -1277.415022723889\n",
      "    - -1715.5174340489268\n",
      "    - -1271.9088453023085\n",
      "    - -1827.2597656488967\n",
      "    - -1469.6748118874116\n",
      "    - -912.7313084298835\n",
      "    - -1018.0892376564783\n",
      "    - -1003.9511875141969\n",
      "    - -1046.792050572522\n",
      "    - -998.5184767738225\n",
      "    - -995.3921542106389\n",
      "    - -1021.7903107906606\n",
      "    - -1319.4725760849012\n",
      "    - -970.88476125836\n",
      "    - -961.479692225729\n",
      "    - -1532.413249160248\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1942515691575163\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14900573203780432\n",
      "    mean_inference_ms: 0.9797145655129711\n",
      "    mean_raw_obs_processing_ms: 0.12258903676314986\n",
      "time_since_restore: 399.281619310379\n",
      "time_this_iter_s: 11.723360776901245\n",
      "time_total_s: 399.281619310379\n",
      "timers:\n",
      "  learn_throughput: 847.237\n",
      "  learn_time_ms: 4721.23\n",
      "  load_throughput: 15383473.317\n",
      "  load_time_ms: 0.26\n",
      "  training_iteration_time_ms: 10680.029\n",
      "  update_time_ms: 2.197\n",
      "timestamp: 1660564318\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 152000\n",
      "training_iteration: 38\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 156000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 156000\n",
      "  num_agent_steps_trained: 156000\n",
      "  num_env_steps_sampled: 156000\n",
      "  num_env_steps_trained: 156000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-52-09\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -763.8439210480203\n",
      "episode_reward_mean: -1086.9600029385529\n",
      "episode_reward_min: -1827.2597656488967\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 780\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.24027100205421448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.12317335605621338\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010128717869520187\n",
      "        model: {}\n",
      "        policy_loss: 0.011793169192969799\n",
      "        total_loss: 9.902416229248047\n",
      "        vf_explained_var: -0.013405188918113708\n",
      "        vf_loss: 9.888189315795898\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 156000\n",
      "  num_agent_steps_trained: 156000\n",
      "  num_env_steps_sampled: 156000\n",
      "  num_env_steps_trained: 156000\n",
      "iterations_since_restore: 39\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 156000\n",
      "num_agent_steps_trained: 156000\n",
      "num_env_steps_sampled: 156000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 156000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 61.314285714285724\n",
      "  ram_util_percent: 59.142857142857146\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1940514971214915\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14891124253952248\n",
      "  mean_inference_ms: 0.9790425957202357\n",
      "  mean_raw_obs_processing_ms: 0.12247779168749734\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -763.8439210480203\n",
      "  episode_reward_mean: -1086.9600029385529\n",
      "  episode_reward_min: -1827.2597656488967\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -996.4739988223664\n",
      "    - -894.2875924591823\n",
      "    - -882.938594292758\n",
      "    - -1431.0036790442432\n",
      "    - -1257.9783318284917\n",
      "    - -1339.6426842407516\n",
      "    - -900.5417984109486\n",
      "    - -1627.0248712060143\n",
      "    - -890.2728595353485\n",
      "    - -1003.8138094262573\n",
      "    - -1098.2667614235884\n",
      "    - -1023.2617747255795\n",
      "    - -763.8439210480203\n",
      "    - -1243.0352953729914\n",
      "    - -1008.6855761946198\n",
      "    - -911.5451914475241\n",
      "    - -1106.566559838001\n",
      "    - -1228.6165514912386\n",
      "    - -1440.5957478072664\n",
      "    - -1557.3531330147268\n",
      "    - -1399.4276665572406\n",
      "    - -1469.5907787791573\n",
      "    - -936.4966445935714\n",
      "    - -985.930161647759\n",
      "    - -1013.6478188412814\n",
      "    - -993.3068066495853\n",
      "    - -1118.324432553549\n",
      "    - -933.1434825821538\n",
      "    - -1180.6232001545432\n",
      "    - -898.4671003596978\n",
      "    - -1075.021782840154\n",
      "    - -1239.5698363434487\n",
      "    - -1105.7634598378738\n",
      "    - -1025.7862729720312\n",
      "    - -1065.520721906349\n",
      "    - -1121.7330937903273\n",
      "    - -904.5490482102094\n",
      "    - -1275.3195666335027\n",
      "    - -897.3407994037107\n",
      "    - -1577.3659984099722\n",
      "    - -1164.0103803593731\n",
      "    - -894.6997327899509\n",
      "    - -1071.6690748761177\n",
      "    - -1076.2683794464774\n",
      "    - -906.638693790309\n",
      "    - -897.5419167158091\n",
      "    - -899.7781737712398\n",
      "    - -890.2891384266278\n",
      "    - -985.1732029307428\n",
      "    - -1160.8702122915945\n",
      "    - -903.646852927247\n",
      "    - -1616.9784088652302\n",
      "    - -1097.766342735619\n",
      "    - -1181.2202994482655\n",
      "    - -924.993382442306\n",
      "    - -1425.8690269839299\n",
      "    - -1080.0229815488997\n",
      "    - -987.7502613965568\n",
      "    - -957.0868904012423\n",
      "    - -962.9690079358534\n",
      "    - -1346.0245074043467\n",
      "    - -932.0599277190797\n",
      "    - -887.2015991692954\n",
      "    - -890.7423909279318\n",
      "    - -1277.415022723889\n",
      "    - -1715.5174340489268\n",
      "    - -1271.9088453023085\n",
      "    - -1827.2597656488967\n",
      "    - -1469.6748118874116\n",
      "    - -912.7313084298835\n",
      "    - -1018.0892376564783\n",
      "    - -1003.9511875141969\n",
      "    - -1046.792050572522\n",
      "    - -998.5184767738225\n",
      "    - -995.3921542106389\n",
      "    - -1021.7903107906606\n",
      "    - -1319.4725760849012\n",
      "    - -970.88476125836\n",
      "    - -961.479692225729\n",
      "    - -1532.413249160248\n",
      "    - -893.944039595846\n",
      "    - -977.5053313452328\n",
      "    - -887.6001645280157\n",
      "    - -928.3254761790952\n",
      "    - -921.662211879738\n",
      "    - -884.23167344659\n",
      "    - -902.8259829495571\n",
      "    - -939.2682935134884\n",
      "    - -1272.3783801121817\n",
      "    - -941.7079107802384\n",
      "    - -1080.2425385327358\n",
      "    - -881.3997119903919\n",
      "    - -921.5092945928606\n",
      "    - -1099.2739589830292\n",
      "    - -894.3244891554185\n",
      "    - -1192.333229672574\n",
      "    - -1003.3720149681809\n",
      "    - -995.0549749946119\n",
      "    - -883.9241064035124\n",
      "    - -887.877435943046\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1940514971214915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14891124253952248\n",
      "    mean_inference_ms: 0.9790425957202357\n",
      "    mean_raw_obs_processing_ms: 0.12247779168749734\n",
      "time_since_restore: 409.4206244945526\n",
      "time_this_iter_s: 10.139005184173584\n",
      "time_total_s: 409.4206244945526\n",
      "timers:\n",
      "  learn_throughput: 842.848\n",
      "  learn_time_ms: 4745.814\n",
      "  load_throughput: 14826101.096\n",
      "  load_time_ms: 0.27\n",
      "  training_iteration_time_ms: 10632.517\n",
      "  update_time_ms: 2.225\n",
      "timestamp: 1660564329\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 156000\n",
      "training_iteration: 39\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 160000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 160000\n",
      "  num_agent_steps_trained: 160000\n",
      "  num_env_steps_sampled: 160000\n",
      "  num_env_steps_trained: 160000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-52-20\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -865.8090060118909\n",
      "episode_reward_mean: -1082.7806655392974\n",
      "episode_reward_min: -1827.2597656488967\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 800\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.24027100205421448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.06803560256958\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.037395913153886795\n",
      "        model: {}\n",
      "        policy_loss: 0.008072677068412304\n",
      "        total_loss: 9.847801208496094\n",
      "        vf_explained_var: -0.02205246314406395\n",
      "        vf_loss: 9.830743789672852\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 160000\n",
      "  num_agent_steps_trained: 160000\n",
      "  num_env_steps_sampled: 160000\n",
      "  num_env_steps_trained: 160000\n",
      "iterations_since_restore: 40\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 160000\n",
      "num_agent_steps_trained: 160000\n",
      "num_env_steps_sampled: 160000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 160000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.900000000000006\n",
      "  ram_util_percent: 59.150000000000006\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19413354506778752\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1490132955892627\n",
      "  mean_inference_ms: 0.9797999567136864\n",
      "  mean_raw_obs_processing_ms: 0.12251726149905096\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -865.8090060118909\n",
      "  episode_reward_mean: -1082.7806655392974\n",
      "  episode_reward_min: -1827.2597656488967\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1399.4276665572406\n",
      "    - -1469.5907787791573\n",
      "    - -936.4966445935714\n",
      "    - -985.930161647759\n",
      "    - -1013.6478188412814\n",
      "    - -993.3068066495853\n",
      "    - -1118.324432553549\n",
      "    - -933.1434825821538\n",
      "    - -1180.6232001545432\n",
      "    - -898.4671003596978\n",
      "    - -1075.021782840154\n",
      "    - -1239.5698363434487\n",
      "    - -1105.7634598378738\n",
      "    - -1025.7862729720312\n",
      "    - -1065.520721906349\n",
      "    - -1121.7330937903273\n",
      "    - -904.5490482102094\n",
      "    - -1275.3195666335027\n",
      "    - -897.3407994037107\n",
      "    - -1577.3659984099722\n",
      "    - -1164.0103803593731\n",
      "    - -894.6997327899509\n",
      "    - -1071.6690748761177\n",
      "    - -1076.2683794464774\n",
      "    - -906.638693790309\n",
      "    - -897.5419167158091\n",
      "    - -899.7781737712398\n",
      "    - -890.2891384266278\n",
      "    - -985.1732029307428\n",
      "    - -1160.8702122915945\n",
      "    - -903.646852927247\n",
      "    - -1616.9784088652302\n",
      "    - -1097.766342735619\n",
      "    - -1181.2202994482655\n",
      "    - -924.993382442306\n",
      "    - -1425.8690269839299\n",
      "    - -1080.0229815488997\n",
      "    - -987.7502613965568\n",
      "    - -957.0868904012423\n",
      "    - -962.9690079358534\n",
      "    - -1346.0245074043467\n",
      "    - -932.0599277190797\n",
      "    - -887.2015991692954\n",
      "    - -890.7423909279318\n",
      "    - -1277.415022723889\n",
      "    - -1715.5174340489268\n",
      "    - -1271.9088453023085\n",
      "    - -1827.2597656488967\n",
      "    - -1469.6748118874116\n",
      "    - -912.7313084298835\n",
      "    - -1018.0892376564783\n",
      "    - -1003.9511875141969\n",
      "    - -1046.792050572522\n",
      "    - -998.5184767738225\n",
      "    - -995.3921542106389\n",
      "    - -1021.7903107906606\n",
      "    - -1319.4725760849012\n",
      "    - -970.88476125836\n",
      "    - -961.479692225729\n",
      "    - -1532.413249160248\n",
      "    - -893.944039595846\n",
      "    - -977.5053313452328\n",
      "    - -887.6001645280157\n",
      "    - -928.3254761790952\n",
      "    - -921.662211879738\n",
      "    - -884.23167344659\n",
      "    - -902.8259829495571\n",
      "    - -939.2682935134884\n",
      "    - -1272.3783801121817\n",
      "    - -941.7079107802384\n",
      "    - -1080.2425385327358\n",
      "    - -881.3997119903919\n",
      "    - -921.5092945928606\n",
      "    - -1099.2739589830292\n",
      "    - -894.3244891554185\n",
      "    - -1192.333229672574\n",
      "    - -1003.3720149681809\n",
      "    - -995.0549749946119\n",
      "    - -883.9241064035124\n",
      "    - -887.877435943046\n",
      "    - -969.7223884785053\n",
      "    - -1057.5314319174458\n",
      "    - -973.1121196745086\n",
      "    - -1088.1729316933795\n",
      "    - -1621.685421679152\n",
      "    - -1020.7556186219017\n",
      "    - -1066.4893683169648\n",
      "    - -1091.1787544719496\n",
      "    - -889.6905364509329\n",
      "    - -897.9522063035813\n",
      "    - -1630.9052978421141\n",
      "    - -1262.454510899884\n",
      "    - -1027.3047080861438\n",
      "    - -865.8090060118909\n",
      "    - -931.6526402954365\n",
      "    - -1272.2597796018706\n",
      "    - -1019.9516992828522\n",
      "    - -1386.535246967604\n",
      "    - -917.338587397723\n",
      "    - -1197.3127377105159\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19413354506778752\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1490132955892627\n",
      "    mean_inference_ms: 0.9797999567136864\n",
      "    mean_raw_obs_processing_ms: 0.12251726149905096\n",
      "time_since_restore: 420.81332993507385\n",
      "time_this_iter_s: 11.39270544052124\n",
      "time_total_s: 420.81332993507385\n",
      "timers:\n",
      "  learn_throughput: 863.566\n",
      "  learn_time_ms: 4631.956\n",
      "  load_throughput: 15167901.636\n",
      "  load_time_ms: 0.264\n",
      "  training_iteration_time_ms: 10556.368\n",
      "  update_time_ms: 2.208\n",
      "timestamp: 1660564340\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 160000\n",
      "training_iteration: 40\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 164000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 164000\n",
      "  num_agent_steps_trained: 164000\n",
      "  num_env_steps_sampled: 164000\n",
      "  num_env_steps_trained: 164000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-52-30\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -865.8090060118909\n",
      "episode_reward_mean: -1089.6922873208687\n",
      "episode_reward_min: -1827.2597656488967\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 820\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3604064881801605\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3387839794158936\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01479267980903387\n",
      "        model: {}\n",
      "        policy_loss: 0.007764566224068403\n",
      "        total_loss: 9.901369094848633\n",
      "        vf_explained_var: -0.009188837371766567\n",
      "        vf_loss: 9.888274192810059\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 164000\n",
      "  num_agent_steps_trained: 164000\n",
      "  num_env_steps_sampled: 164000\n",
      "  num_env_steps_trained: 164000\n",
      "iterations_since_restore: 41\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 164000\n",
      "num_agent_steps_trained: 164000\n",
      "num_env_steps_sampled: 164000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 164000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.40714285714285\n",
      "  ram_util_percent: 59.042857142857144\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19423359918991254\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14913924247203914\n",
      "  mean_inference_ms: 0.9806605352503704\n",
      "  mean_raw_obs_processing_ms: 0.12258052964935987\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -865.8090060118909\n",
      "  episode_reward_mean: -1089.6922873208687\n",
      "  episode_reward_min: -1827.2597656488967\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1164.0103803593731\n",
      "    - -894.6997327899509\n",
      "    - -1071.6690748761177\n",
      "    - -1076.2683794464774\n",
      "    - -906.638693790309\n",
      "    - -897.5419167158091\n",
      "    - -899.7781737712398\n",
      "    - -890.2891384266278\n",
      "    - -985.1732029307428\n",
      "    - -1160.8702122915945\n",
      "    - -903.646852927247\n",
      "    - -1616.9784088652302\n",
      "    - -1097.766342735619\n",
      "    - -1181.2202994482655\n",
      "    - -924.993382442306\n",
      "    - -1425.8690269839299\n",
      "    - -1080.0229815488997\n",
      "    - -987.7502613965568\n",
      "    - -957.0868904012423\n",
      "    - -962.9690079358534\n",
      "    - -1346.0245074043467\n",
      "    - -932.0599277190797\n",
      "    - -887.2015991692954\n",
      "    - -890.7423909279318\n",
      "    - -1277.415022723889\n",
      "    - -1715.5174340489268\n",
      "    - -1271.9088453023085\n",
      "    - -1827.2597656488967\n",
      "    - -1469.6748118874116\n",
      "    - -912.7313084298835\n",
      "    - -1018.0892376564783\n",
      "    - -1003.9511875141969\n",
      "    - -1046.792050572522\n",
      "    - -998.5184767738225\n",
      "    - -995.3921542106389\n",
      "    - -1021.7903107906606\n",
      "    - -1319.4725760849012\n",
      "    - -970.88476125836\n",
      "    - -961.479692225729\n",
      "    - -1532.413249160248\n",
      "    - -893.944039595846\n",
      "    - -977.5053313452328\n",
      "    - -887.6001645280157\n",
      "    - -928.3254761790952\n",
      "    - -921.662211879738\n",
      "    - -884.23167344659\n",
      "    - -902.8259829495571\n",
      "    - -939.2682935134884\n",
      "    - -1272.3783801121817\n",
      "    - -941.7079107802384\n",
      "    - -1080.2425385327358\n",
      "    - -881.3997119903919\n",
      "    - -921.5092945928606\n",
      "    - -1099.2739589830292\n",
      "    - -894.3244891554185\n",
      "    - -1192.333229672574\n",
      "    - -1003.3720149681809\n",
      "    - -995.0549749946119\n",
      "    - -883.9241064035124\n",
      "    - -887.877435943046\n",
      "    - -969.7223884785053\n",
      "    - -1057.5314319174458\n",
      "    - -973.1121196745086\n",
      "    - -1088.1729316933795\n",
      "    - -1621.685421679152\n",
      "    - -1020.7556186219017\n",
      "    - -1066.4893683169648\n",
      "    - -1091.1787544719496\n",
      "    - -889.6905364509329\n",
      "    - -897.9522063035813\n",
      "    - -1630.9052978421141\n",
      "    - -1262.454510899884\n",
      "    - -1027.3047080861438\n",
      "    - -865.8090060118909\n",
      "    - -931.6526402954365\n",
      "    - -1272.2597796018706\n",
      "    - -1019.9516992828522\n",
      "    - -1386.535246967604\n",
      "    - -917.338587397723\n",
      "    - -1197.3127377105159\n",
      "    - -1118.7848714366849\n",
      "    - -1503.201121433946\n",
      "    - -1509.96243304584\n",
      "    - -1007.2611883588785\n",
      "    - -1011.4553591423795\n",
      "    - -913.6401629293174\n",
      "    - -893.0103851601041\n",
      "    - -1271.719926215015\n",
      "    - -994.38811899478\n",
      "    - -899.0733103201233\n",
      "    - -1481.8877927107835\n",
      "    - -1587.9687818712578\n",
      "    - -1434.3789636939448\n",
      "    - -989.8728304621548\n",
      "    - -889.0650290136914\n",
      "    - -1035.260901483971\n",
      "    - -1358.658944675563\n",
      "    - -1016.2511643806299\n",
      "    - -979.6187359012713\n",
      "    - -1012.6308299929208\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19423359918991254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14913924247203914\n",
      "    mean_inference_ms: 0.9806605352503704\n",
      "    mean_raw_obs_processing_ms: 0.12258052964935987\n",
      "time_since_restore: 430.3894760608673\n",
      "time_this_iter_s: 9.576146125793457\n",
      "time_total_s: 430.3894760608673\n",
      "timers:\n",
      "  learn_throughput: 882.741\n",
      "  learn_time_ms: 4531.34\n",
      "  load_throughput: 15943377.364\n",
      "  load_time_ms: 0.251\n",
      "  training_iteration_time_ms: 10357.072\n",
      "  update_time_ms: 2.215\n",
      "timestamp: 1660564350\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 164000\n",
      "training_iteration: 41\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 168000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 168000\n",
      "  num_agent_steps_trained: 168000\n",
      "  num_env_steps_sampled: 168000\n",
      "  num_env_steps_trained: 168000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-52-40\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -865.8090060118909\n",
      "episode_reward_mean: -1107.2678624615105\n",
      "episode_reward_min: -1827.2597656488967\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 840\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3604064881801605\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2104339599609375\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008179510943591595\n",
      "        model: {}\n",
      "        policy_loss: 0.010263867676258087\n",
      "        total_loss: 9.841324806213379\n",
      "        vf_explained_var: -0.01692870631814003\n",
      "        vf_loss: 9.828112602233887\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 168000\n",
      "  num_agent_steps_trained: 168000\n",
      "  num_env_steps_sampled: 168000\n",
      "  num_env_steps_trained: 168000\n",
      "iterations_since_restore: 42\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 168000\n",
      "num_agent_steps_trained: 168000\n",
      "num_env_steps_sampled: 168000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 168000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 58.34666666666667\n",
      "  ram_util_percent: 59.06000000000001\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1944037997631622\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14930533173057092\n",
      "  mean_inference_ms: 0.98183018153425\n",
      "  mean_raw_obs_processing_ms: 0.12267570736540584\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -865.8090060118909\n",
      "  episode_reward_mean: -1107.2678624615105\n",
      "  episode_reward_min: -1827.2597656488967\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1346.0245074043467\n",
      "    - -932.0599277190797\n",
      "    - -887.2015991692954\n",
      "    - -890.7423909279318\n",
      "    - -1277.415022723889\n",
      "    - -1715.5174340489268\n",
      "    - -1271.9088453023085\n",
      "    - -1827.2597656488967\n",
      "    - -1469.6748118874116\n",
      "    - -912.7313084298835\n",
      "    - -1018.0892376564783\n",
      "    - -1003.9511875141969\n",
      "    - -1046.792050572522\n",
      "    - -998.5184767738225\n",
      "    - -995.3921542106389\n",
      "    - -1021.7903107906606\n",
      "    - -1319.4725760849012\n",
      "    - -970.88476125836\n",
      "    - -961.479692225729\n",
      "    - -1532.413249160248\n",
      "    - -893.944039595846\n",
      "    - -977.5053313452328\n",
      "    - -887.6001645280157\n",
      "    - -928.3254761790952\n",
      "    - -921.662211879738\n",
      "    - -884.23167344659\n",
      "    - -902.8259829495571\n",
      "    - -939.2682935134884\n",
      "    - -1272.3783801121817\n",
      "    - -941.7079107802384\n",
      "    - -1080.2425385327358\n",
      "    - -881.3997119903919\n",
      "    - -921.5092945928606\n",
      "    - -1099.2739589830292\n",
      "    - -894.3244891554185\n",
      "    - -1192.333229672574\n",
      "    - -1003.3720149681809\n",
      "    - -995.0549749946119\n",
      "    - -883.9241064035124\n",
      "    - -887.877435943046\n",
      "    - -969.7223884785053\n",
      "    - -1057.5314319174458\n",
      "    - -973.1121196745086\n",
      "    - -1088.1729316933795\n",
      "    - -1621.685421679152\n",
      "    - -1020.7556186219017\n",
      "    - -1066.4893683169648\n",
      "    - -1091.1787544719496\n",
      "    - -889.6905364509329\n",
      "    - -897.9522063035813\n",
      "    - -1630.9052978421141\n",
      "    - -1262.454510899884\n",
      "    - -1027.3047080861438\n",
      "    - -865.8090060118909\n",
      "    - -931.6526402954365\n",
      "    - -1272.2597796018706\n",
      "    - -1019.9516992828522\n",
      "    - -1386.535246967604\n",
      "    - -917.338587397723\n",
      "    - -1197.3127377105159\n",
      "    - -1118.7848714366849\n",
      "    - -1503.201121433946\n",
      "    - -1509.96243304584\n",
      "    - -1007.2611883588785\n",
      "    - -1011.4553591423795\n",
      "    - -913.6401629293174\n",
      "    - -893.0103851601041\n",
      "    - -1271.719926215015\n",
      "    - -994.38811899478\n",
      "    - -899.0733103201233\n",
      "    - -1481.8877927107835\n",
      "    - -1587.9687818712578\n",
      "    - -1434.3789636939448\n",
      "    - -989.8728304621548\n",
      "    - -889.0650290136914\n",
      "    - -1035.260901483971\n",
      "    - -1358.658944675563\n",
      "    - -1016.2511643806299\n",
      "    - -979.6187359012713\n",
      "    - -1012.6308299929208\n",
      "    - -1737.8965197157586\n",
      "    - -1020.2250621271887\n",
      "    - -977.0502686960305\n",
      "    - -1012.6884865545318\n",
      "    - -1305.1436467926721\n",
      "    - -970.0976147840034\n",
      "    - -1413.9084093789363\n",
      "    - -1061.3574922400198\n",
      "    - -1200.0092323222632\n",
      "    - -1029.150586287532\n",
      "    - -928.3458872517315\n",
      "    - -978.7476108213499\n",
      "    - -1062.4727403402753\n",
      "    - -1424.1918599584221\n",
      "    - -896.2177190961886\n",
      "    - -1021.2773618456524\n",
      "    - -1473.7313020743397\n",
      "    - -1024.499752939185\n",
      "    - -987.7281095772817\n",
      "    - -1318.0602113442067\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1944037997631622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14930533173057092\n",
      "    mean_inference_ms: 0.98183018153425\n",
      "    mean_raw_obs_processing_ms: 0.12267570736540584\n",
      "time_since_restore: 440.7544331550598\n",
      "time_this_iter_s: 10.364957094192505\n",
      "time_total_s: 440.7544331550598\n",
      "timers:\n",
      "  learn_throughput: 883.89\n",
      "  learn_time_ms: 4525.452\n",
      "  load_throughput: 16141250.722\n",
      "  load_time_ms: 0.248\n",
      "  training_iteration_time_ms: 10317.258\n",
      "  update_time_ms: 2.224\n",
      "timestamp: 1660564360\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 168000\n",
      "training_iteration: 42\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 172000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 172000\n",
      "  num_agent_steps_trained: 172000\n",
      "  num_env_steps_sampled: 172000\n",
      "  num_env_steps_trained: 172000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-52-51\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -865.8090060118909\n",
      "episode_reward_mean: -1125.7831162083496\n",
      "episode_reward_min: -1753.431521198059\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 860\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3604064881801605\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.259838104248047\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013158419169485569\n",
      "        model: {}\n",
      "        policy_loss: 0.0074471072293818\n",
      "        total_loss: 9.944632530212402\n",
      "        vf_explained_var: -0.011597562581300735\n",
      "        vf_loss: 9.932441711425781\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 172000\n",
      "  num_agent_steps_trained: 172000\n",
      "  num_env_steps_sampled: 172000\n",
      "  num_env_steps_trained: 172000\n",
      "iterations_since_restore: 43\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 172000\n",
      "num_agent_steps_trained: 172000\n",
      "num_env_steps_sampled: 172000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 172000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.70666666666667\n",
      "  ram_util_percent: 59.10000000000001\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19441223501331187\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14935123271694511\n",
      "  mean_inference_ms: 0.9820721734841041\n",
      "  mean_raw_obs_processing_ms: 0.12268138468112964\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -865.8090060118909\n",
      "  episode_reward_mean: -1125.7831162083496\n",
      "  episode_reward_min: -1753.431521198059\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -893.944039595846\n",
      "    - -977.5053313452328\n",
      "    - -887.6001645280157\n",
      "    - -928.3254761790952\n",
      "    - -921.662211879738\n",
      "    - -884.23167344659\n",
      "    - -902.8259829495571\n",
      "    - -939.2682935134884\n",
      "    - -1272.3783801121817\n",
      "    - -941.7079107802384\n",
      "    - -1080.2425385327358\n",
      "    - -881.3997119903919\n",
      "    - -921.5092945928606\n",
      "    - -1099.2739589830292\n",
      "    - -894.3244891554185\n",
      "    - -1192.333229672574\n",
      "    - -1003.3720149681809\n",
      "    - -995.0549749946119\n",
      "    - -883.9241064035124\n",
      "    - -887.877435943046\n",
      "    - -969.7223884785053\n",
      "    - -1057.5314319174458\n",
      "    - -973.1121196745086\n",
      "    - -1088.1729316933795\n",
      "    - -1621.685421679152\n",
      "    - -1020.7556186219017\n",
      "    - -1066.4893683169648\n",
      "    - -1091.1787544719496\n",
      "    - -889.6905364509329\n",
      "    - -897.9522063035813\n",
      "    - -1630.9052978421141\n",
      "    - -1262.454510899884\n",
      "    - -1027.3047080861438\n",
      "    - -865.8090060118909\n",
      "    - -931.6526402954365\n",
      "    - -1272.2597796018706\n",
      "    - -1019.9516992828522\n",
      "    - -1386.535246967604\n",
      "    - -917.338587397723\n",
      "    - -1197.3127377105159\n",
      "    - -1118.7848714366849\n",
      "    - -1503.201121433946\n",
      "    - -1509.96243304584\n",
      "    - -1007.2611883588785\n",
      "    - -1011.4553591423795\n",
      "    - -913.6401629293174\n",
      "    - -893.0103851601041\n",
      "    - -1271.719926215015\n",
      "    - -994.38811899478\n",
      "    - -899.0733103201233\n",
      "    - -1481.8877927107835\n",
      "    - -1587.9687818712578\n",
      "    - -1434.3789636939448\n",
      "    - -989.8728304621548\n",
      "    - -889.0650290136914\n",
      "    - -1035.260901483971\n",
      "    - -1358.658944675563\n",
      "    - -1016.2511643806299\n",
      "    - -979.6187359012713\n",
      "    - -1012.6308299929208\n",
      "    - -1737.8965197157586\n",
      "    - -1020.2250621271887\n",
      "    - -977.0502686960305\n",
      "    - -1012.6884865545318\n",
      "    - -1305.1436467926721\n",
      "    - -970.0976147840034\n",
      "    - -1413.9084093789363\n",
      "    - -1061.3574922400198\n",
      "    - -1200.0092323222632\n",
      "    - -1029.150586287532\n",
      "    - -928.3458872517315\n",
      "    - -978.7476108213499\n",
      "    - -1062.4727403402753\n",
      "    - -1424.1918599584221\n",
      "    - -896.2177190961886\n",
      "    - -1021.2773618456524\n",
      "    - -1473.7313020743397\n",
      "    - -1024.499752939185\n",
      "    - -987.7281095772817\n",
      "    - -1318.0602113442067\n",
      "    - -1367.9914670810995\n",
      "    - -1475.4908767043185\n",
      "    - -1336.8719030228362\n",
      "    - -1130.173973840131\n",
      "    - -1447.8660518519134\n",
      "    - -1137.746483020049\n",
      "    - -1706.3229562572835\n",
      "    - -1709.7460753481007\n",
      "    - -1449.9210767408565\n",
      "    - -1551.379039996936\n",
      "    - -999.5789147272883\n",
      "    - -894.9347903473817\n",
      "    - -936.9552365866952\n",
      "    - -1753.431521198059\n",
      "    - -1024.8057948080618\n",
      "    - -1015.1351260284416\n",
      "    - -872.8636167636639\n",
      "    - -1435.813734565814\n",
      "    - -931.1760730176976\n",
      "    - -1072.6399722868018\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19441223501331187\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14935123271694511\n",
      "    mean_inference_ms: 0.9820721734841041\n",
      "    mean_raw_obs_processing_ms: 0.12268138468112964\n",
      "time_since_restore: 451.37905621528625\n",
      "time_this_iter_s: 10.62462306022644\n",
      "time_total_s: 451.37905621528625\n",
      "timers:\n",
      "  learn_throughput: 891.309\n",
      "  learn_time_ms: 4487.78\n",
      "  load_throughput: 16576638.672\n",
      "  load_time_ms: 0.241\n",
      "  training_iteration_time_ms: 10300.387\n",
      "  update_time_ms: 2.236\n",
      "timestamp: 1660564371\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 172000\n",
      "training_iteration: 43\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 176000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 176000\n",
      "  num_agent_steps_trained: 176000\n",
      "  num_env_steps_sampled: 176000\n",
      "  num_env_steps_trained: 176000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-53-01\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -865.5823664020162\n",
      "episode_reward_mean: -1169.6032172215876\n",
      "episode_reward_min: -1753.431521198059\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 880\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3604064881801605\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6940406560897827\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019114792346954346\n",
      "        model: {}\n",
      "        policy_loss: 0.01271845307201147\n",
      "        total_loss: 9.871426582336426\n",
      "        vf_explained_var: -0.02290642261505127\n",
      "        vf_loss: 9.85181999206543\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 176000\n",
      "  num_agent_steps_trained: 176000\n",
      "  num_env_steps_sampled: 176000\n",
      "  num_env_steps_trained: 176000\n",
      "iterations_since_restore: 44\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 176000\n",
      "num_agent_steps_trained: 176000\n",
      "num_env_steps_sampled: 176000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 176000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.21333333333333\n",
      "  ram_util_percent: 59.059999999999995\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19449183168858078\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14944031324774254\n",
      "  mean_inference_ms: 0.9826292130876706\n",
      "  mean_raw_obs_processing_ms: 0.12271860096350881\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -865.5823664020162\n",
      "  episode_reward_mean: -1169.6032172215876\n",
      "  episode_reward_min: -1753.431521198059\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -969.7223884785053\n",
      "    - -1057.5314319174458\n",
      "    - -973.1121196745086\n",
      "    - -1088.1729316933795\n",
      "    - -1621.685421679152\n",
      "    - -1020.7556186219017\n",
      "    - -1066.4893683169648\n",
      "    - -1091.1787544719496\n",
      "    - -889.6905364509329\n",
      "    - -897.9522063035813\n",
      "    - -1630.9052978421141\n",
      "    - -1262.454510899884\n",
      "    - -1027.3047080861438\n",
      "    - -865.8090060118909\n",
      "    - -931.6526402954365\n",
      "    - -1272.2597796018706\n",
      "    - -1019.9516992828522\n",
      "    - -1386.535246967604\n",
      "    - -917.338587397723\n",
      "    - -1197.3127377105159\n",
      "    - -1118.7848714366849\n",
      "    - -1503.201121433946\n",
      "    - -1509.96243304584\n",
      "    - -1007.2611883588785\n",
      "    - -1011.4553591423795\n",
      "    - -913.6401629293174\n",
      "    - -893.0103851601041\n",
      "    - -1271.719926215015\n",
      "    - -994.38811899478\n",
      "    - -899.0733103201233\n",
      "    - -1481.8877927107835\n",
      "    - -1587.9687818712578\n",
      "    - -1434.3789636939448\n",
      "    - -989.8728304621548\n",
      "    - -889.0650290136914\n",
      "    - -1035.260901483971\n",
      "    - -1358.658944675563\n",
      "    - -1016.2511643806299\n",
      "    - -979.6187359012713\n",
      "    - -1012.6308299929208\n",
      "    - -1737.8965197157586\n",
      "    - -1020.2250621271887\n",
      "    - -977.0502686960305\n",
      "    - -1012.6884865545318\n",
      "    - -1305.1436467926721\n",
      "    - -970.0976147840034\n",
      "    - -1413.9084093789363\n",
      "    - -1061.3574922400198\n",
      "    - -1200.0092323222632\n",
      "    - -1029.150586287532\n",
      "    - -928.3458872517315\n",
      "    - -978.7476108213499\n",
      "    - -1062.4727403402753\n",
      "    - -1424.1918599584221\n",
      "    - -896.2177190961886\n",
      "    - -1021.2773618456524\n",
      "    - -1473.7313020743397\n",
      "    - -1024.499752939185\n",
      "    - -987.7281095772817\n",
      "    - -1318.0602113442067\n",
      "    - -1367.9914670810995\n",
      "    - -1475.4908767043185\n",
      "    - -1336.8719030228362\n",
      "    - -1130.173973840131\n",
      "    - -1447.8660518519134\n",
      "    - -1137.746483020049\n",
      "    - -1706.3229562572835\n",
      "    - -1709.7460753481007\n",
      "    - -1449.9210767408565\n",
      "    - -1551.379039996936\n",
      "    - -999.5789147272883\n",
      "    - -894.9347903473817\n",
      "    - -936.9552365866952\n",
      "    - -1753.431521198059\n",
      "    - -1024.8057948080618\n",
      "    - -1015.1351260284416\n",
      "    - -872.8636167636639\n",
      "    - -1435.813734565814\n",
      "    - -931.1760730176976\n",
      "    - -1072.6399722868018\n",
      "    - -938.9259336742056\n",
      "    - -995.8703777447818\n",
      "    - -1020.3109420232598\n",
      "    - -898.4529299378286\n",
      "    - -1687.5225647355876\n",
      "    - -1101.8881691280665\n",
      "    - -1500.2716146773184\n",
      "    - -1012.9173764574842\n",
      "    - -1384.959936841807\n",
      "    - -1747.216560298775\n",
      "    - -1022.631421858048\n",
      "    - -1015.3868487336493\n",
      "    - -865.5823664020162\n",
      "    - -1662.408526432367\n",
      "    - -903.3796272612228\n",
      "    - -994.1240511379629\n",
      "    - -1012.4115316439434\n",
      "    - -1009.3792782313988\n",
      "    - -1524.653407852374\n",
      "    - -1472.4778558180392\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19449183168858078\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14944031324774254\n",
      "    mean_inference_ms: 0.9826292130876706\n",
      "    mean_raw_obs_processing_ms: 0.12271860096350881\n",
      "time_since_restore: 461.61219930648804\n",
      "time_this_iter_s: 10.233143091201782\n",
      "time_total_s: 461.61219930648804\n",
      "timers:\n",
      "  learn_throughput: 889.896\n",
      "  learn_time_ms: 4494.905\n",
      "  load_throughput: 16713703.925\n",
      "  load_time_ms: 0.239\n",
      "  training_iteration_time_ms: 10337.391\n",
      "  update_time_ms: 2.332\n",
      "timestamp: 1660564381\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 176000\n",
      "training_iteration: 44\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 180000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 180000\n",
      "  num_agent_steps_trained: 180000\n",
      "  num_env_steps_sampled: 180000\n",
      "  num_env_steps_trained: 180000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-53-11\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -865.5823664020162\n",
      "episode_reward_mean: -1184.8441503832705\n",
      "episode_reward_min: -1753.431521198059\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 900\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3604064881801605\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.744528889656067\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.024911509826779366\n",
      "        model: {}\n",
      "        policy_loss: 0.017835916951298714\n",
      "        total_loss: 9.915993690490723\n",
      "        vf_explained_var: -0.014527450315654278\n",
      "        vf_loss: 9.889178276062012\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 180000\n",
      "  num_agent_steps_trained: 180000\n",
      "  num_env_steps_sampled: 180000\n",
      "  num_env_steps_trained: 180000\n",
      "iterations_since_restore: 45\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 180000\n",
      "num_agent_steps_trained: 180000\n",
      "num_env_steps_sampled: 180000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 180000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.15714285714286\n",
      "  ram_util_percent: 58.97142857142857\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19435561239846585\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14937419542782795\n",
      "  mean_inference_ms: 0.9820598415448967\n",
      "  mean_raw_obs_processing_ms: 0.12263760656895543\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -865.5823664020162\n",
      "  episode_reward_mean: -1184.8441503832705\n",
      "  episode_reward_min: -1753.431521198059\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1118.7848714366849\n",
      "    - -1503.201121433946\n",
      "    - -1509.96243304584\n",
      "    - -1007.2611883588785\n",
      "    - -1011.4553591423795\n",
      "    - -913.6401629293174\n",
      "    - -893.0103851601041\n",
      "    - -1271.719926215015\n",
      "    - -994.38811899478\n",
      "    - -899.0733103201233\n",
      "    - -1481.8877927107835\n",
      "    - -1587.9687818712578\n",
      "    - -1434.3789636939448\n",
      "    - -989.8728304621548\n",
      "    - -889.0650290136914\n",
      "    - -1035.260901483971\n",
      "    - -1358.658944675563\n",
      "    - -1016.2511643806299\n",
      "    - -979.6187359012713\n",
      "    - -1012.6308299929208\n",
      "    - -1737.8965197157586\n",
      "    - -1020.2250621271887\n",
      "    - -977.0502686960305\n",
      "    - -1012.6884865545318\n",
      "    - -1305.1436467926721\n",
      "    - -970.0976147840034\n",
      "    - -1413.9084093789363\n",
      "    - -1061.3574922400198\n",
      "    - -1200.0092323222632\n",
      "    - -1029.150586287532\n",
      "    - -928.3458872517315\n",
      "    - -978.7476108213499\n",
      "    - -1062.4727403402753\n",
      "    - -1424.1918599584221\n",
      "    - -896.2177190961886\n",
      "    - -1021.2773618456524\n",
      "    - -1473.7313020743397\n",
      "    - -1024.499752939185\n",
      "    - -987.7281095772817\n",
      "    - -1318.0602113442067\n",
      "    - -1367.9914670810995\n",
      "    - -1475.4908767043185\n",
      "    - -1336.8719030228362\n",
      "    - -1130.173973840131\n",
      "    - -1447.8660518519134\n",
      "    - -1137.746483020049\n",
      "    - -1706.3229562572835\n",
      "    - -1709.7460753481007\n",
      "    - -1449.9210767408565\n",
      "    - -1551.379039996936\n",
      "    - -999.5789147272883\n",
      "    - -894.9347903473817\n",
      "    - -936.9552365866952\n",
      "    - -1753.431521198059\n",
      "    - -1024.8057948080618\n",
      "    - -1015.1351260284416\n",
      "    - -872.8636167636639\n",
      "    - -1435.813734565814\n",
      "    - -931.1760730176976\n",
      "    - -1072.6399722868018\n",
      "    - -938.9259336742056\n",
      "    - -995.8703777447818\n",
      "    - -1020.3109420232598\n",
      "    - -898.4529299378286\n",
      "    - -1687.5225647355876\n",
      "    - -1101.8881691280665\n",
      "    - -1500.2716146773184\n",
      "    - -1012.9173764574842\n",
      "    - -1384.959936841807\n",
      "    - -1747.216560298775\n",
      "    - -1022.631421858048\n",
      "    - -1015.3868487336493\n",
      "    - -865.5823664020162\n",
      "    - -1662.408526432367\n",
      "    - -903.3796272612228\n",
      "    - -994.1240511379629\n",
      "    - -1012.4115316439434\n",
      "    - -1009.3792782313988\n",
      "    - -1524.653407852374\n",
      "    - -1472.4778558180392\n",
      "    - -1107.3622105926956\n",
      "    - -960.0536153923642\n",
      "    - -1055.0554206760096\n",
      "    - -1353.924095271768\n",
      "    - -1432.9940677368206\n",
      "    - -1454.2018807208538\n",
      "    - -1277.613624062459\n",
      "    - -920.0095619598072\n",
      "    - -999.0625355394654\n",
      "    - -1082.6982964278532\n",
      "    - -1587.0352879731852\n",
      "    - -1166.0754012216419\n",
      "    - -1495.2128331188565\n",
      "    - -1026.855264041456\n",
      "    - -1427.145210754385\n",
      "    - -1007.3125194420812\n",
      "    - -893.7656911521359\n",
      "    - -1598.7581532836653\n",
      "    - -876.418811216981\n",
      "    - -990.3538272881838\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19435561239846585\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14937419542782795\n",
      "    mean_inference_ms: 0.9820598415448967\n",
      "    mean_raw_obs_processing_ms: 0.12263760656895543\n",
      "time_since_restore: 471.5349097251892\n",
      "time_this_iter_s: 9.922710418701172\n",
      "time_total_s: 471.5349097251892\n",
      "timers:\n",
      "  learn_throughput: 884.999\n",
      "  learn_time_ms: 4519.778\n",
      "  load_throughput: 16550474.499\n",
      "  load_time_ms: 0.242\n",
      "  training_iteration_time_ms: 10389.616\n",
      "  update_time_ms: 2.292\n",
      "timestamp: 1660564391\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 180000\n",
      "training_iteration: 45\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 184000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 184000\n",
      "  num_agent_steps_trained: 184000\n",
      "  num_env_steps_sampled: 184000\n",
      "  num_env_steps_trained: 184000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-53-21\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -865.5823664020162\n",
      "episode_reward_mean: -1191.5825990313365\n",
      "episode_reward_min: -1753.431521198059\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 920\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5406097173690796\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4889951944351196\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017148973420262337\n",
      "        model: {}\n",
      "        policy_loss: 0.013458756729960442\n",
      "        total_loss: 9.889354705810547\n",
      "        vf_explained_var: -0.013537825085222721\n",
      "        vf_loss: 9.86662483215332\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 184000\n",
      "  num_agent_steps_trained: 184000\n",
      "  num_env_steps_sampled: 184000\n",
      "  num_env_steps_trained: 184000\n",
      "iterations_since_restore: 46\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 184000\n",
      "num_agent_steps_trained: 184000\n",
      "num_env_steps_sampled: 184000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 184000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.03333333333333\n",
      "  ram_util_percent: 58.953333333333326\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19428400128725654\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14933608766225484\n",
      "  mean_inference_ms: 0.9817931563180825\n",
      "  mean_raw_obs_processing_ms: 0.12258272932247874\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -865.5823664020162\n",
      "  episode_reward_mean: -1191.5825990313365\n",
      "  episode_reward_min: -1753.431521198059\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1737.8965197157586\n",
      "    - -1020.2250621271887\n",
      "    - -977.0502686960305\n",
      "    - -1012.6884865545318\n",
      "    - -1305.1436467926721\n",
      "    - -970.0976147840034\n",
      "    - -1413.9084093789363\n",
      "    - -1061.3574922400198\n",
      "    - -1200.0092323222632\n",
      "    - -1029.150586287532\n",
      "    - -928.3458872517315\n",
      "    - -978.7476108213499\n",
      "    - -1062.4727403402753\n",
      "    - -1424.1918599584221\n",
      "    - -896.2177190961886\n",
      "    - -1021.2773618456524\n",
      "    - -1473.7313020743397\n",
      "    - -1024.499752939185\n",
      "    - -987.7281095772817\n",
      "    - -1318.0602113442067\n",
      "    - -1367.9914670810995\n",
      "    - -1475.4908767043185\n",
      "    - -1336.8719030228362\n",
      "    - -1130.173973840131\n",
      "    - -1447.8660518519134\n",
      "    - -1137.746483020049\n",
      "    - -1706.3229562572835\n",
      "    - -1709.7460753481007\n",
      "    - -1449.9210767408565\n",
      "    - -1551.379039996936\n",
      "    - -999.5789147272883\n",
      "    - -894.9347903473817\n",
      "    - -936.9552365866952\n",
      "    - -1753.431521198059\n",
      "    - -1024.8057948080618\n",
      "    - -1015.1351260284416\n",
      "    - -872.8636167636639\n",
      "    - -1435.813734565814\n",
      "    - -931.1760730176976\n",
      "    - -1072.6399722868018\n",
      "    - -938.9259336742056\n",
      "    - -995.8703777447818\n",
      "    - -1020.3109420232598\n",
      "    - -898.4529299378286\n",
      "    - -1687.5225647355876\n",
      "    - -1101.8881691280665\n",
      "    - -1500.2716146773184\n",
      "    - -1012.9173764574842\n",
      "    - -1384.959936841807\n",
      "    - -1747.216560298775\n",
      "    - -1022.631421858048\n",
      "    - -1015.3868487336493\n",
      "    - -865.5823664020162\n",
      "    - -1662.408526432367\n",
      "    - -903.3796272612228\n",
      "    - -994.1240511379629\n",
      "    - -1012.4115316439434\n",
      "    - -1009.3792782313988\n",
      "    - -1524.653407852374\n",
      "    - -1472.4778558180392\n",
      "    - -1107.3622105926956\n",
      "    - -960.0536153923642\n",
      "    - -1055.0554206760096\n",
      "    - -1353.924095271768\n",
      "    - -1432.9940677368206\n",
      "    - -1454.2018807208538\n",
      "    - -1277.613624062459\n",
      "    - -920.0095619598072\n",
      "    - -999.0625355394654\n",
      "    - -1082.6982964278532\n",
      "    - -1587.0352879731852\n",
      "    - -1166.0754012216419\n",
      "    - -1495.2128331188565\n",
      "    - -1026.855264041456\n",
      "    - -1427.145210754385\n",
      "    - -1007.3125194420812\n",
      "    - -893.7656911521359\n",
      "    - -1598.7581532836653\n",
      "    - -876.418811216981\n",
      "    - -990.3538272881838\n",
      "    - -1348.0190341434343\n",
      "    - -1091.0200452850652\n",
      "    - -1145.9689270836122\n",
      "    - -1499.0175734949585\n",
      "    - -1580.1463133227721\n",
      "    - -1431.6904601716997\n",
      "    - -891.3291164487235\n",
      "    - -1126.8647402989284\n",
      "    - -994.3492503345808\n",
      "    - -1029.8832602537\n",
      "    - -1033.8117345831122\n",
      "    - -1591.0373010284309\n",
      "    - -1079.8273229532804\n",
      "    - -1401.089648611702\n",
      "    - -1007.2955515239696\n",
      "    - -1003.2378146415479\n",
      "    - -920.7640995174812\n",
      "    - -1009.9290564567373\n",
      "    - -1102.5580584956128\n",
      "    - -1294.096407380496\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19428400128725654\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14933608766225484\n",
      "    mean_inference_ms: 0.9817931563180825\n",
      "    mean_raw_obs_processing_ms: 0.12258272932247874\n",
      "time_since_restore: 481.81278824806213\n",
      "time_this_iter_s: 10.277878522872925\n",
      "time_total_s: 481.81278824806213\n",
      "timers:\n",
      "  learn_throughput: 876.476\n",
      "  learn_time_ms: 4563.73\n",
      "  load_throughput: 17095186.468\n",
      "  load_time_ms: 0.234\n",
      "  training_iteration_time_ms: 10473.333\n",
      "  update_time_ms: 2.299\n",
      "timestamp: 1660564401\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 184000\n",
      "training_iteration: 46\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 188000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 188000\n",
      "  num_agent_steps_trained: 188000\n",
      "  num_env_steps_sampled: 188000\n",
      "  num_env_steps_trained: 188000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-53-31\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -865.5823664020162\n",
      "episode_reward_mean: -1191.7600529884476\n",
      "episode_reward_min: -1753.431521198059\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 940\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5406097173690796\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1762288808822632\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012380586005747318\n",
      "        model: {}\n",
      "        policy_loss: 0.014273307286202908\n",
      "        total_loss: 9.753670692443848\n",
      "        vf_explained_var: -0.020504996180534363\n",
      "        vf_loss: 9.73270320892334\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 188000\n",
      "  num_agent_steps_trained: 188000\n",
      "  num_env_steps_sampled: 188000\n",
      "  num_env_steps_trained: 188000\n",
      "iterations_since_restore: 47\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 188000\n",
      "num_agent_steps_trained: 188000\n",
      "num_env_steps_sampled: 188000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 188000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.32857142857142\n",
      "  ram_util_percent: 58.914285714285704\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19421281737283735\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1492987390390425\n",
      "  mean_inference_ms: 0.9815836346966604\n",
      "  mean_raw_obs_processing_ms: 0.12252962317513084\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -865.5823664020162\n",
      "  episode_reward_mean: -1191.7600529884476\n",
      "  episode_reward_min: -1753.431521198059\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1367.9914670810995\n",
      "    - -1475.4908767043185\n",
      "    - -1336.8719030228362\n",
      "    - -1130.173973840131\n",
      "    - -1447.8660518519134\n",
      "    - -1137.746483020049\n",
      "    - -1706.3229562572835\n",
      "    - -1709.7460753481007\n",
      "    - -1449.9210767408565\n",
      "    - -1551.379039996936\n",
      "    - -999.5789147272883\n",
      "    - -894.9347903473817\n",
      "    - -936.9552365866952\n",
      "    - -1753.431521198059\n",
      "    - -1024.8057948080618\n",
      "    - -1015.1351260284416\n",
      "    - -872.8636167636639\n",
      "    - -1435.813734565814\n",
      "    - -931.1760730176976\n",
      "    - -1072.6399722868018\n",
      "    - -938.9259336742056\n",
      "    - -995.8703777447818\n",
      "    - -1020.3109420232598\n",
      "    - -898.4529299378286\n",
      "    - -1687.5225647355876\n",
      "    - -1101.8881691280665\n",
      "    - -1500.2716146773184\n",
      "    - -1012.9173764574842\n",
      "    - -1384.959936841807\n",
      "    - -1747.216560298775\n",
      "    - -1022.631421858048\n",
      "    - -1015.3868487336493\n",
      "    - -865.5823664020162\n",
      "    - -1662.408526432367\n",
      "    - -903.3796272612228\n",
      "    - -994.1240511379629\n",
      "    - -1012.4115316439434\n",
      "    - -1009.3792782313988\n",
      "    - -1524.653407852374\n",
      "    - -1472.4778558180392\n",
      "    - -1107.3622105926956\n",
      "    - -960.0536153923642\n",
      "    - -1055.0554206760096\n",
      "    - -1353.924095271768\n",
      "    - -1432.9940677368206\n",
      "    - -1454.2018807208538\n",
      "    - -1277.613624062459\n",
      "    - -920.0095619598072\n",
      "    - -999.0625355394654\n",
      "    - -1082.6982964278532\n",
      "    - -1587.0352879731852\n",
      "    - -1166.0754012216419\n",
      "    - -1495.2128331188565\n",
      "    - -1026.855264041456\n",
      "    - -1427.145210754385\n",
      "    - -1007.3125194420812\n",
      "    - -893.7656911521359\n",
      "    - -1598.7581532836653\n",
      "    - -876.418811216981\n",
      "    - -990.3538272881838\n",
      "    - -1348.0190341434343\n",
      "    - -1091.0200452850652\n",
      "    - -1145.9689270836122\n",
      "    - -1499.0175734949585\n",
      "    - -1580.1463133227721\n",
      "    - -1431.6904601716997\n",
      "    - -891.3291164487235\n",
      "    - -1126.8647402989284\n",
      "    - -994.3492503345808\n",
      "    - -1029.8832602537\n",
      "    - -1033.8117345831122\n",
      "    - -1591.0373010284309\n",
      "    - -1079.8273229532804\n",
      "    - -1401.089648611702\n",
      "    - -1007.2955515239696\n",
      "    - -1003.2378146415479\n",
      "    - -920.7640995174812\n",
      "    - -1009.9290564567373\n",
      "    - -1102.5580584956128\n",
      "    - -1294.096407380496\n",
      "    - -1021.8292129832743\n",
      "    - -1175.0515567122318\n",
      "    - -1370.504896797876\n",
      "    - -884.647861883548\n",
      "    - -1098.717845780862\n",
      "    - -1101.397270048886\n",
      "    - -995.7845095979786\n",
      "    - -1010.3558893966991\n",
      "    - -1007.7803809799426\n",
      "    - -1117.1971363605799\n",
      "    - -1694.65892043809\n",
      "    - -1129.5409864368173\n",
      "    - -1022.8885155896945\n",
      "    - -1011.5502886928542\n",
      "    - -1007.9076809726924\n",
      "    - -1580.2135519802637\n",
      "    - -1037.6054976497603\n",
      "    - -1012.9408042553854\n",
      "    - -1016.1242459505585\n",
      "    - -1563.8482173506943\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19421281737283735\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1492987390390425\n",
      "    mean_inference_ms: 0.9815836346966604\n",
      "    mean_raw_obs_processing_ms: 0.12252962317513084\n",
      "time_since_restore: 491.8213963508606\n",
      "time_this_iter_s: 10.008608102798462\n",
      "time_total_s: 491.8213963508606\n",
      "timers:\n",
      "  learn_throughput: 894.458\n",
      "  learn_time_ms: 4471.981\n",
      "  load_throughput: 17549389.121\n",
      "  load_time_ms: 0.228\n",
      "  training_iteration_time_ms: 10419.351\n",
      "  update_time_ms: 2.306\n",
      "timestamp: 1660564411\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 188000\n",
      "training_iteration: 47\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 192000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 192000\n",
      "  num_agent_steps_trained: 192000\n",
      "  num_env_steps_sampled: 192000\n",
      "  num_env_steps_trained: 192000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-53-41\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -865.5823664020162\n",
      "episode_reward_mean: -1177.1457110632218\n",
      "episode_reward_min: -1747.216560298775\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 960\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5406097173690796\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6943646669387817\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0067157079465687275\n",
      "        model: {}\n",
      "        policy_loss: 0.009101838804781437\n",
      "        total_loss: 9.925889015197754\n",
      "        vf_explained_var: -0.015766911208629608\n",
      "        vf_loss: 9.91315746307373\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 192000\n",
      "  num_agent_steps_trained: 192000\n",
      "  num_env_steps_sampled: 192000\n",
      "  num_env_steps_trained: 192000\n",
      "iterations_since_restore: 48\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 192000\n",
      "num_agent_steps_trained: 192000\n",
      "num_env_steps_sampled: 192000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 192000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.76428571428572\n",
      "  ram_util_percent: 58.99285714285714\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1940444271760063\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1491902515032118\n",
      "  mean_inference_ms: 0.9808629377803563\n",
      "  mean_raw_obs_processing_ms: 0.1224202960443004\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -865.5823664020162\n",
      "  episode_reward_mean: -1177.1457110632218\n",
      "  episode_reward_min: -1747.216560298775\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -938.9259336742056\n",
      "    - -995.8703777447818\n",
      "    - -1020.3109420232598\n",
      "    - -898.4529299378286\n",
      "    - -1687.5225647355876\n",
      "    - -1101.8881691280665\n",
      "    - -1500.2716146773184\n",
      "    - -1012.9173764574842\n",
      "    - -1384.959936841807\n",
      "    - -1747.216560298775\n",
      "    - -1022.631421858048\n",
      "    - -1015.3868487336493\n",
      "    - -865.5823664020162\n",
      "    - -1662.408526432367\n",
      "    - -903.3796272612228\n",
      "    - -994.1240511379629\n",
      "    - -1012.4115316439434\n",
      "    - -1009.3792782313988\n",
      "    - -1524.653407852374\n",
      "    - -1472.4778558180392\n",
      "    - -1107.3622105926956\n",
      "    - -960.0536153923642\n",
      "    - -1055.0554206760096\n",
      "    - -1353.924095271768\n",
      "    - -1432.9940677368206\n",
      "    - -1454.2018807208538\n",
      "    - -1277.613624062459\n",
      "    - -920.0095619598072\n",
      "    - -999.0625355394654\n",
      "    - -1082.6982964278532\n",
      "    - -1587.0352879731852\n",
      "    - -1166.0754012216419\n",
      "    - -1495.2128331188565\n",
      "    - -1026.855264041456\n",
      "    - -1427.145210754385\n",
      "    - -1007.3125194420812\n",
      "    - -893.7656911521359\n",
      "    - -1598.7581532836653\n",
      "    - -876.418811216981\n",
      "    - -990.3538272881838\n",
      "    - -1348.0190341434343\n",
      "    - -1091.0200452850652\n",
      "    - -1145.9689270836122\n",
      "    - -1499.0175734949585\n",
      "    - -1580.1463133227721\n",
      "    - -1431.6904601716997\n",
      "    - -891.3291164487235\n",
      "    - -1126.8647402989284\n",
      "    - -994.3492503345808\n",
      "    - -1029.8832602537\n",
      "    - -1033.8117345831122\n",
      "    - -1591.0373010284309\n",
      "    - -1079.8273229532804\n",
      "    - -1401.089648611702\n",
      "    - -1007.2955515239696\n",
      "    - -1003.2378146415479\n",
      "    - -920.7640995174812\n",
      "    - -1009.9290564567373\n",
      "    - -1102.5580584956128\n",
      "    - -1294.096407380496\n",
      "    - -1021.8292129832743\n",
      "    - -1175.0515567122318\n",
      "    - -1370.504896797876\n",
      "    - -884.647861883548\n",
      "    - -1098.717845780862\n",
      "    - -1101.397270048886\n",
      "    - -995.7845095979786\n",
      "    - -1010.3558893966991\n",
      "    - -1007.7803809799426\n",
      "    - -1117.1971363605799\n",
      "    - -1694.65892043809\n",
      "    - -1129.5409864368173\n",
      "    - -1022.8885155896945\n",
      "    - -1011.5502886928542\n",
      "    - -1007.9076809726924\n",
      "    - -1580.2135519802637\n",
      "    - -1037.6054976497603\n",
      "    - -1012.9408042553854\n",
      "    - -1016.1242459505585\n",
      "    - -1563.8482173506943\n",
      "    - -940.7427538800409\n",
      "    - -1407.099900087722\n",
      "    - -1307.6726169278318\n",
      "    - -1042.0431864925554\n",
      "    - -886.946335266757\n",
      "    - -1078.823247883579\n",
      "    - -1300.2381144336885\n",
      "    - -1707.2048670622678\n",
      "    - -978.2156820513302\n",
      "    - -1436.7879277890745\n",
      "    - -948.7166929854352\n",
      "    - -1404.566015109356\n",
      "    - -1089.5818403844717\n",
      "    - -1011.7619684410513\n",
      "    - -1296.6247252566216\n",
      "    - -1064.6752148460537\n",
      "    - -988.7235126144589\n",
      "    - -1199.031272485914\n",
      "    - -1016.4716596560045\n",
      "    - -1683.4829580166363\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1940444271760063\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1491902515032118\n",
      "    mean_inference_ms: 0.9808629377803563\n",
      "    mean_raw_obs_processing_ms: 0.1224202960443004\n",
      "time_since_restore: 501.39714074134827\n",
      "time_this_iter_s: 9.575744390487671\n",
      "time_total_s: 501.39714074134827\n",
      "timers:\n",
      "  learn_throughput: 904.517\n",
      "  learn_time_ms: 4422.248\n",
      "  load_throughput: 17527388.216\n",
      "  load_time_ms: 0.228\n",
      "  training_iteration_time_ms: 10204.657\n",
      "  update_time_ms: 2.314\n",
      "timestamp: 1660564421\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 192000\n",
      "training_iteration: 48\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 196000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 196000\n",
      "  num_agent_steps_trained: 196000\n",
      "  num_env_steps_sampled: 196000\n",
      "  num_env_steps_trained: 196000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-53-50\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -876.418811216981\n",
      "episode_reward_mean: -1169.3394783314272\n",
      "episode_reward_min: -1709.38182723091\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 980\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5406097173690796\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2250852584838867\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007100042887032032\n",
      "        model: {}\n",
      "        policy_loss: 0.007591642439365387\n",
      "        total_loss: 9.906814575195312\n",
      "        vf_explained_var: -0.015428387559950352\n",
      "        vf_loss: 9.895383834838867\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 196000\n",
      "  num_agent_steps_trained: 196000\n",
      "  num_env_steps_sampled: 196000\n",
      "  num_env_steps_trained: 196000\n",
      "iterations_since_restore: 49\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 196000\n",
      "num_agent_steps_trained: 196000\n",
      "num_env_steps_sampled: 196000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 196000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.646153846153844\n",
      "  ram_util_percent: 59.01538461538462\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19382517728936002\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1490462325547041\n",
      "  mean_inference_ms: 0.9799095173451459\n",
      "  mean_raw_obs_processing_ms: 0.12228738954931584\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -876.418811216981\n",
      "  episode_reward_mean: -1169.3394783314272\n",
      "  episode_reward_min: -1709.38182723091\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1107.3622105926956\n",
      "    - -960.0536153923642\n",
      "    - -1055.0554206760096\n",
      "    - -1353.924095271768\n",
      "    - -1432.9940677368206\n",
      "    - -1454.2018807208538\n",
      "    - -1277.613624062459\n",
      "    - -920.0095619598072\n",
      "    - -999.0625355394654\n",
      "    - -1082.6982964278532\n",
      "    - -1587.0352879731852\n",
      "    - -1166.0754012216419\n",
      "    - -1495.2128331188565\n",
      "    - -1026.855264041456\n",
      "    - -1427.145210754385\n",
      "    - -1007.3125194420812\n",
      "    - -893.7656911521359\n",
      "    - -1598.7581532836653\n",
      "    - -876.418811216981\n",
      "    - -990.3538272881838\n",
      "    - -1348.0190341434343\n",
      "    - -1091.0200452850652\n",
      "    - -1145.9689270836122\n",
      "    - -1499.0175734949585\n",
      "    - -1580.1463133227721\n",
      "    - -1431.6904601716997\n",
      "    - -891.3291164487235\n",
      "    - -1126.8647402989284\n",
      "    - -994.3492503345808\n",
      "    - -1029.8832602537\n",
      "    - -1033.8117345831122\n",
      "    - -1591.0373010284309\n",
      "    - -1079.8273229532804\n",
      "    - -1401.089648611702\n",
      "    - -1007.2955515239696\n",
      "    - -1003.2378146415479\n",
      "    - -920.7640995174812\n",
      "    - -1009.9290564567373\n",
      "    - -1102.5580584956128\n",
      "    - -1294.096407380496\n",
      "    - -1021.8292129832743\n",
      "    - -1175.0515567122318\n",
      "    - -1370.504896797876\n",
      "    - -884.647861883548\n",
      "    - -1098.717845780862\n",
      "    - -1101.397270048886\n",
      "    - -995.7845095979786\n",
      "    - -1010.3558893966991\n",
      "    - -1007.7803809799426\n",
      "    - -1117.1971363605799\n",
      "    - -1694.65892043809\n",
      "    - -1129.5409864368173\n",
      "    - -1022.8885155896945\n",
      "    - -1011.5502886928542\n",
      "    - -1007.9076809726924\n",
      "    - -1580.2135519802637\n",
      "    - -1037.6054976497603\n",
      "    - -1012.9408042553854\n",
      "    - -1016.1242459505585\n",
      "    - -1563.8482173506943\n",
      "    - -940.7427538800409\n",
      "    - -1407.099900087722\n",
      "    - -1307.6726169278318\n",
      "    - -1042.0431864925554\n",
      "    - -886.946335266757\n",
      "    - -1078.823247883579\n",
      "    - -1300.2381144336885\n",
      "    - -1707.2048670622678\n",
      "    - -978.2156820513302\n",
      "    - -1436.7879277890745\n",
      "    - -948.7166929854352\n",
      "    - -1404.566015109356\n",
      "    - -1089.5818403844717\n",
      "    - -1011.7619684410513\n",
      "    - -1296.6247252566216\n",
      "    - -1064.6752148460537\n",
      "    - -988.7235126144589\n",
      "    - -1199.031272485914\n",
      "    - -1016.4716596560045\n",
      "    - -1683.4829580166363\n",
      "    - -968.9578183427573\n",
      "    - -1025.8582263383803\n",
      "    - -1011.2092838421196\n",
      "    - -1086.1003400214452\n",
      "    - -1046.8313704769375\n",
      "    - -1647.5206142579825\n",
      "    - -1071.125382948226\n",
      "    - -1023.4412663515499\n",
      "    - -1026.7624023412322\n",
      "    - -1709.38182723091\n",
      "    - -989.7731910085804\n",
      "    - -994.4000764717432\n",
      "    - -1423.7427071902882\n",
      "    - -1020.2980598801196\n",
      "    - -1077.0056511125551\n",
      "    - -923.0560310211972\n",
      "    - -1185.814113025967\n",
      "    - -1007.7839345547839\n",
      "    - -1122.479898522787\n",
      "    - -1628.6058527711311\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19382517728936002\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1490462325547041\n",
      "    mean_inference_ms: 0.9799095173451459\n",
      "    mean_raw_obs_processing_ms: 0.12228738954931584\n",
      "time_since_restore: 510.9613070487976\n",
      "time_this_iter_s: 9.56416630744934\n",
      "time_total_s: 510.9613070487976\n",
      "timers:\n",
      "  learn_throughput: 916.184\n",
      "  learn_time_ms: 4365.937\n",
      "  load_throughput: 18404142.168\n",
      "  load_time_ms: 0.217\n",
      "  training_iteration_time_ms: 10147.393\n",
      "  update_time_ms: 2.283\n",
      "timestamp: 1660564430\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 196000\n",
      "training_iteration: 49\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 200000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 200000\n",
      "  num_agent_steps_trained: 200000\n",
      "  num_env_steps_sampled: 200000\n",
      "  num_env_steps_trained: 200000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-54-01\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -884.647861883548\n",
      "episode_reward_mean: -1152.5037942875979\n",
      "episode_reward_min: -1721.1620401482462\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1000\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5406097173690796\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9996206760406494\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02042306587100029\n",
      "        model: {}\n",
      "        policy_loss: 0.011821440421044827\n",
      "        total_loss: 9.833074569702148\n",
      "        vf_explained_var: -0.01222673337906599\n",
      "        vf_loss: 9.810213088989258\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 200000\n",
      "  num_agent_steps_trained: 200000\n",
      "  num_env_steps_sampled: 200000\n",
      "  num_env_steps_trained: 200000\n",
      "iterations_since_restore: 50\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 200000\n",
      "num_agent_steps_trained: 200000\n",
      "num_env_steps_sampled: 200000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 200000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.18\n",
      "  ram_util_percent: 58.99333333333333\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19361785788674937\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14890298114240244\n",
      "  mean_inference_ms: 0.9790599914175513\n",
      "  mean_raw_obs_processing_ms: 0.12215847159644155\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -884.647861883548\n",
      "  episode_reward_mean: -1152.5037942875979\n",
      "  episode_reward_min: -1721.1620401482462\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1348.0190341434343\n",
      "    - -1091.0200452850652\n",
      "    - -1145.9689270836122\n",
      "    - -1499.0175734949585\n",
      "    - -1580.1463133227721\n",
      "    - -1431.6904601716997\n",
      "    - -891.3291164487235\n",
      "    - -1126.8647402989284\n",
      "    - -994.3492503345808\n",
      "    - -1029.8832602537\n",
      "    - -1033.8117345831122\n",
      "    - -1591.0373010284309\n",
      "    - -1079.8273229532804\n",
      "    - -1401.089648611702\n",
      "    - -1007.2955515239696\n",
      "    - -1003.2378146415479\n",
      "    - -920.7640995174812\n",
      "    - -1009.9290564567373\n",
      "    - -1102.5580584956128\n",
      "    - -1294.096407380496\n",
      "    - -1021.8292129832743\n",
      "    - -1175.0515567122318\n",
      "    - -1370.504896797876\n",
      "    - -884.647861883548\n",
      "    - -1098.717845780862\n",
      "    - -1101.397270048886\n",
      "    - -995.7845095979786\n",
      "    - -1010.3558893966991\n",
      "    - -1007.7803809799426\n",
      "    - -1117.1971363605799\n",
      "    - -1694.65892043809\n",
      "    - -1129.5409864368173\n",
      "    - -1022.8885155896945\n",
      "    - -1011.5502886928542\n",
      "    - -1007.9076809726924\n",
      "    - -1580.2135519802637\n",
      "    - -1037.6054976497603\n",
      "    - -1012.9408042553854\n",
      "    - -1016.1242459505585\n",
      "    - -1563.8482173506943\n",
      "    - -940.7427538800409\n",
      "    - -1407.099900087722\n",
      "    - -1307.6726169278318\n",
      "    - -1042.0431864925554\n",
      "    - -886.946335266757\n",
      "    - -1078.823247883579\n",
      "    - -1300.2381144336885\n",
      "    - -1707.2048670622678\n",
      "    - -978.2156820513302\n",
      "    - -1436.7879277890745\n",
      "    - -948.7166929854352\n",
      "    - -1404.566015109356\n",
      "    - -1089.5818403844717\n",
      "    - -1011.7619684410513\n",
      "    - -1296.6247252566216\n",
      "    - -1064.6752148460537\n",
      "    - -988.7235126144589\n",
      "    - -1199.031272485914\n",
      "    - -1016.4716596560045\n",
      "    - -1683.4829580166363\n",
      "    - -968.9578183427573\n",
      "    - -1025.8582263383803\n",
      "    - -1011.2092838421196\n",
      "    - -1086.1003400214452\n",
      "    - -1046.8313704769375\n",
      "    - -1647.5206142579825\n",
      "    - -1071.125382948226\n",
      "    - -1023.4412663515499\n",
      "    - -1026.7624023412322\n",
      "    - -1709.38182723091\n",
      "    - -989.7731910085804\n",
      "    - -994.4000764717432\n",
      "    - -1423.7427071902882\n",
      "    - -1020.2980598801196\n",
      "    - -1077.0056511125551\n",
      "    - -923.0560310211972\n",
      "    - -1185.814113025967\n",
      "    - -1007.7839345547839\n",
      "    - -1122.479898522787\n",
      "    - -1628.6058527711311\n",
      "    - -1115.156647364688\n",
      "    - -1030.1621268776985\n",
      "    - -979.3364221076803\n",
      "    - -1205.7324698738419\n",
      "    - -1209.3659798631859\n",
      "    - -1479.0654561562621\n",
      "    - -940.4962984603691\n",
      "    - -913.9595739256733\n",
      "    - -1014.5534448174728\n",
      "    - -992.342083255569\n",
      "    - -956.2556376176999\n",
      "    - -1012.0359084828822\n",
      "    - -1721.1620401482462\n",
      "    - -1089.8866953166562\n",
      "    - -897.125084569185\n",
      "    - -1011.1086369763859\n",
      "    - -895.8874756669176\n",
      "    - -1111.7368871962233\n",
      "    - -1467.3897530258869\n",
      "    - -985.5812817871966\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19361785788674937\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14890298114240244\n",
      "    mean_inference_ms: 0.9790599914175513\n",
      "    mean_raw_obs_processing_ms: 0.12215847159644155\n",
      "time_since_restore: 521.3460698127747\n",
      "time_this_iter_s: 10.38476276397705\n",
      "time_total_s: 521.3460698127747\n",
      "timers:\n",
      "  learn_throughput: 911.273\n",
      "  learn_time_ms: 4389.463\n",
      "  load_throughput: 18299755.672\n",
      "  load_time_ms: 0.219\n",
      "  training_iteration_time_ms: 10046.712\n",
      "  update_time_ms: 2.28\n",
      "timestamp: 1660564441\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 200000\n",
      "training_iteration: 50\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 204000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 204000\n",
      "  num_agent_steps_trained: 204000\n",
      "  num_env_steps_sampled: 204000\n",
      "  num_env_steps_trained: 204000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-54-13\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -873.6727126307102\n",
      "episode_reward_mean: -1149.11921719295\n",
      "episode_reward_min: -1721.1620401482462\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1020\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.8109146356582642\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3969765901565552\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00700235553085804\n",
      "        model: {}\n",
      "        policy_loss: 0.013108622282743454\n",
      "        total_loss: 9.943036079406738\n",
      "        vf_explained_var: -0.020107675343751907\n",
      "        vf_loss: 9.924249649047852\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 204000\n",
      "  num_agent_steps_trained: 204000\n",
      "  num_env_steps_sampled: 204000\n",
      "  num_env_steps_trained: 204000\n",
      "iterations_since_restore: 51\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 204000\n",
      "num_agent_steps_trained: 204000\n",
      "num_env_steps_sampled: 204000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 204000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.688235294117646\n",
      "  ram_util_percent: 59.04705882352941\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19346867691235062\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14880585364025145\n",
      "  mean_inference_ms: 0.9785318038469271\n",
      "  mean_raw_obs_processing_ms: 0.12206977282330886\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -873.6727126307102\n",
      "  episode_reward_mean: -1149.11921719295\n",
      "  episode_reward_min: -1721.1620401482462\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1021.8292129832743\n",
      "    - -1175.0515567122318\n",
      "    - -1370.504896797876\n",
      "    - -884.647861883548\n",
      "    - -1098.717845780862\n",
      "    - -1101.397270048886\n",
      "    - -995.7845095979786\n",
      "    - -1010.3558893966991\n",
      "    - -1007.7803809799426\n",
      "    - -1117.1971363605799\n",
      "    - -1694.65892043809\n",
      "    - -1129.5409864368173\n",
      "    - -1022.8885155896945\n",
      "    - -1011.5502886928542\n",
      "    - -1007.9076809726924\n",
      "    - -1580.2135519802637\n",
      "    - -1037.6054976497603\n",
      "    - -1012.9408042553854\n",
      "    - -1016.1242459505585\n",
      "    - -1563.8482173506943\n",
      "    - -940.7427538800409\n",
      "    - -1407.099900087722\n",
      "    - -1307.6726169278318\n",
      "    - -1042.0431864925554\n",
      "    - -886.946335266757\n",
      "    - -1078.823247883579\n",
      "    - -1300.2381144336885\n",
      "    - -1707.2048670622678\n",
      "    - -978.2156820513302\n",
      "    - -1436.7879277890745\n",
      "    - -948.7166929854352\n",
      "    - -1404.566015109356\n",
      "    - -1089.5818403844717\n",
      "    - -1011.7619684410513\n",
      "    - -1296.6247252566216\n",
      "    - -1064.6752148460537\n",
      "    - -988.7235126144589\n",
      "    - -1199.031272485914\n",
      "    - -1016.4716596560045\n",
      "    - -1683.4829580166363\n",
      "    - -968.9578183427573\n",
      "    - -1025.8582263383803\n",
      "    - -1011.2092838421196\n",
      "    - -1086.1003400214452\n",
      "    - -1046.8313704769375\n",
      "    - -1647.5206142579825\n",
      "    - -1071.125382948226\n",
      "    - -1023.4412663515499\n",
      "    - -1026.7624023412322\n",
      "    - -1709.38182723091\n",
      "    - -989.7731910085804\n",
      "    - -994.4000764717432\n",
      "    - -1423.7427071902882\n",
      "    - -1020.2980598801196\n",
      "    - -1077.0056511125551\n",
      "    - -923.0560310211972\n",
      "    - -1185.814113025967\n",
      "    - -1007.7839345547839\n",
      "    - -1122.479898522787\n",
      "    - -1628.6058527711311\n",
      "    - -1115.156647364688\n",
      "    - -1030.1621268776985\n",
      "    - -979.3364221076803\n",
      "    - -1205.7324698738419\n",
      "    - -1209.3659798631859\n",
      "    - -1479.0654561562621\n",
      "    - -940.4962984603691\n",
      "    - -913.9595739256733\n",
      "    - -1014.5534448174728\n",
      "    - -992.342083255569\n",
      "    - -956.2556376176999\n",
      "    - -1012.0359084828822\n",
      "    - -1721.1620401482462\n",
      "    - -1089.8866953166562\n",
      "    - -897.125084569185\n",
      "    - -1011.1086369763859\n",
      "    - -895.8874756669176\n",
      "    - -1111.7368871962233\n",
      "    - -1467.3897530258869\n",
      "    - -985.5812817871966\n",
      "    - -1038.954771713833\n",
      "    - -1021.4664948838309\n",
      "    - -1435.9657646684368\n",
      "    - -1071.4662037884307\n",
      "    - -1441.646553820155\n",
      "    - -1003.1940714858292\n",
      "    - -1605.807884838127\n",
      "    - -1106.7717299441542\n",
      "    - -1400.6832330799987\n",
      "    - -1005.209484316596\n",
      "    - -1094.6671936883495\n",
      "    - -1198.8705940485788\n",
      "    - -1540.5040743104482\n",
      "    - -1000.3488694833744\n",
      "    - -1132.4155403041539\n",
      "    - -1009.1005161559043\n",
      "    - -1118.8154119113688\n",
      "    - -1203.4697081580937\n",
      "    - -940.447193334669\n",
      "    - -873.6727126307102\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19346867691235062\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14880585364025145\n",
      "    mean_inference_ms: 0.9785318038469271\n",
      "    mean_raw_obs_processing_ms: 0.12206977282330886\n",
      "time_since_restore: 533.2980327606201\n",
      "time_this_iter_s: 11.951962947845459\n",
      "time_total_s: 533.2980327606201\n",
      "timers:\n",
      "  learn_throughput: 878.938\n",
      "  learn_time_ms: 4550.949\n",
      "  load_throughput: 17534715.719\n",
      "  load_time_ms: 0.228\n",
      "  training_iteration_time_ms: 10284.018\n",
      "  update_time_ms: 2.275\n",
      "timestamp: 1660564453\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 204000\n",
      "training_iteration: 51\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 208000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 208000\n",
      "  num_agent_steps_trained: 208000\n",
      "  num_env_steps_sampled: 208000\n",
      "  num_env_steps_trained: 208000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-54-24\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -857.5583190981622\n",
      "episode_reward_mean: -1150.0179594474396\n",
      "episode_reward_min: -1750.9226221880378\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1040\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.8109146356582642\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4849016666412354\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009183854795992374\n",
      "        model: {}\n",
      "        policy_loss: 0.012061391957104206\n",
      "        total_loss: 9.889216423034668\n",
      "        vf_explained_var: -0.017482588067650795\n",
      "        vf_loss: 9.869709968566895\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 208000\n",
      "  num_agent_steps_trained: 208000\n",
      "  num_env_steps_sampled: 208000\n",
      "  num_env_steps_trained: 208000\n",
      "iterations_since_restore: 52\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 208000\n",
      "num_agent_steps_trained: 208000\n",
      "num_env_steps_sampled: 208000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 208000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 37.5\n",
      "  ram_util_percent: 59.0125\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19340517563185414\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14876630155761042\n",
      "  mean_inference_ms: 0.9783620622830622\n",
      "  mean_raw_obs_processing_ms: 0.12202791875581369\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -857.5583190981622\n",
      "  episode_reward_mean: -1150.0179594474396\n",
      "  episode_reward_min: -1750.9226221880378\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -940.7427538800409\n",
      "    - -1407.099900087722\n",
      "    - -1307.6726169278318\n",
      "    - -1042.0431864925554\n",
      "    - -886.946335266757\n",
      "    - -1078.823247883579\n",
      "    - -1300.2381144336885\n",
      "    - -1707.2048670622678\n",
      "    - -978.2156820513302\n",
      "    - -1436.7879277890745\n",
      "    - -948.7166929854352\n",
      "    - -1404.566015109356\n",
      "    - -1089.5818403844717\n",
      "    - -1011.7619684410513\n",
      "    - -1296.6247252566216\n",
      "    - -1064.6752148460537\n",
      "    - -988.7235126144589\n",
      "    - -1199.031272485914\n",
      "    - -1016.4716596560045\n",
      "    - -1683.4829580166363\n",
      "    - -968.9578183427573\n",
      "    - -1025.8582263383803\n",
      "    - -1011.2092838421196\n",
      "    - -1086.1003400214452\n",
      "    - -1046.8313704769375\n",
      "    - -1647.5206142579825\n",
      "    - -1071.125382948226\n",
      "    - -1023.4412663515499\n",
      "    - -1026.7624023412322\n",
      "    - -1709.38182723091\n",
      "    - -989.7731910085804\n",
      "    - -994.4000764717432\n",
      "    - -1423.7427071902882\n",
      "    - -1020.2980598801196\n",
      "    - -1077.0056511125551\n",
      "    - -923.0560310211972\n",
      "    - -1185.814113025967\n",
      "    - -1007.7839345547839\n",
      "    - -1122.479898522787\n",
      "    - -1628.6058527711311\n",
      "    - -1115.156647364688\n",
      "    - -1030.1621268776985\n",
      "    - -979.3364221076803\n",
      "    - -1205.7324698738419\n",
      "    - -1209.3659798631859\n",
      "    - -1479.0654561562621\n",
      "    - -940.4962984603691\n",
      "    - -913.9595739256733\n",
      "    - -1014.5534448174728\n",
      "    - -992.342083255569\n",
      "    - -956.2556376176999\n",
      "    - -1012.0359084828822\n",
      "    - -1721.1620401482462\n",
      "    - -1089.8866953166562\n",
      "    - -897.125084569185\n",
      "    - -1011.1086369763859\n",
      "    - -895.8874756669176\n",
      "    - -1111.7368871962233\n",
      "    - -1467.3897530258869\n",
      "    - -985.5812817871966\n",
      "    - -1038.954771713833\n",
      "    - -1021.4664948838309\n",
      "    - -1435.9657646684368\n",
      "    - -1071.4662037884307\n",
      "    - -1441.646553820155\n",
      "    - -1003.1940714858292\n",
      "    - -1605.807884838127\n",
      "    - -1106.7717299441542\n",
      "    - -1400.6832330799987\n",
      "    - -1005.209484316596\n",
      "    - -1094.6671936883495\n",
      "    - -1198.8705940485788\n",
      "    - -1540.5040743104482\n",
      "    - -1000.3488694833744\n",
      "    - -1132.4155403041539\n",
      "    - -1009.1005161559043\n",
      "    - -1118.8154119113688\n",
      "    - -1203.4697081580937\n",
      "    - -940.447193334669\n",
      "    - -873.6727126307102\n",
      "    - -980.4303339674691\n",
      "    - -1533.8882536945573\n",
      "    - -1089.654205705567\n",
      "    - -1750.9226221880378\n",
      "    - -886.4892384846638\n",
      "    - -1092.8889946022975\n",
      "    - -1283.2671179811837\n",
      "    - -1185.2337717847454\n",
      "    - -1470.6065616532185\n",
      "    - -1575.7934934929965\n",
      "    - -1015.4813661942354\n",
      "    - -1024.3405693027617\n",
      "    - -978.3876040612195\n",
      "    - -857.5583190981622\n",
      "    - -946.4046061351438\n",
      "    - -1465.8563512660403\n",
      "    - -916.1936481587699\n",
      "    - -990.3676833060448\n",
      "    - -889.8343742242862\n",
      "    - -1016.8203800062416\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19340517563185414\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14876630155761042\n",
      "    mean_inference_ms: 0.9783620622830622\n",
      "    mean_raw_obs_processing_ms: 0.12202791875581369\n",
      "time_since_restore: 544.1230924129486\n",
      "time_this_iter_s: 10.825059652328491\n",
      "time_total_s: 544.1230924129486\n",
      "timers:\n",
      "  learn_throughput: 881.378\n",
      "  learn_time_ms: 4538.346\n",
      "  load_throughput: 17608329.135\n",
      "  load_time_ms: 0.227\n",
      "  training_iteration_time_ms: 10329.762\n",
      "  update_time_ms: 2.494\n",
      "timestamp: 1660564464\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 208000\n",
      "training_iteration: 52\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 212000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 212000\n",
      "  num_agent_steps_trained: 212000\n",
      "  num_env_steps_sampled: 212000\n",
      "  num_env_steps_trained: 212000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-54-34\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -857.5583190981622\n",
      "episode_reward_mean: -1144.6037962030402\n",
      "episode_reward_min: -1789.1043505062696\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1060\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.8109146356582642\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6032103300094604\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017087887972593307\n",
      "        model: {}\n",
      "        policy_loss: 0.015134851448237896\n",
      "        total_loss: 9.948925018310547\n",
      "        vf_explained_var: -0.004514911212027073\n",
      "        vf_loss: 9.91993236541748\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 212000\n",
      "  num_agent_steps_trained: 212000\n",
      "  num_env_steps_sampled: 212000\n",
      "  num_env_steps_trained: 212000\n",
      "iterations_since_restore: 53\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 212000\n",
      "num_agent_steps_trained: 212000\n",
      "num_env_steps_sampled: 212000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 212000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.0\n",
      "  ram_util_percent: 59.0\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19338278394746544\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14876256991466974\n",
      "  mean_inference_ms: 0.9784516489914091\n",
      "  mean_raw_obs_processing_ms: 0.12201970069873443\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -857.5583190981622\n",
      "  episode_reward_mean: -1144.6037962030402\n",
      "  episode_reward_min: -1789.1043505062696\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -968.9578183427573\n",
      "    - -1025.8582263383803\n",
      "    - -1011.2092838421196\n",
      "    - -1086.1003400214452\n",
      "    - -1046.8313704769375\n",
      "    - -1647.5206142579825\n",
      "    - -1071.125382948226\n",
      "    - -1023.4412663515499\n",
      "    - -1026.7624023412322\n",
      "    - -1709.38182723091\n",
      "    - -989.7731910085804\n",
      "    - -994.4000764717432\n",
      "    - -1423.7427071902882\n",
      "    - -1020.2980598801196\n",
      "    - -1077.0056511125551\n",
      "    - -923.0560310211972\n",
      "    - -1185.814113025967\n",
      "    - -1007.7839345547839\n",
      "    - -1122.479898522787\n",
      "    - -1628.6058527711311\n",
      "    - -1115.156647364688\n",
      "    - -1030.1621268776985\n",
      "    - -979.3364221076803\n",
      "    - -1205.7324698738419\n",
      "    - -1209.3659798631859\n",
      "    - -1479.0654561562621\n",
      "    - -940.4962984603691\n",
      "    - -913.9595739256733\n",
      "    - -1014.5534448174728\n",
      "    - -992.342083255569\n",
      "    - -956.2556376176999\n",
      "    - -1012.0359084828822\n",
      "    - -1721.1620401482462\n",
      "    - -1089.8866953166562\n",
      "    - -897.125084569185\n",
      "    - -1011.1086369763859\n",
      "    - -895.8874756669176\n",
      "    - -1111.7368871962233\n",
      "    - -1467.3897530258869\n",
      "    - -985.5812817871966\n",
      "    - -1038.954771713833\n",
      "    - -1021.4664948838309\n",
      "    - -1435.9657646684368\n",
      "    - -1071.4662037884307\n",
      "    - -1441.646553820155\n",
      "    - -1003.1940714858292\n",
      "    - -1605.807884838127\n",
      "    - -1106.7717299441542\n",
      "    - -1400.6832330799987\n",
      "    - -1005.209484316596\n",
      "    - -1094.6671936883495\n",
      "    - -1198.8705940485788\n",
      "    - -1540.5040743104482\n",
      "    - -1000.3488694833744\n",
      "    - -1132.4155403041539\n",
      "    - -1009.1005161559043\n",
      "    - -1118.8154119113688\n",
      "    - -1203.4697081580937\n",
      "    - -940.447193334669\n",
      "    - -873.6727126307102\n",
      "    - -980.4303339674691\n",
      "    - -1533.8882536945573\n",
      "    - -1089.654205705567\n",
      "    - -1750.9226221880378\n",
      "    - -886.4892384846638\n",
      "    - -1092.8889946022975\n",
      "    - -1283.2671179811837\n",
      "    - -1185.2337717847454\n",
      "    - -1470.6065616532185\n",
      "    - -1575.7934934929965\n",
      "    - -1015.4813661942354\n",
      "    - -1024.3405693027617\n",
      "    - -978.3876040612195\n",
      "    - -857.5583190981622\n",
      "    - -946.4046061351438\n",
      "    - -1465.8563512660403\n",
      "    - -916.1936481587699\n",
      "    - -990.3676833060448\n",
      "    - -889.8343742242862\n",
      "    - -1016.8203800062416\n",
      "    - -1127.5036107199796\n",
      "    - -1022.9372860002231\n",
      "    - -1513.4109302526033\n",
      "    - -1461.9870532161547\n",
      "    - -993.6482102859229\n",
      "    - -1528.3640884415659\n",
      "    - -921.1953887398782\n",
      "    - -1287.1439378722391\n",
      "    - -984.4456662443404\n",
      "    - -929.8914628189431\n",
      "    - -862.5449477607443\n",
      "    - -897.1753321608603\n",
      "    - -924.1686224694218\n",
      "    - -1082.956836632465\n",
      "    - -951.177037564126\n",
      "    - -1789.1043505062696\n",
      "    - -1105.6056752777313\n",
      "    - -1416.6828231058962\n",
      "    - -1558.5904499569665\n",
      "    - -889.460457204592\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19338278394746544\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14876256991466974\n",
      "    mean_inference_ms: 0.9784516489914091\n",
      "    mean_raw_obs_processing_ms: 0.12201970069873443\n",
      "time_since_restore: 554.3206198215485\n",
      "time_this_iter_s: 10.197527408599854\n",
      "time_total_s: 554.3206198215485\n",
      "timers:\n",
      "  learn_throughput: 882.617\n",
      "  learn_time_ms: 4531.979\n",
      "  load_throughput: 17575126.755\n",
      "  load_time_ms: 0.228\n",
      "  training_iteration_time_ms: 10287.139\n",
      "  update_time_ms: 2.513\n",
      "timestamp: 1660564474\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 212000\n",
      "training_iteration: 53\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 216000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 216000\n",
      "  num_agent_steps_trained: 216000\n",
      "  num_env_steps_sampled: 216000\n",
      "  num_env_steps_trained: 216000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-54-44\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -857.5583190981622\n",
      "episode_reward_mean: -1130.2621431868247\n",
      "episode_reward_min: -1789.1043505062696\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1080\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.8109146356582642\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8141473531723022\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006391167175024748\n",
      "        model: {}\n",
      "        policy_loss: 0.011044210754334927\n",
      "        total_loss: 9.928954124450684\n",
      "        vf_explained_var: -0.010914160870015621\n",
      "        vf_loss: 9.912728309631348\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 216000\n",
      "  num_agent_steps_trained: 216000\n",
      "  num_env_steps_sampled: 216000\n",
      "  num_env_steps_trained: 216000\n",
      "iterations_since_restore: 54\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 216000\n",
      "num_agent_steps_trained: 216000\n",
      "num_env_steps_sampled: 216000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 216000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.285714285714286\n",
      "  ram_util_percent: 59.028571428571425\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19340852681091675\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14879378490764603\n",
      "  mean_inference_ms: 0.9787616321249797\n",
      "  mean_raw_obs_processing_ms: 0.12203672635520936\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -857.5583190981622\n",
      "  episode_reward_mean: -1130.2621431868247\n",
      "  episode_reward_min: -1789.1043505062696\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1115.156647364688\n",
      "    - -1030.1621268776985\n",
      "    - -979.3364221076803\n",
      "    - -1205.7324698738419\n",
      "    - -1209.3659798631859\n",
      "    - -1479.0654561562621\n",
      "    - -940.4962984603691\n",
      "    - -913.9595739256733\n",
      "    - -1014.5534448174728\n",
      "    - -992.342083255569\n",
      "    - -956.2556376176999\n",
      "    - -1012.0359084828822\n",
      "    - -1721.1620401482462\n",
      "    - -1089.8866953166562\n",
      "    - -897.125084569185\n",
      "    - -1011.1086369763859\n",
      "    - -895.8874756669176\n",
      "    - -1111.7368871962233\n",
      "    - -1467.3897530258869\n",
      "    - -985.5812817871966\n",
      "    - -1038.954771713833\n",
      "    - -1021.4664948838309\n",
      "    - -1435.9657646684368\n",
      "    - -1071.4662037884307\n",
      "    - -1441.646553820155\n",
      "    - -1003.1940714858292\n",
      "    - -1605.807884838127\n",
      "    - -1106.7717299441542\n",
      "    - -1400.6832330799987\n",
      "    - -1005.209484316596\n",
      "    - -1094.6671936883495\n",
      "    - -1198.8705940485788\n",
      "    - -1540.5040743104482\n",
      "    - -1000.3488694833744\n",
      "    - -1132.4155403041539\n",
      "    - -1009.1005161559043\n",
      "    - -1118.8154119113688\n",
      "    - -1203.4697081580937\n",
      "    - -940.447193334669\n",
      "    - -873.6727126307102\n",
      "    - -980.4303339674691\n",
      "    - -1533.8882536945573\n",
      "    - -1089.654205705567\n",
      "    - -1750.9226221880378\n",
      "    - -886.4892384846638\n",
      "    - -1092.8889946022975\n",
      "    - -1283.2671179811837\n",
      "    - -1185.2337717847454\n",
      "    - -1470.6065616532185\n",
      "    - -1575.7934934929965\n",
      "    - -1015.4813661942354\n",
      "    - -1024.3405693027617\n",
      "    - -978.3876040612195\n",
      "    - -857.5583190981622\n",
      "    - -946.4046061351438\n",
      "    - -1465.8563512660403\n",
      "    - -916.1936481587699\n",
      "    - -990.3676833060448\n",
      "    - -889.8343742242862\n",
      "    - -1016.8203800062416\n",
      "    - -1127.5036107199796\n",
      "    - -1022.9372860002231\n",
      "    - -1513.4109302526033\n",
      "    - -1461.9870532161547\n",
      "    - -993.6482102859229\n",
      "    - -1528.3640884415659\n",
      "    - -921.1953887398782\n",
      "    - -1287.1439378722391\n",
      "    - -984.4456662443404\n",
      "    - -929.8914628189431\n",
      "    - -862.5449477607443\n",
      "    - -897.1753321608603\n",
      "    - -924.1686224694218\n",
      "    - -1082.956836632465\n",
      "    - -951.177037564126\n",
      "    - -1789.1043505062696\n",
      "    - -1105.6056752777313\n",
      "    - -1416.6828231058962\n",
      "    - -1558.5904499569665\n",
      "    - -889.460457204592\n",
      "    - -1130.741406239743\n",
      "    - -1163.5994426463737\n",
      "    - -894.3645881379807\n",
      "    - -1459.7821775061877\n",
      "    - -908.0223896562018\n",
      "    - -1341.5532654948704\n",
      "    - -945.4682202417944\n",
      "    - -1029.6987521708627\n",
      "    - -887.0787633104496\n",
      "    - -987.9175024024977\n",
      "    - -1324.3729527346418\n",
      "    - -1025.1835323397697\n",
      "    - -1518.4148792762082\n",
      "    - -1021.6227446482093\n",
      "    - -882.3528706040513\n",
      "    - -902.7198791487908\n",
      "    - -956.1118998753801\n",
      "    - -905.9668090434134\n",
      "    - -964.0031711185214\n",
      "    - -1307.0074994932002\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19340852681091675\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14879378490764603\n",
      "    mean_inference_ms: 0.9787616321249797\n",
      "    mean_raw_obs_processing_ms: 0.12203672635520936\n",
      "time_since_restore: 564.3342306613922\n",
      "time_this_iter_s: 10.01361083984375\n",
      "time_total_s: 564.3342306613922\n",
      "timers:\n",
      "  learn_throughput: 885.881\n",
      "  learn_time_ms: 4515.279\n",
      "  load_throughput: 17599093.675\n",
      "  load_time_ms: 0.227\n",
      "  training_iteration_time_ms: 10265.258\n",
      "  update_time_ms: 2.416\n",
      "timestamp: 1660564484\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 216000\n",
      "training_iteration: 54\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 220000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 220000\n",
      "  num_agent_steps_trained: 220000\n",
      "  num_env_steps_sampled: 220000\n",
      "  num_env_steps_trained: 220000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-54-55\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -756.8886331249014\n",
      "episode_reward_mean: -1116.4833334469058\n",
      "episode_reward_min: -1789.1043505062696\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1100\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.8109146356582642\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5669823884963989\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007924162782728672\n",
      "        model: {}\n",
      "        policy_loss: 0.012298161163926125\n",
      "        total_loss: 9.867901802062988\n",
      "        vf_explained_var: -0.025640763342380524\n",
      "        vf_loss: 9.849176406860352\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 220000\n",
      "  num_agent_steps_trained: 220000\n",
      "  num_env_steps_sampled: 220000\n",
      "  num_env_steps_trained: 220000\n",
      "iterations_since_restore: 55\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 220000\n",
      "num_agent_steps_trained: 220000\n",
      "num_env_steps_sampled: 220000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 220000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 40.0\n",
      "  ram_util_percent: 59.0\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19344369493307506\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14883354428077453\n",
      "  mean_inference_ms: 0.9790274065514106\n",
      "  mean_raw_obs_processing_ms: 0.12205802859419027\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -756.8886331249014\n",
      "  episode_reward_mean: -1116.4833334469058\n",
      "  episode_reward_min: -1789.1043505062696\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1038.954771713833\n",
      "    - -1021.4664948838309\n",
      "    - -1435.9657646684368\n",
      "    - -1071.4662037884307\n",
      "    - -1441.646553820155\n",
      "    - -1003.1940714858292\n",
      "    - -1605.807884838127\n",
      "    - -1106.7717299441542\n",
      "    - -1400.6832330799987\n",
      "    - -1005.209484316596\n",
      "    - -1094.6671936883495\n",
      "    - -1198.8705940485788\n",
      "    - -1540.5040743104482\n",
      "    - -1000.3488694833744\n",
      "    - -1132.4155403041539\n",
      "    - -1009.1005161559043\n",
      "    - -1118.8154119113688\n",
      "    - -1203.4697081580937\n",
      "    - -940.447193334669\n",
      "    - -873.6727126307102\n",
      "    - -980.4303339674691\n",
      "    - -1533.8882536945573\n",
      "    - -1089.654205705567\n",
      "    - -1750.9226221880378\n",
      "    - -886.4892384846638\n",
      "    - -1092.8889946022975\n",
      "    - -1283.2671179811837\n",
      "    - -1185.2337717847454\n",
      "    - -1470.6065616532185\n",
      "    - -1575.7934934929965\n",
      "    - -1015.4813661942354\n",
      "    - -1024.3405693027617\n",
      "    - -978.3876040612195\n",
      "    - -857.5583190981622\n",
      "    - -946.4046061351438\n",
      "    - -1465.8563512660403\n",
      "    - -916.1936481587699\n",
      "    - -990.3676833060448\n",
      "    - -889.8343742242862\n",
      "    - -1016.8203800062416\n",
      "    - -1127.5036107199796\n",
      "    - -1022.9372860002231\n",
      "    - -1513.4109302526033\n",
      "    - -1461.9870532161547\n",
      "    - -993.6482102859229\n",
      "    - -1528.3640884415659\n",
      "    - -921.1953887398782\n",
      "    - -1287.1439378722391\n",
      "    - -984.4456662443404\n",
      "    - -929.8914628189431\n",
      "    - -862.5449477607443\n",
      "    - -897.1753321608603\n",
      "    - -924.1686224694218\n",
      "    - -1082.956836632465\n",
      "    - -951.177037564126\n",
      "    - -1789.1043505062696\n",
      "    - -1105.6056752777313\n",
      "    - -1416.6828231058962\n",
      "    - -1558.5904499569665\n",
      "    - -889.460457204592\n",
      "    - -1130.741406239743\n",
      "    - -1163.5994426463737\n",
      "    - -894.3645881379807\n",
      "    - -1459.7821775061877\n",
      "    - -908.0223896562018\n",
      "    - -1341.5532654948704\n",
      "    - -945.4682202417944\n",
      "    - -1029.6987521708627\n",
      "    - -887.0787633104496\n",
      "    - -987.9175024024977\n",
      "    - -1324.3729527346418\n",
      "    - -1025.1835323397697\n",
      "    - -1518.4148792762082\n",
      "    - -1021.6227446482093\n",
      "    - -882.3528706040513\n",
      "    - -902.7198791487908\n",
      "    - -956.1118998753801\n",
      "    - -905.9668090434134\n",
      "    - -964.0031711185214\n",
      "    - -1307.0074994932002\n",
      "    - -868.2160678518353\n",
      "    - -988.5316425304559\n",
      "    - -1152.960584922523\n",
      "    - -1446.9358910658318\n",
      "    - -941.9916758573589\n",
      "    - -970.1599117966132\n",
      "    - -986.721320990783\n",
      "    - -1107.3731625326816\n",
      "    - -1005.8988521990713\n",
      "    - -1011.1342465945258\n",
      "    - -756.8886331249014\n",
      "    - -971.6180332908663\n",
      "    - -1017.8187321397959\n",
      "    - -1571.534386486302\n",
      "    - -990.7988423352103\n",
      "    - -931.0461330507543\n",
      "    - -945.3894995165874\n",
      "    - -926.18100856074\n",
      "    - -900.2969814929221\n",
      "    - -1158.9633231580574\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19344369493307506\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14883354428077453\n",
      "    mean_inference_ms: 0.9790274065514106\n",
      "    mean_raw_obs_processing_ms: 0.12205802859419027\n",
      "time_since_restore: 574.8100304603577\n",
      "time_this_iter_s: 10.475799798965454\n",
      "time_total_s: 574.8100304603577\n",
      "timers:\n",
      "  learn_throughput: 875.731\n",
      "  learn_time_ms: 4567.615\n",
      "  load_throughput: 17753667.725\n",
      "  load_time_ms: 0.225\n",
      "  training_iteration_time_ms: 10320.509\n",
      "  update_time_ms: 2.453\n",
      "timestamp: 1660564495\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 220000\n",
      "training_iteration: 55\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 224000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 224000\n",
      "  num_agent_steps_trained: 224000\n",
      "  num_env_steps_sampled: 224000\n",
      "  num_env_steps_trained: 224000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-55-05\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -756.8886331249014\n",
      "episode_reward_mean: -1109.9090881492236\n",
      "episode_reward_min: -1789.1043505062696\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1120\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.8109146356582642\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3053768873214722\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006651537027209997\n",
      "        model: {}\n",
      "        policy_loss: 0.010639441199600697\n",
      "        total_loss: 9.834990501403809\n",
      "        vf_explained_var: -0.01534096710383892\n",
      "        vf_loss: 9.818957328796387\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 224000\n",
      "  num_agent_steps_trained: 224000\n",
      "  num_env_steps_sampled: 224000\n",
      "  num_env_steps_trained: 224000\n",
      "iterations_since_restore: 56\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 224000\n",
      "num_agent_steps_trained: 224000\n",
      "num_env_steps_sampled: 224000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 224000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 20.357142857142858\n",
      "  ram_util_percent: 58.99285714285714\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19344820024550338\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14884715585045882\n",
      "  mean_inference_ms: 0.9791153098165626\n",
      "  mean_raw_obs_processing_ms: 0.12205181779460521\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -756.8886331249014\n",
      "  episode_reward_mean: -1109.9090881492236\n",
      "  episode_reward_min: -1789.1043505062696\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -980.4303339674691\n",
      "    - -1533.8882536945573\n",
      "    - -1089.654205705567\n",
      "    - -1750.9226221880378\n",
      "    - -886.4892384846638\n",
      "    - -1092.8889946022975\n",
      "    - -1283.2671179811837\n",
      "    - -1185.2337717847454\n",
      "    - -1470.6065616532185\n",
      "    - -1575.7934934929965\n",
      "    - -1015.4813661942354\n",
      "    - -1024.3405693027617\n",
      "    - -978.3876040612195\n",
      "    - -857.5583190981622\n",
      "    - -946.4046061351438\n",
      "    - -1465.8563512660403\n",
      "    - -916.1936481587699\n",
      "    - -990.3676833060448\n",
      "    - -889.8343742242862\n",
      "    - -1016.8203800062416\n",
      "    - -1127.5036107199796\n",
      "    - -1022.9372860002231\n",
      "    - -1513.4109302526033\n",
      "    - -1461.9870532161547\n",
      "    - -993.6482102859229\n",
      "    - -1528.3640884415659\n",
      "    - -921.1953887398782\n",
      "    - -1287.1439378722391\n",
      "    - -984.4456662443404\n",
      "    - -929.8914628189431\n",
      "    - -862.5449477607443\n",
      "    - -897.1753321608603\n",
      "    - -924.1686224694218\n",
      "    - -1082.956836632465\n",
      "    - -951.177037564126\n",
      "    - -1789.1043505062696\n",
      "    - -1105.6056752777313\n",
      "    - -1416.6828231058962\n",
      "    - -1558.5904499569665\n",
      "    - -889.460457204592\n",
      "    - -1130.741406239743\n",
      "    - -1163.5994426463737\n",
      "    - -894.3645881379807\n",
      "    - -1459.7821775061877\n",
      "    - -908.0223896562018\n",
      "    - -1341.5532654948704\n",
      "    - -945.4682202417944\n",
      "    - -1029.6987521708627\n",
      "    - -887.0787633104496\n",
      "    - -987.9175024024977\n",
      "    - -1324.3729527346418\n",
      "    - -1025.1835323397697\n",
      "    - -1518.4148792762082\n",
      "    - -1021.6227446482093\n",
      "    - -882.3528706040513\n",
      "    - -902.7198791487908\n",
      "    - -956.1118998753801\n",
      "    - -905.9668090434134\n",
      "    - -964.0031711185214\n",
      "    - -1307.0074994932002\n",
      "    - -868.2160678518353\n",
      "    - -988.5316425304559\n",
      "    - -1152.960584922523\n",
      "    - -1446.9358910658318\n",
      "    - -941.9916758573589\n",
      "    - -970.1599117966132\n",
      "    - -986.721320990783\n",
      "    - -1107.3731625326816\n",
      "    - -1005.8988521990713\n",
      "    - -1011.1342465945258\n",
      "    - -756.8886331249014\n",
      "    - -971.6180332908663\n",
      "    - -1017.8187321397959\n",
      "    - -1571.534386486302\n",
      "    - -990.7988423352103\n",
      "    - -931.0461330507543\n",
      "    - -945.3894995165874\n",
      "    - -926.18100856074\n",
      "    - -900.2969814929221\n",
      "    - -1158.9633231580574\n",
      "    - -1178.7127244217347\n",
      "    - -1015.8456084810258\n",
      "    - -1410.1910084971717\n",
      "    - -879.3510875472685\n",
      "    - -1131.5577075525748\n",
      "    - -981.461446451575\n",
      "    - -1264.7180426366813\n",
      "    - -1415.4769703560899\n",
      "    - -1617.2785941739219\n",
      "    - -877.1597142816817\n",
      "    - -888.7932488889732\n",
      "    - -978.9169482721874\n",
      "    - -1519.0383522202226\n",
      "    - -888.8018804949206\n",
      "    - -880.7878506315894\n",
      "    - -967.7325863280979\n",
      "    - -1538.0489341640769\n",
      "    - -877.576798643874\n",
      "    - -984.598591072792\n",
      "    - -1290.0053816803488\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19344820024550338\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14884715585045882\n",
      "    mean_inference_ms: 0.9791153098165626\n",
      "    mean_raw_obs_processing_ms: 0.12205181779460521\n",
      "time_since_restore: 585.0175886154175\n",
      "time_this_iter_s: 10.207558155059814\n",
      "time_total_s: 585.0175886154175\n",
      "timers:\n",
      "  learn_throughput: 880.39\n",
      "  learn_time_ms: 4543.441\n",
      "  load_throughput: 17244543.119\n",
      "  load_time_ms: 0.232\n",
      "  training_iteration_time_ms: 10313.685\n",
      "  update_time_ms: 2.428\n",
      "timestamp: 1660564505\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 224000\n",
      "training_iteration: 56\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 228000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 228000\n",
      "  num_agent_steps_trained: 228000\n",
      "  num_env_steps_sampled: 228000\n",
      "  num_env_steps_trained: 228000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-55-15\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -755.5872822421768\n",
      "episode_reward_mean: -1105.197714316995\n",
      "episode_reward_min: -1789.1043505062696\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1140\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.8109146356582642\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3227790594100952\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008990146219730377\n",
      "        model: {}\n",
      "        policy_loss: 0.010817053727805614\n",
      "        total_loss: 9.83968734741211\n",
      "        vf_explained_var: -0.030666355043649673\n",
      "        vf_loss: 9.82158088684082\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 228000\n",
      "  num_agent_steps_trained: 228000\n",
      "  num_env_steps_sampled: 228000\n",
      "  num_env_steps_trained: 228000\n",
      "iterations_since_restore: 57\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 228000\n",
      "num_agent_steps_trained: 228000\n",
      "num_env_steps_sampled: 228000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 228000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 41.11333333333334\n",
      "  ram_util_percent: 58.97333333333333\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1933487783802336\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14879533356589442\n",
      "  mean_inference_ms: 0.9787147145247488\n",
      "  mean_raw_obs_processing_ms: 0.12199383723553392\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -755.5872822421768\n",
      "  episode_reward_mean: -1105.197714316995\n",
      "  episode_reward_min: -1789.1043505062696\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1127.5036107199796\n",
      "    - -1022.9372860002231\n",
      "    - -1513.4109302526033\n",
      "    - -1461.9870532161547\n",
      "    - -993.6482102859229\n",
      "    - -1528.3640884415659\n",
      "    - -921.1953887398782\n",
      "    - -1287.1439378722391\n",
      "    - -984.4456662443404\n",
      "    - -929.8914628189431\n",
      "    - -862.5449477607443\n",
      "    - -897.1753321608603\n",
      "    - -924.1686224694218\n",
      "    - -1082.956836632465\n",
      "    - -951.177037564126\n",
      "    - -1789.1043505062696\n",
      "    - -1105.6056752777313\n",
      "    - -1416.6828231058962\n",
      "    - -1558.5904499569665\n",
      "    - -889.460457204592\n",
      "    - -1130.741406239743\n",
      "    - -1163.5994426463737\n",
      "    - -894.3645881379807\n",
      "    - -1459.7821775061877\n",
      "    - -908.0223896562018\n",
      "    - -1341.5532654948704\n",
      "    - -945.4682202417944\n",
      "    - -1029.6987521708627\n",
      "    - -887.0787633104496\n",
      "    - -987.9175024024977\n",
      "    - -1324.3729527346418\n",
      "    - -1025.1835323397697\n",
      "    - -1518.4148792762082\n",
      "    - -1021.6227446482093\n",
      "    - -882.3528706040513\n",
      "    - -902.7198791487908\n",
      "    - -956.1118998753801\n",
      "    - -905.9668090434134\n",
      "    - -964.0031711185214\n",
      "    - -1307.0074994932002\n",
      "    - -868.2160678518353\n",
      "    - -988.5316425304559\n",
      "    - -1152.960584922523\n",
      "    - -1446.9358910658318\n",
      "    - -941.9916758573589\n",
      "    - -970.1599117966132\n",
      "    - -986.721320990783\n",
      "    - -1107.3731625326816\n",
      "    - -1005.8988521990713\n",
      "    - -1011.1342465945258\n",
      "    - -756.8886331249014\n",
      "    - -971.6180332908663\n",
      "    - -1017.8187321397959\n",
      "    - -1571.534386486302\n",
      "    - -990.7988423352103\n",
      "    - -931.0461330507543\n",
      "    - -945.3894995165874\n",
      "    - -926.18100856074\n",
      "    - -900.2969814929221\n",
      "    - -1158.9633231580574\n",
      "    - -1178.7127244217347\n",
      "    - -1015.8456084810258\n",
      "    - -1410.1910084971717\n",
      "    - -879.3510875472685\n",
      "    - -1131.5577075525748\n",
      "    - -981.461446451575\n",
      "    - -1264.7180426366813\n",
      "    - -1415.4769703560899\n",
      "    - -1617.2785941739219\n",
      "    - -877.1597142816817\n",
      "    - -888.7932488889732\n",
      "    - -978.9169482721874\n",
      "    - -1519.0383522202226\n",
      "    - -888.8018804949206\n",
      "    - -880.7878506315894\n",
      "    - -967.7325863280979\n",
      "    - -1538.0489341640769\n",
      "    - -877.576798643874\n",
      "    - -984.598591072792\n",
      "    - -1290.0053816803488\n",
      "    - -888.1169893176419\n",
      "    - -1047.025741648053\n",
      "    - -1004.236242483707\n",
      "    - -755.5872822421768\n",
      "    - -1545.4884406354038\n",
      "    - -883.9067343566842\n",
      "    - -870.7239471247846\n",
      "    - -1186.6370858754753\n",
      "    - -986.7143755502852\n",
      "    - -1167.236830306112\n",
      "    - -1438.488765120824\n",
      "    - -1028.4425311812543\n",
      "    - -883.6108944854309\n",
      "    - -1704.3891521403548\n",
      "    - -1101.458195730202\n",
      "    - -1562.840950724364\n",
      "    - -892.5129026870918\n",
      "    - -890.066947247743\n",
      "    - -1540.544262564095\n",
      "    - -1101.2538406631181\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1933487783802336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14879533356589442\n",
      "    mean_inference_ms: 0.9787147145247488\n",
      "    mean_raw_obs_processing_ms: 0.12199383723553392\n",
      "time_since_restore: 594.9417033195496\n",
      "time_this_iter_s: 9.92411470413208\n",
      "time_total_s: 594.9417033195496\n",
      "timers:\n",
      "  learn_throughput: 877.834\n",
      "  learn_time_ms: 4556.67\n",
      "  load_throughput: 17371314.972\n",
      "  load_time_ms: 0.23\n",
      "  training_iteration_time_ms: 10305.278\n",
      "  update_time_ms: 2.435\n",
      "timestamp: 1660564515\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 228000\n",
      "training_iteration: 57\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 232000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 232000\n",
      "  num_agent_steps_trained: 232000\n",
      "  num_env_steps_sampled: 232000\n",
      "  num_env_steps_trained: 232000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-55-25\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -739.2603987336628\n",
      "episode_reward_mean: -1097.33232212504\n",
      "episode_reward_min: -1704.3891521403548\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1160\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.8109146356582642\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3258713483810425\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012577608227729797\n",
      "        model: {}\n",
      "        policy_loss: 0.010874524712562561\n",
      "        total_loss: 9.954258918762207\n",
      "        vf_explained_var: -0.017391739413142204\n",
      "        vf_loss: 9.933185577392578\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 232000\n",
      "  num_agent_steps_trained: 232000\n",
      "  num_env_steps_sampled: 232000\n",
      "  num_env_steps_trained: 232000\n",
      "iterations_since_restore: 58\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 232000\n",
      "num_agent_steps_trained: 232000\n",
      "num_env_steps_sampled: 232000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 232000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 51.785714285714285\n",
      "  ram_util_percent: 59.035714285714285\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19325632389729616\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14874161466792543\n",
      "  mean_inference_ms: 0.9783077239694253\n",
      "  mean_raw_obs_processing_ms: 0.12192952791554179\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -739.2603987336628\n",
      "  episode_reward_mean: -1097.33232212504\n",
      "  episode_reward_min: -1704.3891521403548\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1130.741406239743\n",
      "    - -1163.5994426463737\n",
      "    - -894.3645881379807\n",
      "    - -1459.7821775061877\n",
      "    - -908.0223896562018\n",
      "    - -1341.5532654948704\n",
      "    - -945.4682202417944\n",
      "    - -1029.6987521708627\n",
      "    - -887.0787633104496\n",
      "    - -987.9175024024977\n",
      "    - -1324.3729527346418\n",
      "    - -1025.1835323397697\n",
      "    - -1518.4148792762082\n",
      "    - -1021.6227446482093\n",
      "    - -882.3528706040513\n",
      "    - -902.7198791487908\n",
      "    - -956.1118998753801\n",
      "    - -905.9668090434134\n",
      "    - -964.0031711185214\n",
      "    - -1307.0074994932002\n",
      "    - -868.2160678518353\n",
      "    - -988.5316425304559\n",
      "    - -1152.960584922523\n",
      "    - -1446.9358910658318\n",
      "    - -941.9916758573589\n",
      "    - -970.1599117966132\n",
      "    - -986.721320990783\n",
      "    - -1107.3731625326816\n",
      "    - -1005.8988521990713\n",
      "    - -1011.1342465945258\n",
      "    - -756.8886331249014\n",
      "    - -971.6180332908663\n",
      "    - -1017.8187321397959\n",
      "    - -1571.534386486302\n",
      "    - -990.7988423352103\n",
      "    - -931.0461330507543\n",
      "    - -945.3894995165874\n",
      "    - -926.18100856074\n",
      "    - -900.2969814929221\n",
      "    - -1158.9633231580574\n",
      "    - -1178.7127244217347\n",
      "    - -1015.8456084810258\n",
      "    - -1410.1910084971717\n",
      "    - -879.3510875472685\n",
      "    - -1131.5577075525748\n",
      "    - -981.461446451575\n",
      "    - -1264.7180426366813\n",
      "    - -1415.4769703560899\n",
      "    - -1617.2785941739219\n",
      "    - -877.1597142816817\n",
      "    - -888.7932488889732\n",
      "    - -978.9169482721874\n",
      "    - -1519.0383522202226\n",
      "    - -888.8018804949206\n",
      "    - -880.7878506315894\n",
      "    - -967.7325863280979\n",
      "    - -1538.0489341640769\n",
      "    - -877.576798643874\n",
      "    - -984.598591072792\n",
      "    - -1290.0053816803488\n",
      "    - -888.1169893176419\n",
      "    - -1047.025741648053\n",
      "    - -1004.236242483707\n",
      "    - -755.5872822421768\n",
      "    - -1545.4884406354038\n",
      "    - -883.9067343566842\n",
      "    - -870.7239471247846\n",
      "    - -1186.6370858754753\n",
      "    - -986.7143755502852\n",
      "    - -1167.236830306112\n",
      "    - -1438.488765120824\n",
      "    - -1028.4425311812543\n",
      "    - -883.6108944854309\n",
      "    - -1704.3891521403548\n",
      "    - -1101.458195730202\n",
      "    - -1562.840950724364\n",
      "    - -892.5129026870918\n",
      "    - -890.066947247743\n",
      "    - -1540.544262564095\n",
      "    - -1101.2538406631181\n",
      "    - -1660.1788803703953\n",
      "    - -1468.7829442962916\n",
      "    - -1423.8743991012097\n",
      "    - -739.2603987336628\n",
      "    - -905.8270280354643\n",
      "    - -1159.2364471726657\n",
      "    - -955.9211092921548\n",
      "    - -1178.8159663943889\n",
      "    - -854.8215216300131\n",
      "    - -990.1505173980762\n",
      "    - -879.1155536533688\n",
      "    - -1242.1634845657657\n",
      "    - -864.1120493784393\n",
      "    - -892.7479950667982\n",
      "    - -884.9604303017641\n",
      "    - -1077.1870476173967\n",
      "    - -1006.5116811937581\n",
      "    - -1101.89910089747\n",
      "    - -1517.5435072501537\n",
      "    - -1658.344885686177\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19325632389729616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14874161466792543\n",
      "    mean_inference_ms: 0.9783077239694253\n",
      "    mean_raw_obs_processing_ms: 0.12192952791554179\n",
      "time_since_restore: 604.9074296951294\n",
      "time_this_iter_s: 9.965726375579834\n",
      "time_total_s: 604.9074296951294\n",
      "timers:\n",
      "  learn_throughput: 876.167\n",
      "  learn_time_ms: 4565.339\n",
      "  load_throughput: 17450817.558\n",
      "  load_time_ms: 0.229\n",
      "  training_iteration_time_ms: 10344.191\n",
      "  update_time_ms: 2.487\n",
      "timestamp: 1660564525\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 232000\n",
      "training_iteration: 58\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 236000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 236000\n",
      "  num_agent_steps_trained: 236000\n",
      "  num_env_steps_sampled: 236000\n",
      "  num_env_steps_trained: 236000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-55-35\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -739.2603987336628\n",
      "episode_reward_mean: -1105.3230082874318\n",
      "episode_reward_min: -1758.767742745019\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1180\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.8109146356582642\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1407145261764526\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012316967360675335\n",
      "        model: {}\n",
      "        policy_loss: 0.011016267351806164\n",
      "        total_loss: 9.842911720275879\n",
      "        vf_explained_var: -0.011871980503201485\n",
      "        vf_loss: 9.821908950805664\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 236000\n",
      "  num_agent_steps_trained: 236000\n",
      "  num_env_steps_sampled: 236000\n",
      "  num_env_steps_trained: 236000\n",
      "iterations_since_restore: 59\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 236000\n",
      "num_agent_steps_trained: 236000\n",
      "num_env_steps_sampled: 236000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 236000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.114285714285714\n",
      "  ram_util_percent: 59.07142857142858\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19317027302961065\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1486888591883332\n",
      "  mean_inference_ms: 0.977965922830376\n",
      "  mean_raw_obs_processing_ms: 0.12187121177033715\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -739.2603987336628\n",
      "  episode_reward_mean: -1105.3230082874318\n",
      "  episode_reward_min: -1758.767742745019\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -868.2160678518353\n",
      "    - -988.5316425304559\n",
      "    - -1152.960584922523\n",
      "    - -1446.9358910658318\n",
      "    - -941.9916758573589\n",
      "    - -970.1599117966132\n",
      "    - -986.721320990783\n",
      "    - -1107.3731625326816\n",
      "    - -1005.8988521990713\n",
      "    - -1011.1342465945258\n",
      "    - -756.8886331249014\n",
      "    - -971.6180332908663\n",
      "    - -1017.8187321397959\n",
      "    - -1571.534386486302\n",
      "    - -990.7988423352103\n",
      "    - -931.0461330507543\n",
      "    - -945.3894995165874\n",
      "    - -926.18100856074\n",
      "    - -900.2969814929221\n",
      "    - -1158.9633231580574\n",
      "    - -1178.7127244217347\n",
      "    - -1015.8456084810258\n",
      "    - -1410.1910084971717\n",
      "    - -879.3510875472685\n",
      "    - -1131.5577075525748\n",
      "    - -981.461446451575\n",
      "    - -1264.7180426366813\n",
      "    - -1415.4769703560899\n",
      "    - -1617.2785941739219\n",
      "    - -877.1597142816817\n",
      "    - -888.7932488889732\n",
      "    - -978.9169482721874\n",
      "    - -1519.0383522202226\n",
      "    - -888.8018804949206\n",
      "    - -880.7878506315894\n",
      "    - -967.7325863280979\n",
      "    - -1538.0489341640769\n",
      "    - -877.576798643874\n",
      "    - -984.598591072792\n",
      "    - -1290.0053816803488\n",
      "    - -888.1169893176419\n",
      "    - -1047.025741648053\n",
      "    - -1004.236242483707\n",
      "    - -755.5872822421768\n",
      "    - -1545.4884406354038\n",
      "    - -883.9067343566842\n",
      "    - -870.7239471247846\n",
      "    - -1186.6370858754753\n",
      "    - -986.7143755502852\n",
      "    - -1167.236830306112\n",
      "    - -1438.488765120824\n",
      "    - -1028.4425311812543\n",
      "    - -883.6108944854309\n",
      "    - -1704.3891521403548\n",
      "    - -1101.458195730202\n",
      "    - -1562.840950724364\n",
      "    - -892.5129026870918\n",
      "    - -890.066947247743\n",
      "    - -1540.544262564095\n",
      "    - -1101.2538406631181\n",
      "    - -1660.1788803703953\n",
      "    - -1468.7829442962916\n",
      "    - -1423.8743991012097\n",
      "    - -739.2603987336628\n",
      "    - -905.8270280354643\n",
      "    - -1159.2364471726657\n",
      "    - -955.9211092921548\n",
      "    - -1178.8159663943889\n",
      "    - -854.8215216300131\n",
      "    - -990.1505173980762\n",
      "    - -879.1155536533688\n",
      "    - -1242.1634845657657\n",
      "    - -864.1120493784393\n",
      "    - -892.7479950667982\n",
      "    - -884.9604303017641\n",
      "    - -1077.1870476173967\n",
      "    - -1006.5116811937581\n",
      "    - -1101.89910089747\n",
      "    - -1517.5435072501537\n",
      "    - -1658.344885686177\n",
      "    - -1509.462891586215\n",
      "    - -882.2296655511706\n",
      "    - -890.7376182529049\n",
      "    - -1758.767742745019\n",
      "    - -1068.6619135240246\n",
      "    - -1018.6250818866893\n",
      "    - -1165.668073540861\n",
      "    - -898.1318454091979\n",
      "    - -1095.450970269547\n",
      "    - -1168.948184870489\n",
      "    - -1171.310755422747\n",
      "    - -1649.5831030026784\n",
      "    - -1273.1675421418902\n",
      "    - -974.2988369203395\n",
      "    - -963.4810450445258\n",
      "    - -882.5259857461617\n",
      "    - -1127.4284707209783\n",
      "    - -891.306649922093\n",
      "    - -1086.6755384263938\n",
      "    - -878.5894473444379\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19317027302961065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1486888591883332\n",
      "    mean_inference_ms: 0.977965922830376\n",
      "    mean_raw_obs_processing_ms: 0.12187121177033715\n",
      "time_since_restore: 615.0427935123444\n",
      "time_this_iter_s: 10.135363817214966\n",
      "time_total_s: 615.0427935123444\n",
      "timers:\n",
      "  learn_throughput: 872.662\n",
      "  learn_time_ms: 4583.677\n",
      "  load_throughput: 17287188.047\n",
      "  load_time_ms: 0.231\n",
      "  training_iteration_time_ms: 10401.267\n",
      "  update_time_ms: 2.507\n",
      "timestamp: 1660564535\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 236000\n",
      "training_iteration: 59\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 240000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 240000\n",
      "  num_agent_steps_trained: 240000\n",
      "  num_env_steps_sampled: 240000\n",
      "  num_env_steps_trained: 240000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-55-45\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -739.2603987336628\n",
      "episode_reward_mean: -1146.090113987916\n",
      "episode_reward_min: -1758.767742745019\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1200\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.8109146356582642\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.2029316425323486\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.002214069478213787\n",
      "        model: {}\n",
      "        policy_loss: 0.010019992478191853\n",
      "        total_loss: 9.911731719970703\n",
      "        vf_explained_var: -0.015371894463896751\n",
      "        vf_loss: 9.89991569519043\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 240000\n",
      "  num_agent_steps_trained: 240000\n",
      "  num_env_steps_sampled: 240000\n",
      "  num_env_steps_trained: 240000\n",
      "iterations_since_restore: 60\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 240000\n",
      "num_agent_steps_trained: 240000\n",
      "num_env_steps_sampled: 240000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 240000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.32666666666667\n",
      "  ram_util_percent: 59.10666666666668\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19306993688269405\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14862239029892158\n",
      "  mean_inference_ms: 0.977521216477772\n",
      "  mean_raw_obs_processing_ms: 0.12180779835752427\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -739.2603987336628\n",
      "  episode_reward_mean: -1146.090113987916\n",
      "  episode_reward_min: -1758.767742745019\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1178.7127244217347\n",
      "    - -1015.8456084810258\n",
      "    - -1410.1910084971717\n",
      "    - -879.3510875472685\n",
      "    - -1131.5577075525748\n",
      "    - -981.461446451575\n",
      "    - -1264.7180426366813\n",
      "    - -1415.4769703560899\n",
      "    - -1617.2785941739219\n",
      "    - -877.1597142816817\n",
      "    - -888.7932488889732\n",
      "    - -978.9169482721874\n",
      "    - -1519.0383522202226\n",
      "    - -888.8018804949206\n",
      "    - -880.7878506315894\n",
      "    - -967.7325863280979\n",
      "    - -1538.0489341640769\n",
      "    - -877.576798643874\n",
      "    - -984.598591072792\n",
      "    - -1290.0053816803488\n",
      "    - -888.1169893176419\n",
      "    - -1047.025741648053\n",
      "    - -1004.236242483707\n",
      "    - -755.5872822421768\n",
      "    - -1545.4884406354038\n",
      "    - -883.9067343566842\n",
      "    - -870.7239471247846\n",
      "    - -1186.6370858754753\n",
      "    - -986.7143755502852\n",
      "    - -1167.236830306112\n",
      "    - -1438.488765120824\n",
      "    - -1028.4425311812543\n",
      "    - -883.6108944854309\n",
      "    - -1704.3891521403548\n",
      "    - -1101.458195730202\n",
      "    - -1562.840950724364\n",
      "    - -892.5129026870918\n",
      "    - -890.066947247743\n",
      "    - -1540.544262564095\n",
      "    - -1101.2538406631181\n",
      "    - -1660.1788803703953\n",
      "    - -1468.7829442962916\n",
      "    - -1423.8743991012097\n",
      "    - -739.2603987336628\n",
      "    - -905.8270280354643\n",
      "    - -1159.2364471726657\n",
      "    - -955.9211092921548\n",
      "    - -1178.8159663943889\n",
      "    - -854.8215216300131\n",
      "    - -990.1505173980762\n",
      "    - -879.1155536533688\n",
      "    - -1242.1634845657657\n",
      "    - -864.1120493784393\n",
      "    - -892.7479950667982\n",
      "    - -884.9604303017641\n",
      "    - -1077.1870476173967\n",
      "    - -1006.5116811937581\n",
      "    - -1101.89910089747\n",
      "    - -1517.5435072501537\n",
      "    - -1658.344885686177\n",
      "    - -1509.462891586215\n",
      "    - -882.2296655511706\n",
      "    - -890.7376182529049\n",
      "    - -1758.767742745019\n",
      "    - -1068.6619135240246\n",
      "    - -1018.6250818866893\n",
      "    - -1165.668073540861\n",
      "    - -898.1318454091979\n",
      "    - -1095.450970269547\n",
      "    - -1168.948184870489\n",
      "    - -1171.310755422747\n",
      "    - -1649.5831030026784\n",
      "    - -1273.1675421418902\n",
      "    - -974.2988369203395\n",
      "    - -963.4810450445258\n",
      "    - -882.5259857461617\n",
      "    - -1127.4284707209783\n",
      "    - -891.306649922093\n",
      "    - -1086.6755384263938\n",
      "    - -878.5894473444379\n",
      "    - -1271.9226283622397\n",
      "    - -1226.7157402195087\n",
      "    - -896.0966478221118\n",
      "    - -1279.7991559524696\n",
      "    - -870.130013995187\n",
      "    - -918.7545026908426\n",
      "    - -1396.702458491494\n",
      "    - -1400.3429106527335\n",
      "    - -1230.1302758551685\n",
      "    - -1022.4410784917038\n",
      "    - -1001.1483325319892\n",
      "    - -864.9958698965706\n",
      "    - -1695.6457371775837\n",
      "    - -1520.1203116534743\n",
      "    - -1617.807690053197\n",
      "    - -936.4289849831665\n",
      "    - -1316.5860687214154\n",
      "    - -1504.0352863613944\n",
      "    - -1588.0590573966447\n",
      "    - -1169.3067482373317\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19306993688269405\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14862239029892158\n",
      "    mean_inference_ms: 0.977521216477772\n",
      "    mean_raw_obs_processing_ms: 0.12180779835752427\n",
      "time_since_restore: 625.4944107532501\n",
      "time_this_iter_s: 10.451617240905762\n",
      "time_total_s: 625.4944107532501\n",
      "timers:\n",
      "  learn_throughput: 867.558\n",
      "  learn_time_ms: 4610.644\n",
      "  load_throughput: 17225067.762\n",
      "  load_time_ms: 0.232\n",
      "  training_iteration_time_ms: 10407.716\n",
      "  update_time_ms: 2.563\n",
      "timestamp: 1660564545\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 240000\n",
      "training_iteration: 60\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 244000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 244000\n",
      "  num_agent_steps_trained: 244000\n",
      "  num_env_steps_sampled: 244000\n",
      "  num_env_steps_trained: 244000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-55-55\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -739.2603987336628\n",
      "episode_reward_mean: -1164.172099268267\n",
      "episode_reward_min: -1758.767742745019\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1220\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.4054573178291321\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0740272998809814\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0051074884831905365\n",
      "        model: {}\n",
      "        policy_loss: 0.010697114281356335\n",
      "        total_loss: 9.880712509155273\n",
      "        vf_explained_var: -0.017695993185043335\n",
      "        vf_loss: 9.86794376373291\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 244000\n",
      "  num_agent_steps_trained: 244000\n",
      "  num_env_steps_sampled: 244000\n",
      "  num_env_steps_trained: 244000\n",
      "iterations_since_restore: 61\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 244000\n",
      "num_agent_steps_trained: 244000\n",
      "num_env_steps_sampled: 244000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 244000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 46.42857142857143\n",
      "  ram_util_percent: 59.057142857142864\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1929212586259382\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14853006780553943\n",
      "  mean_inference_ms: 0.9768548310177856\n",
      "  mean_raw_obs_processing_ms: 0.12172970191428108\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -739.2603987336628\n",
      "  episode_reward_mean: -1164.172099268267\n",
      "  episode_reward_min: -1758.767742745019\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -888.1169893176419\n",
      "    - -1047.025741648053\n",
      "    - -1004.236242483707\n",
      "    - -755.5872822421768\n",
      "    - -1545.4884406354038\n",
      "    - -883.9067343566842\n",
      "    - -870.7239471247846\n",
      "    - -1186.6370858754753\n",
      "    - -986.7143755502852\n",
      "    - -1167.236830306112\n",
      "    - -1438.488765120824\n",
      "    - -1028.4425311812543\n",
      "    - -883.6108944854309\n",
      "    - -1704.3891521403548\n",
      "    - -1101.458195730202\n",
      "    - -1562.840950724364\n",
      "    - -892.5129026870918\n",
      "    - -890.066947247743\n",
      "    - -1540.544262564095\n",
      "    - -1101.2538406631181\n",
      "    - -1660.1788803703953\n",
      "    - -1468.7829442962916\n",
      "    - -1423.8743991012097\n",
      "    - -739.2603987336628\n",
      "    - -905.8270280354643\n",
      "    - -1159.2364471726657\n",
      "    - -955.9211092921548\n",
      "    - -1178.8159663943889\n",
      "    - -854.8215216300131\n",
      "    - -990.1505173980762\n",
      "    - -879.1155536533688\n",
      "    - -1242.1634845657657\n",
      "    - -864.1120493784393\n",
      "    - -892.7479950667982\n",
      "    - -884.9604303017641\n",
      "    - -1077.1870476173967\n",
      "    - -1006.5116811937581\n",
      "    - -1101.89910089747\n",
      "    - -1517.5435072501537\n",
      "    - -1658.344885686177\n",
      "    - -1509.462891586215\n",
      "    - -882.2296655511706\n",
      "    - -890.7376182529049\n",
      "    - -1758.767742745019\n",
      "    - -1068.6619135240246\n",
      "    - -1018.6250818866893\n",
      "    - -1165.668073540861\n",
      "    - -898.1318454091979\n",
      "    - -1095.450970269547\n",
      "    - -1168.948184870489\n",
      "    - -1171.310755422747\n",
      "    - -1649.5831030026784\n",
      "    - -1273.1675421418902\n",
      "    - -974.2988369203395\n",
      "    - -963.4810450445258\n",
      "    - -882.5259857461617\n",
      "    - -1127.4284707209783\n",
      "    - -891.306649922093\n",
      "    - -1086.6755384263938\n",
      "    - -878.5894473444379\n",
      "    - -1271.9226283622397\n",
      "    - -1226.7157402195087\n",
      "    - -896.0966478221118\n",
      "    - -1279.7991559524696\n",
      "    - -870.130013995187\n",
      "    - -918.7545026908426\n",
      "    - -1396.702458491494\n",
      "    - -1400.3429106527335\n",
      "    - -1230.1302758551685\n",
      "    - -1022.4410784917038\n",
      "    - -1001.1483325319892\n",
      "    - -864.9958698965706\n",
      "    - -1695.6457371775837\n",
      "    - -1520.1203116534743\n",
      "    - -1617.807690053197\n",
      "    - -936.4289849831665\n",
      "    - -1316.5860687214154\n",
      "    - -1504.0352863613944\n",
      "    - -1588.0590573966447\n",
      "    - -1169.3067482373317\n",
      "    - -1456.1691178432325\n",
      "    - -877.5408017369605\n",
      "    - -801.0983258629774\n",
      "    - -1647.7883005909887\n",
      "    - -978.2559661307697\n",
      "    - -1391.8902111086868\n",
      "    - -863.99698589505\n",
      "    - -1083.8411548084903\n",
      "    - -1529.80828811728\n",
      "    - -1373.4191550657636\n",
      "    - -1643.634363178496\n",
      "    - -1167.155686920298\n",
      "    - -1339.0956557470208\n",
      "    - -1585.8326037978015\n",
      "    - -1362.5565339323955\n",
      "    - -1073.891766214104\n",
      "    - -1160.6643120348594\n",
      "    - -1185.6891429253646\n",
      "    - -979.8522842724113\n",
      "    - -892.071348648958\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1929212586259382\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14853006780553943\n",
      "    mean_inference_ms: 0.9768548310177856\n",
      "    mean_raw_obs_processing_ms: 0.12172970191428108\n",
      "time_since_restore: 635.2229659557343\n",
      "time_this_iter_s: 9.72855520248413\n",
      "time_total_s: 635.2229659557343\n",
      "timers:\n",
      "  learn_throughput: 898.013\n",
      "  learn_time_ms: 4454.278\n",
      "  load_throughput: 18028385.988\n",
      "  load_time_ms: 0.222\n",
      "  training_iteration_time_ms: 10185.506\n",
      "  update_time_ms: 2.578\n",
      "timestamp: 1660564555\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 244000\n",
      "training_iteration: 61\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 248000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 248000\n",
      "  num_agent_steps_trained: 248000\n",
      "  num_env_steps_sampled: 248000\n",
      "  num_env_steps_trained: 248000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-56-05\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -739.2603987336628\n",
      "episode_reward_mean: -1166.1147655002103\n",
      "episode_reward_min: -1758.767742745019\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1240\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.4054573178291321\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3120226860046387\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012893839739263058\n",
      "        model: {}\n",
      "        policy_loss: 0.00854096282273531\n",
      "        total_loss: 9.858331680297852\n",
      "        vf_explained_var: -0.017793355509638786\n",
      "        vf_loss: 9.844562530517578\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 248000\n",
      "  num_agent_steps_trained: 248000\n",
      "  num_env_steps_sampled: 248000\n",
      "  num_env_steps_trained: 248000\n",
      "iterations_since_restore: 62\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 248000\n",
      "num_agent_steps_trained: 248000\n",
      "num_env_steps_sampled: 248000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 248000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 48.89333333333334\n",
      "  ram_util_percent: 59.00666666666667\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19282354512008112\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14846844630368017\n",
      "  mean_inference_ms: 0.9764134295291469\n",
      "  mean_raw_obs_processing_ms: 0.12167657269577037\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -739.2603987336628\n",
      "  episode_reward_mean: -1166.1147655002103\n",
      "  episode_reward_min: -1758.767742745019\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1660.1788803703953\n",
      "    - -1468.7829442962916\n",
      "    - -1423.8743991012097\n",
      "    - -739.2603987336628\n",
      "    - -905.8270280354643\n",
      "    - -1159.2364471726657\n",
      "    - -955.9211092921548\n",
      "    - -1178.8159663943889\n",
      "    - -854.8215216300131\n",
      "    - -990.1505173980762\n",
      "    - -879.1155536533688\n",
      "    - -1242.1634845657657\n",
      "    - -864.1120493784393\n",
      "    - -892.7479950667982\n",
      "    - -884.9604303017641\n",
      "    - -1077.1870476173967\n",
      "    - -1006.5116811937581\n",
      "    - -1101.89910089747\n",
      "    - -1517.5435072501537\n",
      "    - -1658.344885686177\n",
      "    - -1509.462891586215\n",
      "    - -882.2296655511706\n",
      "    - -890.7376182529049\n",
      "    - -1758.767742745019\n",
      "    - -1068.6619135240246\n",
      "    - -1018.6250818866893\n",
      "    - -1165.668073540861\n",
      "    - -898.1318454091979\n",
      "    - -1095.450970269547\n",
      "    - -1168.948184870489\n",
      "    - -1171.310755422747\n",
      "    - -1649.5831030026784\n",
      "    - -1273.1675421418902\n",
      "    - -974.2988369203395\n",
      "    - -963.4810450445258\n",
      "    - -882.5259857461617\n",
      "    - -1127.4284707209783\n",
      "    - -891.306649922093\n",
      "    - -1086.6755384263938\n",
      "    - -878.5894473444379\n",
      "    - -1271.9226283622397\n",
      "    - -1226.7157402195087\n",
      "    - -896.0966478221118\n",
      "    - -1279.7991559524696\n",
      "    - -870.130013995187\n",
      "    - -918.7545026908426\n",
      "    - -1396.702458491494\n",
      "    - -1400.3429106527335\n",
      "    - -1230.1302758551685\n",
      "    - -1022.4410784917038\n",
      "    - -1001.1483325319892\n",
      "    - -864.9958698965706\n",
      "    - -1695.6457371775837\n",
      "    - -1520.1203116534743\n",
      "    - -1617.807690053197\n",
      "    - -936.4289849831665\n",
      "    - -1316.5860687214154\n",
      "    - -1504.0352863613944\n",
      "    - -1588.0590573966447\n",
      "    - -1169.3067482373317\n",
      "    - -1456.1691178432325\n",
      "    - -877.5408017369605\n",
      "    - -801.0983258629774\n",
      "    - -1647.7883005909887\n",
      "    - -978.2559661307697\n",
      "    - -1391.8902111086868\n",
      "    - -863.99698589505\n",
      "    - -1083.8411548084903\n",
      "    - -1529.80828811728\n",
      "    - -1373.4191550657636\n",
      "    - -1643.634363178496\n",
      "    - -1167.155686920298\n",
      "    - -1339.0956557470208\n",
      "    - -1585.8326037978015\n",
      "    - -1362.5565339323955\n",
      "    - -1073.891766214104\n",
      "    - -1160.6643120348594\n",
      "    - -1185.6891429253646\n",
      "    - -979.8522842724113\n",
      "    - -892.071348648958\n",
      "    - -760.938102458583\n",
      "    - -869.4845278850247\n",
      "    - -1166.8332246266427\n",
      "    - -1338.043669279442\n",
      "    - -902.8760728622633\n",
      "    - -1088.23698559677\n",
      "    - -1661.9317278919068\n",
      "    - -1394.6227660624756\n",
      "    - -1642.86224519764\n",
      "    - -1067.1834679747253\n",
      "    - -1582.4779544332587\n",
      "    - -965.9305976288528\n",
      "    - -876.4422542301102\n",
      "    - -1001.5095856042536\n",
      "    - -975.2327955376976\n",
      "    - -1062.918799254547\n",
      "    - -871.3890355963144\n",
      "    - -1007.873013989039\n",
      "    - -1433.2459740415757\n",
      "    - -1003.5159351280001\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19282354512008112\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14846844630368017\n",
      "    mean_inference_ms: 0.9764134295291469\n",
      "    mean_raw_obs_processing_ms: 0.12167657269577037\n",
      "time_since_restore: 645.2667455673218\n",
      "time_this_iter_s: 10.043779611587524\n",
      "time_total_s: 645.2667455673218\n",
      "timers:\n",
      "  learn_throughput: 905.71\n",
      "  learn_time_ms: 4416.423\n",
      "  load_throughput: 17863304.94\n",
      "  load_time_ms: 0.224\n",
      "  training_iteration_time_ms: 10107.745\n",
      "  update_time_ms: 2.376\n",
      "timestamp: 1660564565\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 248000\n",
      "training_iteration: 62\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 252000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 252000\n",
      "  num_agent_steps_trained: 252000\n",
      "  num_env_steps_sampled: 252000\n",
      "  num_env_steps_trained: 252000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-56-16\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -760.938102458583\n",
      "episode_reward_mean: -1168.3467095377937\n",
      "episode_reward_min: -1758.767742745019\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1260\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.4054573178291321\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3829164505004883\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008965391665697098\n",
      "        model: {}\n",
      "        policy_loss: 0.010099999606609344\n",
      "        total_loss: 9.891300201416016\n",
      "        vf_explained_var: -0.008386739529669285\n",
      "        vf_loss: 9.877564430236816\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 252000\n",
      "  num_agent_steps_trained: 252000\n",
      "  num_env_steps_sampled: 252000\n",
      "  num_env_steps_trained: 252000\n",
      "iterations_since_restore: 63\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 252000\n",
      "num_agent_steps_trained: 252000\n",
      "num_env_steps_sampled: 252000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 252000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 52.77333333333333\n",
      "  ram_util_percent: 59.013333333333335\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1927357568943629\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1484220757167759\n",
      "  mean_inference_ms: 0.9760979675831397\n",
      "  mean_raw_obs_processing_ms: 0.12163535082497255\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -760.938102458583\n",
      "  episode_reward_mean: -1168.3467095377937\n",
      "  episode_reward_min: -1758.767742745019\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1509.462891586215\n",
      "    - -882.2296655511706\n",
      "    - -890.7376182529049\n",
      "    - -1758.767742745019\n",
      "    - -1068.6619135240246\n",
      "    - -1018.6250818866893\n",
      "    - -1165.668073540861\n",
      "    - -898.1318454091979\n",
      "    - -1095.450970269547\n",
      "    - -1168.948184870489\n",
      "    - -1171.310755422747\n",
      "    - -1649.5831030026784\n",
      "    - -1273.1675421418902\n",
      "    - -974.2988369203395\n",
      "    - -963.4810450445258\n",
      "    - -882.5259857461617\n",
      "    - -1127.4284707209783\n",
      "    - -891.306649922093\n",
      "    - -1086.6755384263938\n",
      "    - -878.5894473444379\n",
      "    - -1271.9226283622397\n",
      "    - -1226.7157402195087\n",
      "    - -896.0966478221118\n",
      "    - -1279.7991559524696\n",
      "    - -870.130013995187\n",
      "    - -918.7545026908426\n",
      "    - -1396.702458491494\n",
      "    - -1400.3429106527335\n",
      "    - -1230.1302758551685\n",
      "    - -1022.4410784917038\n",
      "    - -1001.1483325319892\n",
      "    - -864.9958698965706\n",
      "    - -1695.6457371775837\n",
      "    - -1520.1203116534743\n",
      "    - -1617.807690053197\n",
      "    - -936.4289849831665\n",
      "    - -1316.5860687214154\n",
      "    - -1504.0352863613944\n",
      "    - -1588.0590573966447\n",
      "    - -1169.3067482373317\n",
      "    - -1456.1691178432325\n",
      "    - -877.5408017369605\n",
      "    - -801.0983258629774\n",
      "    - -1647.7883005909887\n",
      "    - -978.2559661307697\n",
      "    - -1391.8902111086868\n",
      "    - -863.99698589505\n",
      "    - -1083.8411548084903\n",
      "    - -1529.80828811728\n",
      "    - -1373.4191550657636\n",
      "    - -1643.634363178496\n",
      "    - -1167.155686920298\n",
      "    - -1339.0956557470208\n",
      "    - -1585.8326037978015\n",
      "    - -1362.5565339323955\n",
      "    - -1073.891766214104\n",
      "    - -1160.6643120348594\n",
      "    - -1185.6891429253646\n",
      "    - -979.8522842724113\n",
      "    - -892.071348648958\n",
      "    - -760.938102458583\n",
      "    - -869.4845278850247\n",
      "    - -1166.8332246266427\n",
      "    - -1338.043669279442\n",
      "    - -902.8760728622633\n",
      "    - -1088.23698559677\n",
      "    - -1661.9317278919068\n",
      "    - -1394.6227660624756\n",
      "    - -1642.86224519764\n",
      "    - -1067.1834679747253\n",
      "    - -1582.4779544332587\n",
      "    - -965.9305976288528\n",
      "    - -876.4422542301102\n",
      "    - -1001.5095856042536\n",
      "    - -975.2327955376976\n",
      "    - -1062.918799254547\n",
      "    - -871.3890355963144\n",
      "    - -1007.873013989039\n",
      "    - -1433.2459740415757\n",
      "    - -1003.5159351280001\n",
      "    - -995.9265663306983\n",
      "    - -889.658853463097\n",
      "    - -1283.8932817040782\n",
      "    - -1561.2746357579974\n",
      "    - -1552.5788962560507\n",
      "    - -898.0478766102693\n",
      "    - -793.9152432542096\n",
      "    - -1053.4727333638937\n",
      "    - -1029.389258261106\n",
      "    - -887.4860931150148\n",
      "    - -1738.4743054520582\n",
      "    - -897.3639960147204\n",
      "    - -1001.9529580403282\n",
      "    - -982.2318284231441\n",
      "    - -1590.945531545357\n",
      "    - -1060.577021452099\n",
      "    - -1268.2886745419348\n",
      "    - -950.5841936266464\n",
      "    - -1360.4588587697976\n",
      "    - -888.1285458112206\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1927357568943629\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1484220757167759\n",
      "    mean_inference_ms: 0.9760979675831397\n",
      "    mean_raw_obs_processing_ms: 0.12163535082497255\n",
      "time_since_restore: 655.9133086204529\n",
      "time_this_iter_s: 10.646563053131104\n",
      "time_total_s: 655.9133086204529\n",
      "timers:\n",
      "  learn_throughput: 899.457\n",
      "  learn_time_ms: 4447.128\n",
      "  load_throughput: 17708693.266\n",
      "  load_time_ms: 0.226\n",
      "  training_iteration_time_ms: 10152.213\n",
      "  update_time_ms: 2.388\n",
      "timestamp: 1660564576\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 252000\n",
      "training_iteration: 63\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 256000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 256000\n",
      "  num_agent_steps_trained: 256000\n",
      "  num_env_steps_sampled: 256000\n",
      "  num_env_steps_trained: 256000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-56-25\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -760.938102458583\n",
      "episode_reward_mean: -1195.1853586590357\n",
      "episode_reward_min: -1738.4743054520582\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1280\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.4054573178291321\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.270495653152466\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012287129648029804\n",
      "        model: {}\n",
      "        policy_loss: 0.007133862469345331\n",
      "        total_loss: 9.893561363220215\n",
      "        vf_explained_var: -0.012204857543110847\n",
      "        vf_loss: 9.881444931030273\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 256000\n",
      "  num_agent_steps_trained: 256000\n",
      "  num_env_steps_sampled: 256000\n",
      "  num_env_steps_trained: 256000\n",
      "iterations_since_restore: 64\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 256000\n",
      "num_agent_steps_trained: 256000\n",
      "num_env_steps_sampled: 256000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 256000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 41.66923076923077\n",
      "  ram_util_percent: 59.00769230769231\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19260915876567108\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1483464473242058\n",
      "  mean_inference_ms: 0.9755183790341644\n",
      "  mean_raw_obs_processing_ms: 0.12156482138711532\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -760.938102458583\n",
      "  episode_reward_mean: -1195.1853586590357\n",
      "  episode_reward_min: -1738.4743054520582\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1271.9226283622397\n",
      "    - -1226.7157402195087\n",
      "    - -896.0966478221118\n",
      "    - -1279.7991559524696\n",
      "    - -870.130013995187\n",
      "    - -918.7545026908426\n",
      "    - -1396.702458491494\n",
      "    - -1400.3429106527335\n",
      "    - -1230.1302758551685\n",
      "    - -1022.4410784917038\n",
      "    - -1001.1483325319892\n",
      "    - -864.9958698965706\n",
      "    - -1695.6457371775837\n",
      "    - -1520.1203116534743\n",
      "    - -1617.807690053197\n",
      "    - -936.4289849831665\n",
      "    - -1316.5860687214154\n",
      "    - -1504.0352863613944\n",
      "    - -1588.0590573966447\n",
      "    - -1169.3067482373317\n",
      "    - -1456.1691178432325\n",
      "    - -877.5408017369605\n",
      "    - -801.0983258629774\n",
      "    - -1647.7883005909887\n",
      "    - -978.2559661307697\n",
      "    - -1391.8902111086868\n",
      "    - -863.99698589505\n",
      "    - -1083.8411548084903\n",
      "    - -1529.80828811728\n",
      "    - -1373.4191550657636\n",
      "    - -1643.634363178496\n",
      "    - -1167.155686920298\n",
      "    - -1339.0956557470208\n",
      "    - -1585.8326037978015\n",
      "    - -1362.5565339323955\n",
      "    - -1073.891766214104\n",
      "    - -1160.6643120348594\n",
      "    - -1185.6891429253646\n",
      "    - -979.8522842724113\n",
      "    - -892.071348648958\n",
      "    - -760.938102458583\n",
      "    - -869.4845278850247\n",
      "    - -1166.8332246266427\n",
      "    - -1338.043669279442\n",
      "    - -902.8760728622633\n",
      "    - -1088.23698559677\n",
      "    - -1661.9317278919068\n",
      "    - -1394.6227660624756\n",
      "    - -1642.86224519764\n",
      "    - -1067.1834679747253\n",
      "    - -1582.4779544332587\n",
      "    - -965.9305976288528\n",
      "    - -876.4422542301102\n",
      "    - -1001.5095856042536\n",
      "    - -975.2327955376976\n",
      "    - -1062.918799254547\n",
      "    - -871.3890355963144\n",
      "    - -1007.873013989039\n",
      "    - -1433.2459740415757\n",
      "    - -1003.5159351280001\n",
      "    - -995.9265663306983\n",
      "    - -889.658853463097\n",
      "    - -1283.8932817040782\n",
      "    - -1561.2746357579974\n",
      "    - -1552.5788962560507\n",
      "    - -898.0478766102693\n",
      "    - -793.9152432542096\n",
      "    - -1053.4727333638937\n",
      "    - -1029.389258261106\n",
      "    - -887.4860931150148\n",
      "    - -1738.4743054520582\n",
      "    - -897.3639960147204\n",
      "    - -1001.9529580403282\n",
      "    - -982.2318284231441\n",
      "    - -1590.945531545357\n",
      "    - -1060.577021452099\n",
      "    - -1268.2886745419348\n",
      "    - -950.5841936266464\n",
      "    - -1360.4588587697976\n",
      "    - -888.1285458112206\n",
      "    - -1304.465953239206\n",
      "    - -1433.663589552741\n",
      "    - -1261.3226085105114\n",
      "    - -1002.6578329926288\n",
      "    - -1051.863294236022\n",
      "    - -872.8997292448834\n",
      "    - -856.4027926660495\n",
      "    - -1057.7521595887686\n",
      "    - -1009.5505006895787\n",
      "    - -1560.4535796824794\n",
      "    - -1175.2048538040435\n",
      "    - -1195.0656972487352\n",
      "    - -1091.2966171046126\n",
      "    - -1676.863240927206\n",
      "    - -890.5952772680121\n",
      "    - -1654.3508178745888\n",
      "    - -1675.5567789092865\n",
      "    - -1459.9077058223027\n",
      "    - -1534.7293269641177\n",
      "    - -1274.3139181268039\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19260915876567108\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1483464473242058\n",
      "    mean_inference_ms: 0.9755183790341644\n",
      "    mean_raw_obs_processing_ms: 0.12156482138711532\n",
      "time_since_restore: 665.3016672134399\n",
      "time_this_iter_s: 9.38835859298706\n",
      "time_total_s: 665.3016672134399\n",
      "timers:\n",
      "  learn_throughput: 903.649\n",
      "  learn_time_ms: 4426.5\n",
      "  load_throughput: 17798871.207\n",
      "  load_time_ms: 0.225\n",
      "  training_iteration_time_ms: 10089.766\n",
      "  update_time_ms: 2.413\n",
      "timestamp: 1660564585\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 256000\n",
      "training_iteration: 64\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 260000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 260000\n",
      "  num_agent_steps_trained: 260000\n",
      "  num_env_steps_sampled: 260000\n",
      "  num_env_steps_trained: 260000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-56-35\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -753.3934615179858\n",
      "episode_reward_mean: -1179.6735491217376\n",
      "episode_reward_min: -1738.4743054520582\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1300\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.4054573178291321\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6334826946258545\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01399045716971159\n",
      "        model: {}\n",
      "        policy_loss: 0.013417872600257397\n",
      "        total_loss: 9.857128143310547\n",
      "        vf_explained_var: -0.007772907614707947\n",
      "        vf_loss: 9.83803653717041\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 260000\n",
      "  num_agent_steps_trained: 260000\n",
      "  num_env_steps_sampled: 260000\n",
      "  num_env_steps_trained: 260000\n",
      "iterations_since_restore: 65\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 260000\n",
      "num_agent_steps_trained: 260000\n",
      "num_env_steps_sampled: 260000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 260000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 47.61428571428571\n",
      "  ram_util_percent: 58.921428571428564\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19246351300374465\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14826397018269882\n",
      "  mean_inference_ms: 0.974872407777014\n",
      "  mean_raw_obs_processing_ms: 0.12148843053896297\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -753.3934615179858\n",
      "  episode_reward_mean: -1179.6735491217376\n",
      "  episode_reward_min: -1738.4743054520582\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1456.1691178432325\n",
      "    - -877.5408017369605\n",
      "    - -801.0983258629774\n",
      "    - -1647.7883005909887\n",
      "    - -978.2559661307697\n",
      "    - -1391.8902111086868\n",
      "    - -863.99698589505\n",
      "    - -1083.8411548084903\n",
      "    - -1529.80828811728\n",
      "    - -1373.4191550657636\n",
      "    - -1643.634363178496\n",
      "    - -1167.155686920298\n",
      "    - -1339.0956557470208\n",
      "    - -1585.8326037978015\n",
      "    - -1362.5565339323955\n",
      "    - -1073.891766214104\n",
      "    - -1160.6643120348594\n",
      "    - -1185.6891429253646\n",
      "    - -979.8522842724113\n",
      "    - -892.071348648958\n",
      "    - -760.938102458583\n",
      "    - -869.4845278850247\n",
      "    - -1166.8332246266427\n",
      "    - -1338.043669279442\n",
      "    - -902.8760728622633\n",
      "    - -1088.23698559677\n",
      "    - -1661.9317278919068\n",
      "    - -1394.6227660624756\n",
      "    - -1642.86224519764\n",
      "    - -1067.1834679747253\n",
      "    - -1582.4779544332587\n",
      "    - -965.9305976288528\n",
      "    - -876.4422542301102\n",
      "    - -1001.5095856042536\n",
      "    - -975.2327955376976\n",
      "    - -1062.918799254547\n",
      "    - -871.3890355963144\n",
      "    - -1007.873013989039\n",
      "    - -1433.2459740415757\n",
      "    - -1003.5159351280001\n",
      "    - -995.9265663306983\n",
      "    - -889.658853463097\n",
      "    - -1283.8932817040782\n",
      "    - -1561.2746357579974\n",
      "    - -1552.5788962560507\n",
      "    - -898.0478766102693\n",
      "    - -793.9152432542096\n",
      "    - -1053.4727333638937\n",
      "    - -1029.389258261106\n",
      "    - -887.4860931150148\n",
      "    - -1738.4743054520582\n",
      "    - -897.3639960147204\n",
      "    - -1001.9529580403282\n",
      "    - -982.2318284231441\n",
      "    - -1590.945531545357\n",
      "    - -1060.577021452099\n",
      "    - -1268.2886745419348\n",
      "    - -950.5841936266464\n",
      "    - -1360.4588587697976\n",
      "    - -888.1285458112206\n",
      "    - -1304.465953239206\n",
      "    - -1433.663589552741\n",
      "    - -1261.3226085105114\n",
      "    - -1002.6578329926288\n",
      "    - -1051.863294236022\n",
      "    - -872.8997292448834\n",
      "    - -856.4027926660495\n",
      "    - -1057.7521595887686\n",
      "    - -1009.5505006895787\n",
      "    - -1560.4535796824794\n",
      "    - -1175.2048538040435\n",
      "    - -1195.0656972487352\n",
      "    - -1091.2966171046126\n",
      "    - -1676.863240927206\n",
      "    - -890.5952772680121\n",
      "    - -1654.3508178745888\n",
      "    - -1675.5567789092865\n",
      "    - -1459.9077058223027\n",
      "    - -1534.7293269641177\n",
      "    - -1274.3139181268039\n",
      "    - -877.8408643250859\n",
      "    - -852.8746851631877\n",
      "    - -1113.4175592280062\n",
      "    - -1726.831834840334\n",
      "    - -1427.4561028413398\n",
      "    - -877.4084139202907\n",
      "    - -921.5040344814068\n",
      "    - -1063.9792332125203\n",
      "    - -1604.8659307599855\n",
      "    - -1482.6551359611385\n",
      "    - -753.3934615179858\n",
      "    - -1129.6543473852785\n",
      "    - -984.0127703754557\n",
      "    - -1481.826554469008\n",
      "    - -1394.9362820016559\n",
      "    - -906.3567285941868\n",
      "    - -971.2343850467128\n",
      "    - -882.7982300356235\n",
      "    - -1272.1393635989125\n",
      "    - -1450.8026280583213\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19246351300374465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14826397018269882\n",
      "    mean_inference_ms: 0.974872407777014\n",
      "    mean_raw_obs_processing_ms: 0.12148843053896297\n",
      "time_since_restore: 674.6964817047119\n",
      "time_this_iter_s: 9.394814491271973\n",
      "time_total_s: 674.6964817047119\n",
      "timers:\n",
      "  learn_throughput: 918.674\n",
      "  learn_time_ms: 4354.104\n",
      "  load_throughput: 17738650.878\n",
      "  load_time_ms: 0.225\n",
      "  training_iteration_time_ms: 9981.67\n",
      "  update_time_ms: 2.391\n",
      "timestamp: 1660564595\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 260000\n",
      "training_iteration: 65\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 264000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 264000\n",
      "  num_agent_steps_trained: 264000\n",
      "  num_env_steps_sampled: 264000\n",
      "  num_env_steps_trained: 264000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-56-44\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -753.3934615179858\n",
      "episode_reward_mean: -1187.9322368161745\n",
      "episode_reward_min: -1753.749753810162\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1320\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.4054573178291321\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3533339500427246\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006660667713731527\n",
      "        model: {}\n",
      "        policy_loss: 0.008726653642952442\n",
      "        total_loss: 9.918489456176758\n",
      "        vf_explained_var: -0.01635330729186535\n",
      "        vf_loss: 9.907062530517578\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 264000\n",
      "  num_agent_steps_trained: 264000\n",
      "  num_env_steps_sampled: 264000\n",
      "  num_env_steps_trained: 264000\n",
      "iterations_since_restore: 66\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 264000\n",
      "num_agent_steps_trained: 264000\n",
      "num_env_steps_sampled: 264000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 264000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.76923076923077\n",
      "  ram_util_percent: 59.0\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1922971910011438\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14816096165025158\n",
      "  mean_inference_ms: 0.9740746107638708\n",
      "  mean_raw_obs_processing_ms: 0.1213939716070891\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -753.3934615179858\n",
      "  episode_reward_mean: -1187.9322368161745\n",
      "  episode_reward_min: -1753.749753810162\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -760.938102458583\n",
      "    - -869.4845278850247\n",
      "    - -1166.8332246266427\n",
      "    - -1338.043669279442\n",
      "    - -902.8760728622633\n",
      "    - -1088.23698559677\n",
      "    - -1661.9317278919068\n",
      "    - -1394.6227660624756\n",
      "    - -1642.86224519764\n",
      "    - -1067.1834679747253\n",
      "    - -1582.4779544332587\n",
      "    - -965.9305976288528\n",
      "    - -876.4422542301102\n",
      "    - -1001.5095856042536\n",
      "    - -975.2327955376976\n",
      "    - -1062.918799254547\n",
      "    - -871.3890355963144\n",
      "    - -1007.873013989039\n",
      "    - -1433.2459740415757\n",
      "    - -1003.5159351280001\n",
      "    - -995.9265663306983\n",
      "    - -889.658853463097\n",
      "    - -1283.8932817040782\n",
      "    - -1561.2746357579974\n",
      "    - -1552.5788962560507\n",
      "    - -898.0478766102693\n",
      "    - -793.9152432542096\n",
      "    - -1053.4727333638937\n",
      "    - -1029.389258261106\n",
      "    - -887.4860931150148\n",
      "    - -1738.4743054520582\n",
      "    - -897.3639960147204\n",
      "    - -1001.9529580403282\n",
      "    - -982.2318284231441\n",
      "    - -1590.945531545357\n",
      "    - -1060.577021452099\n",
      "    - -1268.2886745419348\n",
      "    - -950.5841936266464\n",
      "    - -1360.4588587697976\n",
      "    - -888.1285458112206\n",
      "    - -1304.465953239206\n",
      "    - -1433.663589552741\n",
      "    - -1261.3226085105114\n",
      "    - -1002.6578329926288\n",
      "    - -1051.863294236022\n",
      "    - -872.8997292448834\n",
      "    - -856.4027926660495\n",
      "    - -1057.7521595887686\n",
      "    - -1009.5505006895787\n",
      "    - -1560.4535796824794\n",
      "    - -1175.2048538040435\n",
      "    - -1195.0656972487352\n",
      "    - -1091.2966171046126\n",
      "    - -1676.863240927206\n",
      "    - -890.5952772680121\n",
      "    - -1654.3508178745888\n",
      "    - -1675.5567789092865\n",
      "    - -1459.9077058223027\n",
      "    - -1534.7293269641177\n",
      "    - -1274.3139181268039\n",
      "    - -877.8408643250859\n",
      "    - -852.8746851631877\n",
      "    - -1113.4175592280062\n",
      "    - -1726.831834840334\n",
      "    - -1427.4561028413398\n",
      "    - -877.4084139202907\n",
      "    - -921.5040344814068\n",
      "    - -1063.9792332125203\n",
      "    - -1604.8659307599855\n",
      "    - -1482.6551359611385\n",
      "    - -753.3934615179858\n",
      "    - -1129.6543473852785\n",
      "    - -984.0127703754557\n",
      "    - -1481.826554469008\n",
      "    - -1394.9362820016559\n",
      "    - -906.3567285941868\n",
      "    - -971.2343850467128\n",
      "    - -882.7982300356235\n",
      "    - -1272.1393635989125\n",
      "    - -1450.8026280583213\n",
      "    - -1397.4805977157291\n",
      "    - -1370.8338847609632\n",
      "    - -887.1588719128924\n",
      "    - -893.5873164476403\n",
      "    - -1467.4801267121773\n",
      "    - -1277.3186680322794\n",
      "    - -1599.5684830579435\n",
      "    - -1538.0757350283563\n",
      "    - -999.912724582168\n",
      "    - -1753.749753810162\n",
      "    - -1361.2861093516326\n",
      "    - -1206.542320732669\n",
      "    - -1632.7330572434144\n",
      "    - -1160.421514133986\n",
      "    - -869.8230561247464\n",
      "    - -966.0284883925926\n",
      "    - -1096.3397860628827\n",
      "    - -1453.7853340061376\n",
      "    - -1408.1891409672385\n",
      "    - -879.8058052000023\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1922971910011438\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14816096165025158\n",
      "    mean_inference_ms: 0.9740746107638708\n",
      "    mean_raw_obs_processing_ms: 0.1213939716070891\n",
      "time_since_restore: 683.9492144584656\n",
      "time_this_iter_s: 9.252732753753662\n",
      "time_total_s: 683.9492144584656\n",
      "timers:\n",
      "  learn_throughput: 923.785\n",
      "  learn_time_ms: 4330.01\n",
      "  load_throughput: 18297759.843\n",
      "  load_time_ms: 0.219\n",
      "  training_iteration_time_ms: 9886.187\n",
      "  update_time_ms: 2.398\n",
      "timestamp: 1660564604\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 264000\n",
      "training_iteration: 66\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 268000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 268000\n",
      "  num_agent_steps_trained: 268000\n",
      "  num_env_steps_sampled: 268000\n",
      "  num_env_steps_trained: 268000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-56-54\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -753.3193777391818\n",
      "episode_reward_mean: -1199.5151717778308\n",
      "episode_reward_min: -1753.749753810162\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1340\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.4054573178291321\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8360639810562134\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018720531836152077\n",
      "        model: {}\n",
      "        policy_loss: 0.01573062129318714\n",
      "        total_loss: 9.83376693725586\n",
      "        vf_explained_var: -0.01852259412407875\n",
      "        vf_loss: 9.810445785522461\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 268000\n",
      "  num_agent_steps_trained: 268000\n",
      "  num_env_steps_sampled: 268000\n",
      "  num_env_steps_trained: 268000\n",
      "iterations_since_restore: 67\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 268000\n",
      "num_agent_steps_trained: 268000\n",
      "num_env_steps_sampled: 268000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 268000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.49285714285714\n",
      "  ram_util_percent: 59.0\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19207158598176372\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1480198043385363\n",
      "  mean_inference_ms: 0.97300733835909\n",
      "  mean_raw_obs_processing_ms: 0.12126828957599606\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -753.3193777391818\n",
      "  episode_reward_mean: -1199.5151717778308\n",
      "  episode_reward_min: -1753.749753810162\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -995.9265663306983\n",
      "    - -889.658853463097\n",
      "    - -1283.8932817040782\n",
      "    - -1561.2746357579974\n",
      "    - -1552.5788962560507\n",
      "    - -898.0478766102693\n",
      "    - -793.9152432542096\n",
      "    - -1053.4727333638937\n",
      "    - -1029.389258261106\n",
      "    - -887.4860931150148\n",
      "    - -1738.4743054520582\n",
      "    - -897.3639960147204\n",
      "    - -1001.9529580403282\n",
      "    - -982.2318284231441\n",
      "    - -1590.945531545357\n",
      "    - -1060.577021452099\n",
      "    - -1268.2886745419348\n",
      "    - -950.5841936266464\n",
      "    - -1360.4588587697976\n",
      "    - -888.1285458112206\n",
      "    - -1304.465953239206\n",
      "    - -1433.663589552741\n",
      "    - -1261.3226085105114\n",
      "    - -1002.6578329926288\n",
      "    - -1051.863294236022\n",
      "    - -872.8997292448834\n",
      "    - -856.4027926660495\n",
      "    - -1057.7521595887686\n",
      "    - -1009.5505006895787\n",
      "    - -1560.4535796824794\n",
      "    - -1175.2048538040435\n",
      "    - -1195.0656972487352\n",
      "    - -1091.2966171046126\n",
      "    - -1676.863240927206\n",
      "    - -890.5952772680121\n",
      "    - -1654.3508178745888\n",
      "    - -1675.5567789092865\n",
      "    - -1459.9077058223027\n",
      "    - -1534.7293269641177\n",
      "    - -1274.3139181268039\n",
      "    - -877.8408643250859\n",
      "    - -852.8746851631877\n",
      "    - -1113.4175592280062\n",
      "    - -1726.831834840334\n",
      "    - -1427.4561028413398\n",
      "    - -877.4084139202907\n",
      "    - -921.5040344814068\n",
      "    - -1063.9792332125203\n",
      "    - -1604.8659307599855\n",
      "    - -1482.6551359611385\n",
      "    - -753.3934615179858\n",
      "    - -1129.6543473852785\n",
      "    - -984.0127703754557\n",
      "    - -1481.826554469008\n",
      "    - -1394.9362820016559\n",
      "    - -906.3567285941868\n",
      "    - -971.2343850467128\n",
      "    - -882.7982300356235\n",
      "    - -1272.1393635989125\n",
      "    - -1450.8026280583213\n",
      "    - -1397.4805977157291\n",
      "    - -1370.8338847609632\n",
      "    - -887.1588719128924\n",
      "    - -893.5873164476403\n",
      "    - -1467.4801267121773\n",
      "    - -1277.3186680322794\n",
      "    - -1599.5684830579435\n",
      "    - -1538.0757350283563\n",
      "    - -999.912724582168\n",
      "    - -1753.749753810162\n",
      "    - -1361.2861093516326\n",
      "    - -1206.542320732669\n",
      "    - -1632.7330572434144\n",
      "    - -1160.421514133986\n",
      "    - -869.8230561247464\n",
      "    - -966.0284883925926\n",
      "    - -1096.3397860628827\n",
      "    - -1453.7853340061376\n",
      "    - -1408.1891409672385\n",
      "    - -879.8058052000023\n",
      "    - -1128.562987145899\n",
      "    - -889.3703786027046\n",
      "    - -1119.9548930587555\n",
      "    - -1636.0238243187518\n",
      "    - -897.2878625998062\n",
      "    - -913.8517829384733\n",
      "    - -1377.461640114686\n",
      "    - -989.91854724525\n",
      "    - -1691.215491083353\n",
      "    - -1019.5742139708229\n",
      "    - -1556.9590147829424\n",
      "    - -1300.938777418588\n",
      "    - -1564.0895676454068\n",
      "    - -883.2432021063009\n",
      "    - -1342.510379563391\n",
      "    - -1558.6054327805348\n",
      "    - -875.806051227717\n",
      "    - -1361.5939650758903\n",
      "    - -753.3193777391818\n",
      "    - -971.5548420262649\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19207158598176372\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1480198043385363\n",
      "    mean_inference_ms: 0.97300733835909\n",
      "    mean_raw_obs_processing_ms: 0.12126828957599606\n",
      "time_since_restore: 693.3580694198608\n",
      "time_this_iter_s: 9.408854961395264\n",
      "time_total_s: 693.3580694198608\n",
      "timers:\n",
      "  learn_throughput: 930.816\n",
      "  learn_time_ms: 4297.305\n",
      "  load_throughput: 18293769.491\n",
      "  load_time_ms: 0.219\n",
      "  training_iteration_time_ms: 9834.686\n",
      "  update_time_ms: 2.4\n",
      "timestamp: 1660564614\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 268000\n",
      "training_iteration: 67\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 272000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 272000\n",
      "  num_agent_steps_trained: 272000\n",
      "  num_env_steps_sampled: 272000\n",
      "  num_env_steps_trained: 272000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-57-03\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -753.3193777391818\n",
      "episode_reward_mean: -1196.7725929654277\n",
      "episode_reward_min: -1753.749753810162\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1360\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.4054573178291321\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2344461679458618\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015049569308757782\n",
      "        model: {}\n",
      "        policy_loss: 0.010143769904971123\n",
      "        total_loss: 9.843534469604492\n",
      "        vf_explained_var: -0.01891205832362175\n",
      "        vf_loss: 9.827289581298828\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 272000\n",
      "  num_agent_steps_trained: 272000\n",
      "  num_env_steps_sampled: 272000\n",
      "  num_env_steps_trained: 272000\n",
      "iterations_since_restore: 68\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 272000\n",
      "num_agent_steps_trained: 272000\n",
      "num_env_steps_sampled: 272000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 272000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.86923076923077\n",
      "  ram_util_percent: 58.9153846153846\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19180805447540675\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14784226570493675\n",
      "  mean_inference_ms: 0.9716907821585592\n",
      "  mean_raw_obs_processing_ms: 0.12111589798531167\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -753.3193777391818\n",
      "  episode_reward_mean: -1196.7725929654277\n",
      "  episode_reward_min: -1753.749753810162\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1304.465953239206\n",
      "    - -1433.663589552741\n",
      "    - -1261.3226085105114\n",
      "    - -1002.6578329926288\n",
      "    - -1051.863294236022\n",
      "    - -872.8997292448834\n",
      "    - -856.4027926660495\n",
      "    - -1057.7521595887686\n",
      "    - -1009.5505006895787\n",
      "    - -1560.4535796824794\n",
      "    - -1175.2048538040435\n",
      "    - -1195.0656972487352\n",
      "    - -1091.2966171046126\n",
      "    - -1676.863240927206\n",
      "    - -890.5952772680121\n",
      "    - -1654.3508178745888\n",
      "    - -1675.5567789092865\n",
      "    - -1459.9077058223027\n",
      "    - -1534.7293269641177\n",
      "    - -1274.3139181268039\n",
      "    - -877.8408643250859\n",
      "    - -852.8746851631877\n",
      "    - -1113.4175592280062\n",
      "    - -1726.831834840334\n",
      "    - -1427.4561028413398\n",
      "    - -877.4084139202907\n",
      "    - -921.5040344814068\n",
      "    - -1063.9792332125203\n",
      "    - -1604.8659307599855\n",
      "    - -1482.6551359611385\n",
      "    - -753.3934615179858\n",
      "    - -1129.6543473852785\n",
      "    - -984.0127703754557\n",
      "    - -1481.826554469008\n",
      "    - -1394.9362820016559\n",
      "    - -906.3567285941868\n",
      "    - -971.2343850467128\n",
      "    - -882.7982300356235\n",
      "    - -1272.1393635989125\n",
      "    - -1450.8026280583213\n",
      "    - -1397.4805977157291\n",
      "    - -1370.8338847609632\n",
      "    - -887.1588719128924\n",
      "    - -893.5873164476403\n",
      "    - -1467.4801267121773\n",
      "    - -1277.3186680322794\n",
      "    - -1599.5684830579435\n",
      "    - -1538.0757350283563\n",
      "    - -999.912724582168\n",
      "    - -1753.749753810162\n",
      "    - -1361.2861093516326\n",
      "    - -1206.542320732669\n",
      "    - -1632.7330572434144\n",
      "    - -1160.421514133986\n",
      "    - -869.8230561247464\n",
      "    - -966.0284883925926\n",
      "    - -1096.3397860628827\n",
      "    - -1453.7853340061376\n",
      "    - -1408.1891409672385\n",
      "    - -879.8058052000023\n",
      "    - -1128.562987145899\n",
      "    - -889.3703786027046\n",
      "    - -1119.9548930587555\n",
      "    - -1636.0238243187518\n",
      "    - -897.2878625998062\n",
      "    - -913.8517829384733\n",
      "    - -1377.461640114686\n",
      "    - -989.91854724525\n",
      "    - -1691.215491083353\n",
      "    - -1019.5742139708229\n",
      "    - -1556.9590147829424\n",
      "    - -1300.938777418588\n",
      "    - -1564.0895676454068\n",
      "    - -883.2432021063009\n",
      "    - -1342.510379563391\n",
      "    - -1558.6054327805348\n",
      "    - -875.806051227717\n",
      "    - -1361.5939650758903\n",
      "    - -753.3193777391818\n",
      "    - -971.5548420262649\n",
      "    - -1098.963011742978\n",
      "    - -961.4487161863467\n",
      "    - -884.7279484474475\n",
      "    - -965.2178726972343\n",
      "    - -959.594919751276\n",
      "    - -976.1410379984542\n",
      "    - -1412.942375348603\n",
      "    - -1748.984162123915\n",
      "    - -1284.2769234093776\n",
      "    - -887.6688539802756\n",
      "    - -1266.1819058098154\n",
      "    - -1474.8172135179243\n",
      "    - -1563.5557110681912\n",
      "    - -1202.6688668361373\n",
      "    - -869.0619470554208\n",
      "    - -780.6988775786272\n",
      "    - -849.1831038798842\n",
      "    - -883.3816785101136\n",
      "    - -942.1739032347922\n",
      "    - -1398.702441376626\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19180805447540675\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14784226570493675\n",
      "    mean_inference_ms: 0.9716907821585592\n",
      "    mean_raw_obs_processing_ms: 0.12111589798531167\n",
      "time_since_restore: 702.7233157157898\n",
      "time_this_iter_s: 9.365246295928955\n",
      "time_total_s: 702.7233157157898\n",
      "timers:\n",
      "  learn_throughput: 936.444\n",
      "  learn_time_ms: 4271.477\n",
      "  load_throughput: 18315737.991\n",
      "  load_time_ms: 0.218\n",
      "  training_iteration_time_ms: 9774.739\n",
      "  update_time_ms: 2.363\n",
      "timestamp: 1660564623\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 272000\n",
      "training_iteration: 68\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 276000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 276000\n",
      "  num_agent_steps_trained: 276000\n",
      "  num_env_steps_sampled: 276000\n",
      "  num_env_steps_trained: 276000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-57-12\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -753.3193777391818\n",
      "episode_reward_mean: -1185.6803610928773\n",
      "episode_reward_min: -1753.749753810162\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1380\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.4054573178291321\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7754207849502563\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008331301622092724\n",
      "        model: {}\n",
      "        policy_loss: 0.007551795337349176\n",
      "        total_loss: 9.923572540283203\n",
      "        vf_explained_var: -0.004556924104690552\n",
      "        vf_loss: 9.912642478942871\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 276000\n",
      "  num_agent_steps_trained: 276000\n",
      "  num_env_steps_sampled: 276000\n",
      "  num_env_steps_trained: 276000\n",
      "iterations_since_restore: 69\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 276000\n",
      "num_agent_steps_trained: 276000\n",
      "num_env_steps_sampled: 276000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 276000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.785714285714285\n",
      "  ram_util_percent: 58.914285714285704\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19154996106533811\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14767308646717164\n",
      "  mean_inference_ms: 0.9704782825333438\n",
      "  mean_raw_obs_processing_ms: 0.12097367174154455\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -753.3193777391818\n",
      "  episode_reward_mean: -1185.6803610928773\n",
      "  episode_reward_min: -1753.749753810162\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -877.8408643250859\n",
      "    - -852.8746851631877\n",
      "    - -1113.4175592280062\n",
      "    - -1726.831834840334\n",
      "    - -1427.4561028413398\n",
      "    - -877.4084139202907\n",
      "    - -921.5040344814068\n",
      "    - -1063.9792332125203\n",
      "    - -1604.8659307599855\n",
      "    - -1482.6551359611385\n",
      "    - -753.3934615179858\n",
      "    - -1129.6543473852785\n",
      "    - -984.0127703754557\n",
      "    - -1481.826554469008\n",
      "    - -1394.9362820016559\n",
      "    - -906.3567285941868\n",
      "    - -971.2343850467128\n",
      "    - -882.7982300356235\n",
      "    - -1272.1393635989125\n",
      "    - -1450.8026280583213\n",
      "    - -1397.4805977157291\n",
      "    - -1370.8338847609632\n",
      "    - -887.1588719128924\n",
      "    - -893.5873164476403\n",
      "    - -1467.4801267121773\n",
      "    - -1277.3186680322794\n",
      "    - -1599.5684830579435\n",
      "    - -1538.0757350283563\n",
      "    - -999.912724582168\n",
      "    - -1753.749753810162\n",
      "    - -1361.2861093516326\n",
      "    - -1206.542320732669\n",
      "    - -1632.7330572434144\n",
      "    - -1160.421514133986\n",
      "    - -869.8230561247464\n",
      "    - -966.0284883925926\n",
      "    - -1096.3397860628827\n",
      "    - -1453.7853340061376\n",
      "    - -1408.1891409672385\n",
      "    - -879.8058052000023\n",
      "    - -1128.562987145899\n",
      "    - -889.3703786027046\n",
      "    - -1119.9548930587555\n",
      "    - -1636.0238243187518\n",
      "    - -897.2878625998062\n",
      "    - -913.8517829384733\n",
      "    - -1377.461640114686\n",
      "    - -989.91854724525\n",
      "    - -1691.215491083353\n",
      "    - -1019.5742139708229\n",
      "    - -1556.9590147829424\n",
      "    - -1300.938777418588\n",
      "    - -1564.0895676454068\n",
      "    - -883.2432021063009\n",
      "    - -1342.510379563391\n",
      "    - -1558.6054327805348\n",
      "    - -875.806051227717\n",
      "    - -1361.5939650758903\n",
      "    - -753.3193777391818\n",
      "    - -971.5548420262649\n",
      "    - -1098.963011742978\n",
      "    - -961.4487161863467\n",
      "    - -884.7279484474475\n",
      "    - -965.2178726972343\n",
      "    - -959.594919751276\n",
      "    - -976.1410379984542\n",
      "    - -1412.942375348603\n",
      "    - -1748.984162123915\n",
      "    - -1284.2769234093776\n",
      "    - -887.6688539802756\n",
      "    - -1266.1819058098154\n",
      "    - -1474.8172135179243\n",
      "    - -1563.5557110681912\n",
      "    - -1202.6688668361373\n",
      "    - -869.0619470554208\n",
      "    - -780.6988775786272\n",
      "    - -849.1831038798842\n",
      "    - -883.3816785101136\n",
      "    - -942.1739032347922\n",
      "    - -1398.702441376626\n",
      "    - -1352.3601506956397\n",
      "    - -1481.2845270326288\n",
      "    - -1452.5953402469913\n",
      "    - -1004.0348524862907\n",
      "    - -1635.3903494950312\n",
      "    - -1544.2112156041217\n",
      "    - -861.0102409052035\n",
      "    - -1407.7974212540887\n",
      "    - -890.2270557318512\n",
      "    - -1378.968351210593\n",
      "    - -877.8264361114992\n",
      "    - -880.3590977239969\n",
      "    - -1312.9748075336483\n",
      "    - -900.9968184363522\n",
      "    - -1212.5164385728363\n",
      "    - -1000.1608581971524\n",
      "    - -1427.8126647348797\n",
      "    - -864.285415611517\n",
      "    - -1137.7407804246964\n",
      "    - -1307.1402651885212\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19154996106533811\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14767308646717164\n",
      "    mean_inference_ms: 0.9704782825333438\n",
      "    mean_raw_obs_processing_ms: 0.12097367174154455\n",
      "time_since_restore: 712.1871137619019\n",
      "time_this_iter_s: 9.46379804611206\n",
      "time_total_s: 712.1871137619019\n",
      "timers:\n",
      "  learn_throughput: 942.695\n",
      "  learn_time_ms: 4243.156\n",
      "  load_throughput: 18552710.384\n",
      "  load_time_ms: 0.216\n",
      "  training_iteration_time_ms: 9707.551\n",
      "  update_time_ms: 2.348\n",
      "timestamp: 1660564632\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 276000\n",
      "training_iteration: 69\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 280000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 280000\n",
      "  num_agent_steps_trained: 280000\n",
      "  num_env_steps_sampled: 280000\n",
      "  num_env_steps_trained: 280000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-57-22\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -753.3193777391818\n",
      "episode_reward_mean: -1184.7181741945135\n",
      "episode_reward_min: -1753.749753810162\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1400\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.4054573178291321\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.398193120956421\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010406944900751114\n",
      "        model: {}\n",
      "        policy_loss: 0.011667323298752308\n",
      "        total_loss: 9.900619506835938\n",
      "        vf_explained_var: -0.01391176600009203\n",
      "        vf_loss: 9.884732246398926\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 280000\n",
      "  num_agent_steps_trained: 280000\n",
      "  num_env_steps_sampled: 280000\n",
      "  num_env_steps_trained: 280000\n",
      "iterations_since_restore: 70\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 280000\n",
      "num_agent_steps_trained: 280000\n",
      "num_env_steps_sampled: 280000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 280000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.83846153846154\n",
      "  ram_util_percent: 59.04615384615385\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19130045330705223\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14750224487678404\n",
      "  mean_inference_ms: 0.9692950003070075\n",
      "  mean_raw_obs_processing_ms: 0.12082936475765506\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -753.3193777391818\n",
      "  episode_reward_mean: -1184.7181741945135\n",
      "  episode_reward_min: -1753.749753810162\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1397.4805977157291\n",
      "    - -1370.8338847609632\n",
      "    - -887.1588719128924\n",
      "    - -893.5873164476403\n",
      "    - -1467.4801267121773\n",
      "    - -1277.3186680322794\n",
      "    - -1599.5684830579435\n",
      "    - -1538.0757350283563\n",
      "    - -999.912724582168\n",
      "    - -1753.749753810162\n",
      "    - -1361.2861093516326\n",
      "    - -1206.542320732669\n",
      "    - -1632.7330572434144\n",
      "    - -1160.421514133986\n",
      "    - -869.8230561247464\n",
      "    - -966.0284883925926\n",
      "    - -1096.3397860628827\n",
      "    - -1453.7853340061376\n",
      "    - -1408.1891409672385\n",
      "    - -879.8058052000023\n",
      "    - -1128.562987145899\n",
      "    - -889.3703786027046\n",
      "    - -1119.9548930587555\n",
      "    - -1636.0238243187518\n",
      "    - -897.2878625998062\n",
      "    - -913.8517829384733\n",
      "    - -1377.461640114686\n",
      "    - -989.91854724525\n",
      "    - -1691.215491083353\n",
      "    - -1019.5742139708229\n",
      "    - -1556.9590147829424\n",
      "    - -1300.938777418588\n",
      "    - -1564.0895676454068\n",
      "    - -883.2432021063009\n",
      "    - -1342.510379563391\n",
      "    - -1558.6054327805348\n",
      "    - -875.806051227717\n",
      "    - -1361.5939650758903\n",
      "    - -753.3193777391818\n",
      "    - -971.5548420262649\n",
      "    - -1098.963011742978\n",
      "    - -961.4487161863467\n",
      "    - -884.7279484474475\n",
      "    - -965.2178726972343\n",
      "    - -959.594919751276\n",
      "    - -976.1410379984542\n",
      "    - -1412.942375348603\n",
      "    - -1748.984162123915\n",
      "    - -1284.2769234093776\n",
      "    - -887.6688539802756\n",
      "    - -1266.1819058098154\n",
      "    - -1474.8172135179243\n",
      "    - -1563.5557110681912\n",
      "    - -1202.6688668361373\n",
      "    - -869.0619470554208\n",
      "    - -780.6988775786272\n",
      "    - -849.1831038798842\n",
      "    - -883.3816785101136\n",
      "    - -942.1739032347922\n",
      "    - -1398.702441376626\n",
      "    - -1352.3601506956397\n",
      "    - -1481.2845270326288\n",
      "    - -1452.5953402469913\n",
      "    - -1004.0348524862907\n",
      "    - -1635.3903494950312\n",
      "    - -1544.2112156041217\n",
      "    - -861.0102409052035\n",
      "    - -1407.7974212540887\n",
      "    - -890.2270557318512\n",
      "    - -1378.968351210593\n",
      "    - -877.8264361114992\n",
      "    - -880.3590977239969\n",
      "    - -1312.9748075336483\n",
      "    - -900.9968184363522\n",
      "    - -1212.5164385728363\n",
      "    - -1000.1608581971524\n",
      "    - -1427.8126647348797\n",
      "    - -864.285415611517\n",
      "    - -1137.7407804246964\n",
      "    - -1307.1402651885212\n",
      "    - -1055.8559319078242\n",
      "    - -1079.1498851361569\n",
      "    - -1097.2434628955427\n",
      "    - -1603.60971616115\n",
      "    - -913.1074888743949\n",
      "    - -924.1181941550757\n",
      "    - -885.6423517254899\n",
      "    - -1289.6763347841984\n",
      "    - -1156.7004160399224\n",
      "    - -1630.3598779435754\n",
      "    - -1666.3812912054434\n",
      "    - -888.8216566790694\n",
      "    - -833.6597219835722\n",
      "    - -1423.6159696659743\n",
      "    - -1533.9552366782027\n",
      "    - -901.1582320952457\n",
      "    - -1015.2802619834274\n",
      "    - -1350.6833800541267\n",
      "    - -922.6622081775133\n",
      "    - -908.0882378341137\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19130045330705223\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14750224487678404\n",
      "    mean_inference_ms: 0.9692950003070075\n",
      "    mean_raw_obs_processing_ms: 0.12082936475765506\n",
      "time_since_restore: 721.5391819477081\n",
      "time_this_iter_s: 9.352068185806274\n",
      "time_total_s: 721.5391819477081\n",
      "timers:\n",
      "  learn_throughput: 963.07\n",
      "  learn_time_ms: 4153.383\n",
      "  load_throughput: 18804321.901\n",
      "  load_time_ms: 0.213\n",
      "  training_iteration_time_ms: 9597.501\n",
      "  update_time_ms: 2.3\n",
      "timestamp: 1660564642\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 280000\n",
      "training_iteration: 70\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 284000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 284000\n",
      "  num_agent_steps_trained: 284000\n",
      "  num_env_steps_sampled: 284000\n",
      "  num_env_steps_trained: 284000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-57-31\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -753.3193777391818\n",
      "episode_reward_mean: -1161.3792108303137\n",
      "episode_reward_min: -1748.984162123915\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1420\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.4054573178291321\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3482187986373901\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02772776409983635\n",
      "        model: {}\n",
      "        policy_loss: 0.017155878245830536\n",
      "        total_loss: 9.916481971740723\n",
      "        vf_explained_var: -0.017911741510033607\n",
      "        vf_loss: 9.888083457946777\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 284000\n",
      "  num_agent_steps_trained: 284000\n",
      "  num_env_steps_sampled: 284000\n",
      "  num_env_steps_trained: 284000\n",
      "iterations_since_restore: 71\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 284000\n",
      "num_agent_steps_trained: 284000\n",
      "num_env_steps_sampled: 284000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 284000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.33076923076923\n",
      "  ram_util_percent: 59.04615384615385\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19107755847000965\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14735379506823457\n",
      "  mean_inference_ms: 0.9682597267403042\n",
      "  mean_raw_obs_processing_ms: 0.12070080015052827\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -753.3193777391818\n",
      "  episode_reward_mean: -1161.3792108303137\n",
      "  episode_reward_min: -1748.984162123915\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1128.562987145899\n",
      "    - -889.3703786027046\n",
      "    - -1119.9548930587555\n",
      "    - -1636.0238243187518\n",
      "    - -897.2878625998062\n",
      "    - -913.8517829384733\n",
      "    - -1377.461640114686\n",
      "    - -989.91854724525\n",
      "    - -1691.215491083353\n",
      "    - -1019.5742139708229\n",
      "    - -1556.9590147829424\n",
      "    - -1300.938777418588\n",
      "    - -1564.0895676454068\n",
      "    - -883.2432021063009\n",
      "    - -1342.510379563391\n",
      "    - -1558.6054327805348\n",
      "    - -875.806051227717\n",
      "    - -1361.5939650758903\n",
      "    - -753.3193777391818\n",
      "    - -971.5548420262649\n",
      "    - -1098.963011742978\n",
      "    - -961.4487161863467\n",
      "    - -884.7279484474475\n",
      "    - -965.2178726972343\n",
      "    - -959.594919751276\n",
      "    - -976.1410379984542\n",
      "    - -1412.942375348603\n",
      "    - -1748.984162123915\n",
      "    - -1284.2769234093776\n",
      "    - -887.6688539802756\n",
      "    - -1266.1819058098154\n",
      "    - -1474.8172135179243\n",
      "    - -1563.5557110681912\n",
      "    - -1202.6688668361373\n",
      "    - -869.0619470554208\n",
      "    - -780.6988775786272\n",
      "    - -849.1831038798842\n",
      "    - -883.3816785101136\n",
      "    - -942.1739032347922\n",
      "    - -1398.702441376626\n",
      "    - -1352.3601506956397\n",
      "    - -1481.2845270326288\n",
      "    - -1452.5953402469913\n",
      "    - -1004.0348524862907\n",
      "    - -1635.3903494950312\n",
      "    - -1544.2112156041217\n",
      "    - -861.0102409052035\n",
      "    - -1407.7974212540887\n",
      "    - -890.2270557318512\n",
      "    - -1378.968351210593\n",
      "    - -877.8264361114992\n",
      "    - -880.3590977239969\n",
      "    - -1312.9748075336483\n",
      "    - -900.9968184363522\n",
      "    - -1212.5164385728363\n",
      "    - -1000.1608581971524\n",
      "    - -1427.8126647348797\n",
      "    - -864.285415611517\n",
      "    - -1137.7407804246964\n",
      "    - -1307.1402651885212\n",
      "    - -1055.8559319078242\n",
      "    - -1079.1498851361569\n",
      "    - -1097.2434628955427\n",
      "    - -1603.60971616115\n",
      "    - -913.1074888743949\n",
      "    - -924.1181941550757\n",
      "    - -885.6423517254899\n",
      "    - -1289.6763347841984\n",
      "    - -1156.7004160399224\n",
      "    - -1630.3598779435754\n",
      "    - -1666.3812912054434\n",
      "    - -888.8216566790694\n",
      "    - -833.6597219835722\n",
      "    - -1423.6159696659743\n",
      "    - -1533.9552366782027\n",
      "    - -901.1582320952457\n",
      "    - -1015.2802619834274\n",
      "    - -1350.6833800541267\n",
      "    - -922.6622081775133\n",
      "    - -908.0882378341137\n",
      "    - -1017.8664990363548\n",
      "    - -915.070028125772\n",
      "    - -1391.3003185099005\n",
      "    - -1302.2930428270774\n",
      "    - -1508.2305199847622\n",
      "    - -1064.809013332866\n",
      "    - -884.1900721164817\n",
      "    - -892.4577977504075\n",
      "    - -1038.6733391584407\n",
      "    - -1430.311911890511\n",
      "    - -1597.419718424434\n",
      "    - -1585.0732629478084\n",
      "    - -999.5670356636149\n",
      "    - -1044.5091338855068\n",
      "    - -1295.6584475153707\n",
      "    - -873.2068055493561\n",
      "    - -1071.3603344673097\n",
      "    - -753.8899737605212\n",
      "    - -1292.3154153312794\n",
      "    - -928.0217675778745\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19107755847000965\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14735379506823457\n",
      "    mean_inference_ms: 0.9682597267403042\n",
      "    mean_raw_obs_processing_ms: 0.12070080015052827\n",
      "time_since_restore: 730.9572050571442\n",
      "time_this_iter_s: 9.418023109436035\n",
      "time_total_s: 730.9572050571442\n",
      "timers:\n",
      "  learn_throughput: 967.758\n",
      "  learn_time_ms: 4133.263\n",
      "  load_throughput: 18923094.97\n",
      "  load_time_ms: 0.211\n",
      "  training_iteration_time_ms: 9566.695\n",
      "  update_time_ms: 2.337\n",
      "timestamp: 1660564651\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 284000\n",
      "training_iteration: 71\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 288000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 288000\n",
      "  num_agent_steps_trained: 288000\n",
      "  num_env_steps_sampled: 288000\n",
      "  num_env_steps_trained: 288000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-57-42\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -753.8899737605212\n",
      "episode_reward_mean: -1150.636796399091\n",
      "episode_reward_min: -1748.984162123915\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1440\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6081859469413757\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2317432165145874\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009956472553312778\n",
      "        model: {}\n",
      "        policy_loss: 0.010686476714909077\n",
      "        total_loss: 9.855924606323242\n",
      "        vf_explained_var: -0.012227782979607582\n",
      "        vf_loss: 9.839181900024414\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 288000\n",
      "  num_agent_steps_trained: 288000\n",
      "  num_env_steps_sampled: 288000\n",
      "  num_env_steps_trained: 288000\n",
      "iterations_since_restore: 72\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 288000\n",
      "num_agent_steps_trained: 288000\n",
      "num_env_steps_sampled: 288000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 288000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 62.78\n",
      "  ram_util_percent: 59.09333333333335\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19088800291117125\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14722446267024691\n",
      "  mean_inference_ms: 0.9673669734775949\n",
      "  mean_raw_obs_processing_ms: 0.12058652255592746\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -753.8899737605212\n",
      "  episode_reward_mean: -1150.636796399091\n",
      "  episode_reward_min: -1748.984162123915\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1098.963011742978\n",
      "    - -961.4487161863467\n",
      "    - -884.7279484474475\n",
      "    - -965.2178726972343\n",
      "    - -959.594919751276\n",
      "    - -976.1410379984542\n",
      "    - -1412.942375348603\n",
      "    - -1748.984162123915\n",
      "    - -1284.2769234093776\n",
      "    - -887.6688539802756\n",
      "    - -1266.1819058098154\n",
      "    - -1474.8172135179243\n",
      "    - -1563.5557110681912\n",
      "    - -1202.6688668361373\n",
      "    - -869.0619470554208\n",
      "    - -780.6988775786272\n",
      "    - -849.1831038798842\n",
      "    - -883.3816785101136\n",
      "    - -942.1739032347922\n",
      "    - -1398.702441376626\n",
      "    - -1352.3601506956397\n",
      "    - -1481.2845270326288\n",
      "    - -1452.5953402469913\n",
      "    - -1004.0348524862907\n",
      "    - -1635.3903494950312\n",
      "    - -1544.2112156041217\n",
      "    - -861.0102409052035\n",
      "    - -1407.7974212540887\n",
      "    - -890.2270557318512\n",
      "    - -1378.968351210593\n",
      "    - -877.8264361114992\n",
      "    - -880.3590977239969\n",
      "    - -1312.9748075336483\n",
      "    - -900.9968184363522\n",
      "    - -1212.5164385728363\n",
      "    - -1000.1608581971524\n",
      "    - -1427.8126647348797\n",
      "    - -864.285415611517\n",
      "    - -1137.7407804246964\n",
      "    - -1307.1402651885212\n",
      "    - -1055.8559319078242\n",
      "    - -1079.1498851361569\n",
      "    - -1097.2434628955427\n",
      "    - -1603.60971616115\n",
      "    - -913.1074888743949\n",
      "    - -924.1181941550757\n",
      "    - -885.6423517254899\n",
      "    - -1289.6763347841984\n",
      "    - -1156.7004160399224\n",
      "    - -1630.3598779435754\n",
      "    - -1666.3812912054434\n",
      "    - -888.8216566790694\n",
      "    - -833.6597219835722\n",
      "    - -1423.6159696659743\n",
      "    - -1533.9552366782027\n",
      "    - -901.1582320952457\n",
      "    - -1015.2802619834274\n",
      "    - -1350.6833800541267\n",
      "    - -922.6622081775133\n",
      "    - -908.0882378341137\n",
      "    - -1017.8664990363548\n",
      "    - -915.070028125772\n",
      "    - -1391.3003185099005\n",
      "    - -1302.2930428270774\n",
      "    - -1508.2305199847622\n",
      "    - -1064.809013332866\n",
      "    - -884.1900721164817\n",
      "    - -892.4577977504075\n",
      "    - -1038.6733391584407\n",
      "    - -1430.311911890511\n",
      "    - -1597.419718424434\n",
      "    - -1585.0732629478084\n",
      "    - -999.5670356636149\n",
      "    - -1044.5091338855068\n",
      "    - -1295.6584475153707\n",
      "    - -873.2068055493561\n",
      "    - -1071.3603344673097\n",
      "    - -753.8899737605212\n",
      "    - -1292.3154153312794\n",
      "    - -928.0217675778745\n",
      "    - -972.4568625785926\n",
      "    - -972.3627547789558\n",
      "    - -1354.761966040805\n",
      "    - -834.4106233341073\n",
      "    - -959.1753559879168\n",
      "    - -960.0636234851247\n",
      "    - -1098.401766309365\n",
      "    - -1438.698231225175\n",
      "    - -902.90401148568\n",
      "    - -982.6878193821359\n",
      "    - -1695.7353037134114\n",
      "    - -973.4704437667044\n",
      "    - -1181.2381536499854\n",
      "    - -993.2160482033571\n",
      "    - -1676.8190533531993\n",
      "    - -1276.1159099012455\n",
      "    - -903.5435953720951\n",
      "    - -1275.5286736121686\n",
      "    - -867.468892543553\n",
      "    - -1438.5416995988871\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19088800291117125\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14722446267024691\n",
      "    mean_inference_ms: 0.9673669734775949\n",
      "    mean_raw_obs_processing_ms: 0.12058652255592746\n",
      "time_since_restore: 741.3405768871307\n",
      "time_this_iter_s: 10.383371829986572\n",
      "time_total_s: 741.3405768871307\n",
      "timers:\n",
      "  learn_throughput: 952.615\n",
      "  learn_time_ms: 4198.968\n",
      "  load_throughput: 19117155.88\n",
      "  load_time_ms: 0.209\n",
      "  training_iteration_time_ms: 9600.697\n",
      "  update_time_ms: 2.32\n",
      "timestamp: 1660564662\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 288000\n",
      "training_iteration: 72\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 292000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 292000\n",
      "  num_agent_steps_trained: 292000\n",
      "  num_env_steps_sampled: 292000\n",
      "  num_env_steps_trained: 292000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-57-54\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -753.8899737605212\n",
      "episode_reward_mean: -1147.001642065089\n",
      "episode_reward_min: -1704.000976923726\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1460\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6081859469413757\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0234854221343994\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012456346303224564\n",
      "        model: {}\n",
      "        policy_loss: 0.008047455921769142\n",
      "        total_loss: 9.8603515625\n",
      "        vf_explained_var: -0.021859124302864075\n",
      "        vf_loss: 9.844728469848633\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 292000\n",
      "  num_agent_steps_trained: 292000\n",
      "  num_env_steps_sampled: 292000\n",
      "  num_env_steps_trained: 292000\n",
      "iterations_since_restore: 73\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 292000\n",
      "num_agent_steps_trained: 292000\n",
      "num_env_steps_sampled: 292000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 292000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 51.45882352941177\n",
      "  ram_util_percent: 59.11764705882353\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19079883729656003\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14716947684141357\n",
      "  mean_inference_ms: 0.9670262205810352\n",
      "  mean_raw_obs_processing_ms: 0.12053213411042173\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -753.8899737605212\n",
      "  episode_reward_mean: -1147.001642065089\n",
      "  episode_reward_min: -1704.000976923726\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1352.3601506956397\n",
      "    - -1481.2845270326288\n",
      "    - -1452.5953402469913\n",
      "    - -1004.0348524862907\n",
      "    - -1635.3903494950312\n",
      "    - -1544.2112156041217\n",
      "    - -861.0102409052035\n",
      "    - -1407.7974212540887\n",
      "    - -890.2270557318512\n",
      "    - -1378.968351210593\n",
      "    - -877.8264361114992\n",
      "    - -880.3590977239969\n",
      "    - -1312.9748075336483\n",
      "    - -900.9968184363522\n",
      "    - -1212.5164385728363\n",
      "    - -1000.1608581971524\n",
      "    - -1427.8126647348797\n",
      "    - -864.285415611517\n",
      "    - -1137.7407804246964\n",
      "    - -1307.1402651885212\n",
      "    - -1055.8559319078242\n",
      "    - -1079.1498851361569\n",
      "    - -1097.2434628955427\n",
      "    - -1603.60971616115\n",
      "    - -913.1074888743949\n",
      "    - -924.1181941550757\n",
      "    - -885.6423517254899\n",
      "    - -1289.6763347841984\n",
      "    - -1156.7004160399224\n",
      "    - -1630.3598779435754\n",
      "    - -1666.3812912054434\n",
      "    - -888.8216566790694\n",
      "    - -833.6597219835722\n",
      "    - -1423.6159696659743\n",
      "    - -1533.9552366782027\n",
      "    - -901.1582320952457\n",
      "    - -1015.2802619834274\n",
      "    - -1350.6833800541267\n",
      "    - -922.6622081775133\n",
      "    - -908.0882378341137\n",
      "    - -1017.8664990363548\n",
      "    - -915.070028125772\n",
      "    - -1391.3003185099005\n",
      "    - -1302.2930428270774\n",
      "    - -1508.2305199847622\n",
      "    - -1064.809013332866\n",
      "    - -884.1900721164817\n",
      "    - -892.4577977504075\n",
      "    - -1038.6733391584407\n",
      "    - -1430.311911890511\n",
      "    - -1597.419718424434\n",
      "    - -1585.0732629478084\n",
      "    - -999.5670356636149\n",
      "    - -1044.5091338855068\n",
      "    - -1295.6584475153707\n",
      "    - -873.2068055493561\n",
      "    - -1071.3603344673097\n",
      "    - -753.8899737605212\n",
      "    - -1292.3154153312794\n",
      "    - -928.0217675778745\n",
      "    - -972.4568625785926\n",
      "    - -972.3627547789558\n",
      "    - -1354.761966040805\n",
      "    - -834.4106233341073\n",
      "    - -959.1753559879168\n",
      "    - -960.0636234851247\n",
      "    - -1098.401766309365\n",
      "    - -1438.698231225175\n",
      "    - -902.90401148568\n",
      "    - -982.6878193821359\n",
      "    - -1695.7353037134114\n",
      "    - -973.4704437667044\n",
      "    - -1181.2381536499854\n",
      "    - -993.2160482033571\n",
      "    - -1676.8190533531993\n",
      "    - -1276.1159099012455\n",
      "    - -903.5435953720951\n",
      "    - -1275.5286736121686\n",
      "    - -867.468892543553\n",
      "    - -1438.5416995988871\n",
      "    - -1103.9094188313757\n",
      "    - -1178.227295451372\n",
      "    - -875.6801239473599\n",
      "    - -896.9774937694289\n",
      "    - -875.1120978453507\n",
      "    - -1314.9631714843824\n",
      "    - -871.4108667418238\n",
      "    - -991.1584631772722\n",
      "    - -1553.0962471233402\n",
      "    - -902.4600012488111\n",
      "    - -875.0562910102523\n",
      "    - -1490.0195047938632\n",
      "    - -887.6870355780055\n",
      "    - -1306.5822743492997\n",
      "    - -879.2487866507889\n",
      "    - -1438.3528787865816\n",
      "    - -1704.000976923726\n",
      "    - -880.7036031010872\n",
      "    - -928.2308997097236\n",
      "    - -1093.9986066293704\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19079883729656003\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14716947684141357\n",
      "    mean_inference_ms: 0.9670262205810352\n",
      "    mean_raw_obs_processing_ms: 0.12053213411042173\n",
      "time_since_restore: 753.1080405712128\n",
      "time_this_iter_s: 11.767463684082031\n",
      "time_total_s: 753.1080405712128\n",
      "timers:\n",
      "  learn_throughput: 940.377\n",
      "  learn_time_ms: 4253.611\n",
      "  load_throughput: 18929500.169\n",
      "  load_time_ms: 0.211\n",
      "  training_iteration_time_ms: 9713.211\n",
      "  update_time_ms: 2.282\n",
      "timestamp: 1660564674\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 292000\n",
      "training_iteration: 73\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 296000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 296000\n",
      "  num_agent_steps_trained: 296000\n",
      "  num_env_steps_sampled: 296000\n",
      "  num_env_steps_trained: 296000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-58-05\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -753.8899737605212\n",
      "episode_reward_mean: -1121.2394816950732\n",
      "episode_reward_min: -1741.4231050828432\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1480\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6081859469413757\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6308483481407166\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010378042235970497\n",
      "        model: {}\n",
      "        policy_loss: 0.01232943870127201\n",
      "        total_loss: 9.886489868164062\n",
      "        vf_explained_var: -0.03244595229625702\n",
      "        vf_loss: 9.86784839630127\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 296000\n",
      "  num_agent_steps_trained: 296000\n",
      "  num_env_steps_sampled: 296000\n",
      "  num_env_steps_trained: 296000\n",
      "iterations_since_restore: 74\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 296000\n",
      "num_agent_steps_trained: 296000\n",
      "num_env_steps_sampled: 296000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 296000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 47.9125\n",
      "  ram_util_percent: 59.28125\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19080719241271496\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14717445692722955\n",
      "  mean_inference_ms: 0.9671711793568102\n",
      "  mean_raw_obs_processing_ms: 0.12052724826738759\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -753.8899737605212\n",
      "  episode_reward_mean: -1121.2394816950732\n",
      "  episode_reward_min: -1741.4231050828432\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1055.8559319078242\n",
      "    - -1079.1498851361569\n",
      "    - -1097.2434628955427\n",
      "    - -1603.60971616115\n",
      "    - -913.1074888743949\n",
      "    - -924.1181941550757\n",
      "    - -885.6423517254899\n",
      "    - -1289.6763347841984\n",
      "    - -1156.7004160399224\n",
      "    - -1630.3598779435754\n",
      "    - -1666.3812912054434\n",
      "    - -888.8216566790694\n",
      "    - -833.6597219835722\n",
      "    - -1423.6159696659743\n",
      "    - -1533.9552366782027\n",
      "    - -901.1582320952457\n",
      "    - -1015.2802619834274\n",
      "    - -1350.6833800541267\n",
      "    - -922.6622081775133\n",
      "    - -908.0882378341137\n",
      "    - -1017.8664990363548\n",
      "    - -915.070028125772\n",
      "    - -1391.3003185099005\n",
      "    - -1302.2930428270774\n",
      "    - -1508.2305199847622\n",
      "    - -1064.809013332866\n",
      "    - -884.1900721164817\n",
      "    - -892.4577977504075\n",
      "    - -1038.6733391584407\n",
      "    - -1430.311911890511\n",
      "    - -1597.419718424434\n",
      "    - -1585.0732629478084\n",
      "    - -999.5670356636149\n",
      "    - -1044.5091338855068\n",
      "    - -1295.6584475153707\n",
      "    - -873.2068055493561\n",
      "    - -1071.3603344673097\n",
      "    - -753.8899737605212\n",
      "    - -1292.3154153312794\n",
      "    - -928.0217675778745\n",
      "    - -972.4568625785926\n",
      "    - -972.3627547789558\n",
      "    - -1354.761966040805\n",
      "    - -834.4106233341073\n",
      "    - -959.1753559879168\n",
      "    - -960.0636234851247\n",
      "    - -1098.401766309365\n",
      "    - -1438.698231225175\n",
      "    - -902.90401148568\n",
      "    - -982.6878193821359\n",
      "    - -1695.7353037134114\n",
      "    - -973.4704437667044\n",
      "    - -1181.2381536499854\n",
      "    - -993.2160482033571\n",
      "    - -1676.8190533531993\n",
      "    - -1276.1159099012455\n",
      "    - -903.5435953720951\n",
      "    - -1275.5286736121686\n",
      "    - -867.468892543553\n",
      "    - -1438.5416995988871\n",
      "    - -1103.9094188313757\n",
      "    - -1178.227295451372\n",
      "    - -875.6801239473599\n",
      "    - -896.9774937694289\n",
      "    - -875.1120978453507\n",
      "    - -1314.9631714843824\n",
      "    - -871.4108667418238\n",
      "    - -991.1584631772722\n",
      "    - -1553.0962471233402\n",
      "    - -902.4600012488111\n",
      "    - -875.0562910102523\n",
      "    - -1490.0195047938632\n",
      "    - -887.6870355780055\n",
      "    - -1306.5822743492997\n",
      "    - -879.2487866507889\n",
      "    - -1438.3528787865816\n",
      "    - -1704.000976923726\n",
      "    - -880.7036031010872\n",
      "    - -928.2308997097236\n",
      "    - -1093.9986066293704\n",
      "    - -871.2575612111292\n",
      "    - -1741.4231050828432\n",
      "    - -887.6689568343551\n",
      "    - -984.60274245247\n",
      "    - -990.1308348794715\n",
      "    - -912.1912534473171\n",
      "    - -1049.426095400622\n",
      "    - -757.5632903761853\n",
      "    - -963.0318057350192\n",
      "    - -983.0388926743666\n",
      "    - -1375.7348606714775\n",
      "    - -982.6399509896692\n",
      "    - -1555.276712914246\n",
      "    - -1263.134023265849\n",
      "    - -884.718853524465\n",
      "    - -976.4050058965869\n",
      "    - -964.5012084340206\n",
      "    - -939.0715739112661\n",
      "    - -1372.786921963261\n",
      "    - -898.8734005313423\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19080719241271496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14717445692722955\n",
      "    mean_inference_ms: 0.9671711793568102\n",
      "    mean_raw_obs_processing_ms: 0.12052724826738759\n",
      "time_since_restore: 764.4469816684723\n",
      "time_this_iter_s: 11.338941097259521\n",
      "time_total_s: 764.4469816684723\n",
      "timers:\n",
      "  learn_throughput: 921.962\n",
      "  learn_time_ms: 4338.573\n",
      "  load_throughput: 18313738.675\n",
      "  load_time_ms: 0.218\n",
      "  training_iteration_time_ms: 9908.009\n",
      "  update_time_ms: 2.314\n",
      "timestamp: 1660564685\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 296000\n",
      "training_iteration: 74\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 300000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 300000\n",
      "  num_agent_steps_trained: 300000\n",
      "  num_env_steps_sampled: 300000\n",
      "  num_env_steps_trained: 300000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-58-16\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -753.8899737605212\n",
      "episode_reward_mean: -1113.0219788253967\n",
      "episode_reward_min: -1741.4231050828432\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1500\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6081859469413757\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0965683460235596\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011983378790318966\n",
      "        model: {}\n",
      "        policy_loss: 0.011853320524096489\n",
      "        total_loss: 9.832196235656738\n",
      "        vf_explained_var: -0.017378462478518486\n",
      "        vf_loss: 9.813054084777832\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 300000\n",
      "  num_agent_steps_trained: 300000\n",
      "  num_env_steps_sampled: 300000\n",
      "  num_env_steps_trained: 300000\n",
      "iterations_since_restore: 75\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 300000\n",
      "num_agent_steps_trained: 300000\n",
      "num_env_steps_sampled: 300000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 300000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 48.573333333333345\n",
      "  ram_util_percent: 59.240000000000016\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.190902331434118\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1472386468437329\n",
      "  mean_inference_ms: 0.9677403792781354\n",
      "  mean_raw_obs_processing_ms: 0.1205710873225457\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -753.8899737605212\n",
      "  episode_reward_mean: -1113.0219788253967\n",
      "  episode_reward_min: -1741.4231050828432\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1017.8664990363548\n",
      "    - -915.070028125772\n",
      "    - -1391.3003185099005\n",
      "    - -1302.2930428270774\n",
      "    - -1508.2305199847622\n",
      "    - -1064.809013332866\n",
      "    - -884.1900721164817\n",
      "    - -892.4577977504075\n",
      "    - -1038.6733391584407\n",
      "    - -1430.311911890511\n",
      "    - -1597.419718424434\n",
      "    - -1585.0732629478084\n",
      "    - -999.5670356636149\n",
      "    - -1044.5091338855068\n",
      "    - -1295.6584475153707\n",
      "    - -873.2068055493561\n",
      "    - -1071.3603344673097\n",
      "    - -753.8899737605212\n",
      "    - -1292.3154153312794\n",
      "    - -928.0217675778745\n",
      "    - -972.4568625785926\n",
      "    - -972.3627547789558\n",
      "    - -1354.761966040805\n",
      "    - -834.4106233341073\n",
      "    - -959.1753559879168\n",
      "    - -960.0636234851247\n",
      "    - -1098.401766309365\n",
      "    - -1438.698231225175\n",
      "    - -902.90401148568\n",
      "    - -982.6878193821359\n",
      "    - -1695.7353037134114\n",
      "    - -973.4704437667044\n",
      "    - -1181.2381536499854\n",
      "    - -993.2160482033571\n",
      "    - -1676.8190533531993\n",
      "    - -1276.1159099012455\n",
      "    - -903.5435953720951\n",
      "    - -1275.5286736121686\n",
      "    - -867.468892543553\n",
      "    - -1438.5416995988871\n",
      "    - -1103.9094188313757\n",
      "    - -1178.227295451372\n",
      "    - -875.6801239473599\n",
      "    - -896.9774937694289\n",
      "    - -875.1120978453507\n",
      "    - -1314.9631714843824\n",
      "    - -871.4108667418238\n",
      "    - -991.1584631772722\n",
      "    - -1553.0962471233402\n",
      "    - -902.4600012488111\n",
      "    - -875.0562910102523\n",
      "    - -1490.0195047938632\n",
      "    - -887.6870355780055\n",
      "    - -1306.5822743492997\n",
      "    - -879.2487866507889\n",
      "    - -1438.3528787865816\n",
      "    - -1704.000976923726\n",
      "    - -880.7036031010872\n",
      "    - -928.2308997097236\n",
      "    - -1093.9986066293704\n",
      "    - -871.2575612111292\n",
      "    - -1741.4231050828432\n",
      "    - -887.6689568343551\n",
      "    - -984.60274245247\n",
      "    - -990.1308348794715\n",
      "    - -912.1912534473171\n",
      "    - -1049.426095400622\n",
      "    - -757.5632903761853\n",
      "    - -963.0318057350192\n",
      "    - -983.0388926743666\n",
      "    - -1375.7348606714775\n",
      "    - -982.6399509896692\n",
      "    - -1555.276712914246\n",
      "    - -1263.134023265849\n",
      "    - -884.718853524465\n",
      "    - -976.4050058965869\n",
      "    - -964.5012084340206\n",
      "    - -939.0715739112661\n",
      "    - -1372.786921963261\n",
      "    - -898.8734005313423\n",
      "    - -849.9684395557383\n",
      "    - -1612.3633146330887\n",
      "    - -993.236918815796\n",
      "    - -974.543603589715\n",
      "    - -872.1176016223859\n",
      "    - -1612.3123675259292\n",
      "    - -879.2120045669197\n",
      "    - -877.981288238297\n",
      "    - -1175.4196495436365\n",
      "    - -1051.8690345896007\n",
      "    - -859.7933309446175\n",
      "    - -877.8495887337622\n",
      "    - -1286.7428472962667\n",
      "    - -1168.7367868065319\n",
      "    - -1269.9904548816562\n",
      "    - -972.0667586558069\n",
      "    - -1173.251796590443\n",
      "    - -1051.6844569289726\n",
      "    - -1627.677202678112\n",
      "    - -1071.2021228151023\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.190902331434118\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1472386468437329\n",
      "    mean_inference_ms: 0.9677403792781354\n",
      "    mean_raw_obs_processing_ms: 0.1205710873225457\n",
      "time_since_restore: 775.0382363796234\n",
      "time_this_iter_s: 10.591254711151123\n",
      "time_total_s: 775.0382363796234\n",
      "timers:\n",
      "  learn_throughput: 915.378\n",
      "  learn_time_ms: 4369.777\n",
      "  load_throughput: 17641657.203\n",
      "  load_time_ms: 0.227\n",
      "  training_iteration_time_ms: 10027.677\n",
      "  update_time_ms: 2.303\n",
      "timestamp: 1660564696\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 300000\n",
      "training_iteration: 75\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 304000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 304000\n",
      "  num_agent_steps_trained: 304000\n",
      "  num_env_steps_sampled: 304000\n",
      "  num_env_steps_trained: 304000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-58-26\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -757.5632903761853\n",
      "episode_reward_mean: -1096.4605857478941\n",
      "episode_reward_min: -1741.4231050828432\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1520\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6081859469413757\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7393637299537659\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013249780982732773\n",
      "        model: {}\n",
      "        policy_loss: 0.010245182551443577\n",
      "        total_loss: 9.896378517150879\n",
      "        vf_explained_var: -0.009227233938872814\n",
      "        vf_loss: 9.87807559967041\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 304000\n",
      "  num_agent_steps_trained: 304000\n",
      "  num_env_steps_sampled: 304000\n",
      "  num_env_steps_trained: 304000\n",
      "iterations_since_restore: 76\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 304000\n",
      "num_agent_steps_trained: 304000\n",
      "num_env_steps_sampled: 304000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 304000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 45.88\n",
      "  ram_util_percent: 59.13333333333335\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19101617220318431\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14731152995595292\n",
      "  mean_inference_ms: 0.9683859029187107\n",
      "  mean_raw_obs_processing_ms: 0.12062513487409571\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -757.5632903761853\n",
      "  episode_reward_mean: -1096.4605857478941\n",
      "  episode_reward_min: -1741.4231050828432\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -972.4568625785926\n",
      "    - -972.3627547789558\n",
      "    - -1354.761966040805\n",
      "    - -834.4106233341073\n",
      "    - -959.1753559879168\n",
      "    - -960.0636234851247\n",
      "    - -1098.401766309365\n",
      "    - -1438.698231225175\n",
      "    - -902.90401148568\n",
      "    - -982.6878193821359\n",
      "    - -1695.7353037134114\n",
      "    - -973.4704437667044\n",
      "    - -1181.2381536499854\n",
      "    - -993.2160482033571\n",
      "    - -1676.8190533531993\n",
      "    - -1276.1159099012455\n",
      "    - -903.5435953720951\n",
      "    - -1275.5286736121686\n",
      "    - -867.468892543553\n",
      "    - -1438.5416995988871\n",
      "    - -1103.9094188313757\n",
      "    - -1178.227295451372\n",
      "    - -875.6801239473599\n",
      "    - -896.9774937694289\n",
      "    - -875.1120978453507\n",
      "    - -1314.9631714843824\n",
      "    - -871.4108667418238\n",
      "    - -991.1584631772722\n",
      "    - -1553.0962471233402\n",
      "    - -902.4600012488111\n",
      "    - -875.0562910102523\n",
      "    - -1490.0195047938632\n",
      "    - -887.6870355780055\n",
      "    - -1306.5822743492997\n",
      "    - -879.2487866507889\n",
      "    - -1438.3528787865816\n",
      "    - -1704.000976923726\n",
      "    - -880.7036031010872\n",
      "    - -928.2308997097236\n",
      "    - -1093.9986066293704\n",
      "    - -871.2575612111292\n",
      "    - -1741.4231050828432\n",
      "    - -887.6689568343551\n",
      "    - -984.60274245247\n",
      "    - -990.1308348794715\n",
      "    - -912.1912534473171\n",
      "    - -1049.426095400622\n",
      "    - -757.5632903761853\n",
      "    - -963.0318057350192\n",
      "    - -983.0388926743666\n",
      "    - -1375.7348606714775\n",
      "    - -982.6399509896692\n",
      "    - -1555.276712914246\n",
      "    - -1263.134023265849\n",
      "    - -884.718853524465\n",
      "    - -976.4050058965869\n",
      "    - -964.5012084340206\n",
      "    - -939.0715739112661\n",
      "    - -1372.786921963261\n",
      "    - -898.8734005313423\n",
      "    - -849.9684395557383\n",
      "    - -1612.3633146330887\n",
      "    - -993.236918815796\n",
      "    - -974.543603589715\n",
      "    - -872.1176016223859\n",
      "    - -1612.3123675259292\n",
      "    - -879.2120045669197\n",
      "    - -877.981288238297\n",
      "    - -1175.4196495436365\n",
      "    - -1051.8690345896007\n",
      "    - -859.7933309446175\n",
      "    - -877.8495887337622\n",
      "    - -1286.7428472962667\n",
      "    - -1168.7367868065319\n",
      "    - -1269.9904548816562\n",
      "    - -972.0667586558069\n",
      "    - -1173.251796590443\n",
      "    - -1051.6844569289726\n",
      "    - -1627.677202678112\n",
      "    - -1071.2021228151023\n",
      "    - -903.3545807773678\n",
      "    - -968.9655431690267\n",
      "    - -986.4044687687604\n",
      "    - -870.1392350870162\n",
      "    - -1402.0911551464774\n",
      "    - -1573.5995002243967\n",
      "    - -1064.2466263411136\n",
      "    - -957.9359894923897\n",
      "    - -1502.6962084840652\n",
      "    - -841.3415710792159\n",
      "    - -1525.026269044401\n",
      "    - -888.0713947627281\n",
      "    - -970.3527884120756\n",
      "    - -880.1138490550387\n",
      "    - -886.426447064621\n",
      "    - -948.2913941590951\n",
      "    - -1326.676108558978\n",
      "    - -875.4382630036622\n",
      "    - -891.3796587454775\n",
      "    - -967.5340787294866\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19101617220318431\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14731152995595292\n",
      "    mean_inference_ms: 0.9683859029187107\n",
      "    mean_raw_obs_processing_ms: 0.12062513487409571\n",
      "time_since_restore: 785.4756128787994\n",
      "time_this_iter_s: 10.437376499176025\n",
      "time_total_s: 785.4756128787994\n",
      "timers:\n",
      "  learn_throughput: 898.926\n",
      "  learn_time_ms: 4449.754\n",
      "  load_throughput: 17578809.723\n",
      "  load_time_ms: 0.228\n",
      "  training_iteration_time_ms: 10146.158\n",
      "  update_time_ms: 2.299\n",
      "timestamp: 1660564706\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 304000\n",
      "training_iteration: 76\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 308000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 308000\n",
      "  num_agent_steps_trained: 308000\n",
      "  num_env_steps_sampled: 308000\n",
      "  num_env_steps_trained: 308000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-58-38\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -757.5632903761853\n",
      "episode_reward_mean: -1097.3351039365534\n",
      "episode_reward_min: -1741.4231050828432\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1540\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6081859469413757\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2131959199905396\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01120182778686285\n",
      "        model: {}\n",
      "        policy_loss: 0.008853957056999207\n",
      "        total_loss: 9.785505294799805\n",
      "        vf_explained_var: -0.016279591247439384\n",
      "        vf_loss: 9.7698392868042\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 308000\n",
      "  num_agent_steps_trained: 308000\n",
      "  num_env_steps_sampled: 308000\n",
      "  num_env_steps_trained: 308000\n",
      "iterations_since_restore: 77\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 308000\n",
      "num_agent_steps_trained: 308000\n",
      "num_env_steps_sampled: 308000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 308000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.19411764705882\n",
      "  ram_util_percent: 59.205882352941174\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1911644781640515\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14740547627805742\n",
      "  mean_inference_ms: 0.9692029545406384\n",
      "  mean_raw_obs_processing_ms: 0.12069830878814174\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -757.5632903761853\n",
      "  episode_reward_mean: -1097.3351039365534\n",
      "  episode_reward_min: -1741.4231050828432\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1103.9094188313757\n",
      "    - -1178.227295451372\n",
      "    - -875.6801239473599\n",
      "    - -896.9774937694289\n",
      "    - -875.1120978453507\n",
      "    - -1314.9631714843824\n",
      "    - -871.4108667418238\n",
      "    - -991.1584631772722\n",
      "    - -1553.0962471233402\n",
      "    - -902.4600012488111\n",
      "    - -875.0562910102523\n",
      "    - -1490.0195047938632\n",
      "    - -887.6870355780055\n",
      "    - -1306.5822743492997\n",
      "    - -879.2487866507889\n",
      "    - -1438.3528787865816\n",
      "    - -1704.000976923726\n",
      "    - -880.7036031010872\n",
      "    - -928.2308997097236\n",
      "    - -1093.9986066293704\n",
      "    - -871.2575612111292\n",
      "    - -1741.4231050828432\n",
      "    - -887.6689568343551\n",
      "    - -984.60274245247\n",
      "    - -990.1308348794715\n",
      "    - -912.1912534473171\n",
      "    - -1049.426095400622\n",
      "    - -757.5632903761853\n",
      "    - -963.0318057350192\n",
      "    - -983.0388926743666\n",
      "    - -1375.7348606714775\n",
      "    - -982.6399509896692\n",
      "    - -1555.276712914246\n",
      "    - -1263.134023265849\n",
      "    - -884.718853524465\n",
      "    - -976.4050058965869\n",
      "    - -964.5012084340206\n",
      "    - -939.0715739112661\n",
      "    - -1372.786921963261\n",
      "    - -898.8734005313423\n",
      "    - -849.9684395557383\n",
      "    - -1612.3633146330887\n",
      "    - -993.236918815796\n",
      "    - -974.543603589715\n",
      "    - -872.1176016223859\n",
      "    - -1612.3123675259292\n",
      "    - -879.2120045669197\n",
      "    - -877.981288238297\n",
      "    - -1175.4196495436365\n",
      "    - -1051.8690345896007\n",
      "    - -859.7933309446175\n",
      "    - -877.8495887337622\n",
      "    - -1286.7428472962667\n",
      "    - -1168.7367868065319\n",
      "    - -1269.9904548816562\n",
      "    - -972.0667586558069\n",
      "    - -1173.251796590443\n",
      "    - -1051.6844569289726\n",
      "    - -1627.677202678112\n",
      "    - -1071.2021228151023\n",
      "    - -903.3545807773678\n",
      "    - -968.9655431690267\n",
      "    - -986.4044687687604\n",
      "    - -870.1392350870162\n",
      "    - -1402.0911551464774\n",
      "    - -1573.5995002243967\n",
      "    - -1064.2466263411136\n",
      "    - -957.9359894923897\n",
      "    - -1502.6962084840652\n",
      "    - -841.3415710792159\n",
      "    - -1525.026269044401\n",
      "    - -888.0713947627281\n",
      "    - -970.3527884120756\n",
      "    - -880.1138490550387\n",
      "    - -886.426447064621\n",
      "    - -948.2913941590951\n",
      "    - -1326.676108558978\n",
      "    - -875.4382630036622\n",
      "    - -891.3796587454775\n",
      "    - -967.5340787294866\n",
      "    - -933.5631782104873\n",
      "    - -1007.787609233676\n",
      "    - -1088.0155662050568\n",
      "    - -1328.5767845285495\n",
      "    - -891.0017403668425\n",
      "    - -990.3850182941933\n",
      "    - -984.0661249560898\n",
      "    - -1692.1242970463609\n",
      "    - -967.7399538900056\n",
      "    - -1213.2733710417112\n",
      "    - -1076.6860260534916\n",
      "    - -1004.5471876855728\n",
      "    - -1066.6127767703151\n",
      "    - -1541.3795144380613\n",
      "    - -1413.918091289017\n",
      "    - -1307.2962783400908\n",
      "    - -887.836582855169\n",
      "    - -970.2761705607485\n",
      "    - -1288.4597108446965\n",
      "    - -1191.5066245782334\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1911644781640515\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14740547627805742\n",
      "    mean_inference_ms: 0.9692029545406384\n",
      "    mean_raw_obs_processing_ms: 0.12069830878814174\n",
      "time_since_restore: 797.1465346813202\n",
      "time_this_iter_s: 11.670921802520752\n",
      "time_total_s: 797.1465346813202\n",
      "timers:\n",
      "  learn_throughput: 867.085\n",
      "  learn_time_ms: 4613.157\n",
      "  load_throughput: 17093444.727\n",
      "  load_time_ms: 0.234\n",
      "  training_iteration_time_ms: 10372.024\n",
      "  update_time_ms: 2.376\n",
      "timestamp: 1660564718\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 308000\n",
      "training_iteration: 77\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 312000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 312000\n",
      "  num_agent_steps_trained: 312000\n",
      "  num_env_steps_sampled: 312000\n",
      "  num_env_steps_trained: 312000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-58-49\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -757.5632903761853\n",
      "episode_reward_mean: -1094.8828306498494\n",
      "episode_reward_min: -1741.4231050828432\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1560\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6081859469413757\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9634665846824646\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013928858563303947\n",
      "        model: {}\n",
      "        policy_loss: 0.013941437005996704\n",
      "        total_loss: 9.88265323638916\n",
      "        vf_explained_var: -0.0199382733553648\n",
      "        vf_loss: 9.86023998260498\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 312000\n",
      "  num_agent_steps_trained: 312000\n",
      "  num_env_steps_sampled: 312000\n",
      "  num_env_steps_trained: 312000\n",
      "iterations_since_restore: 78\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 312000\n",
      "num_agent_steps_trained: 312000\n",
      "num_env_steps_sampled: 312000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 312000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.0625\n",
      "  ram_util_percent: 58.987500000000004\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19126460353896832\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14747309733880073\n",
      "  mean_inference_ms: 0.9697542541630506\n",
      "  mean_raw_obs_processing_ms: 0.1207418605982155\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -757.5632903761853\n",
      "  episode_reward_mean: -1094.8828306498494\n",
      "  episode_reward_min: -1741.4231050828432\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -871.2575612111292\n",
      "    - -1741.4231050828432\n",
      "    - -887.6689568343551\n",
      "    - -984.60274245247\n",
      "    - -990.1308348794715\n",
      "    - -912.1912534473171\n",
      "    - -1049.426095400622\n",
      "    - -757.5632903761853\n",
      "    - -963.0318057350192\n",
      "    - -983.0388926743666\n",
      "    - -1375.7348606714775\n",
      "    - -982.6399509896692\n",
      "    - -1555.276712914246\n",
      "    - -1263.134023265849\n",
      "    - -884.718853524465\n",
      "    - -976.4050058965869\n",
      "    - -964.5012084340206\n",
      "    - -939.0715739112661\n",
      "    - -1372.786921963261\n",
      "    - -898.8734005313423\n",
      "    - -849.9684395557383\n",
      "    - -1612.3633146330887\n",
      "    - -993.236918815796\n",
      "    - -974.543603589715\n",
      "    - -872.1176016223859\n",
      "    - -1612.3123675259292\n",
      "    - -879.2120045669197\n",
      "    - -877.981288238297\n",
      "    - -1175.4196495436365\n",
      "    - -1051.8690345896007\n",
      "    - -859.7933309446175\n",
      "    - -877.8495887337622\n",
      "    - -1286.7428472962667\n",
      "    - -1168.7367868065319\n",
      "    - -1269.9904548816562\n",
      "    - -972.0667586558069\n",
      "    - -1173.251796590443\n",
      "    - -1051.6844569289726\n",
      "    - -1627.677202678112\n",
      "    - -1071.2021228151023\n",
      "    - -903.3545807773678\n",
      "    - -968.9655431690267\n",
      "    - -986.4044687687604\n",
      "    - -870.1392350870162\n",
      "    - -1402.0911551464774\n",
      "    - -1573.5995002243967\n",
      "    - -1064.2466263411136\n",
      "    - -957.9359894923897\n",
      "    - -1502.6962084840652\n",
      "    - -841.3415710792159\n",
      "    - -1525.026269044401\n",
      "    - -888.0713947627281\n",
      "    - -970.3527884120756\n",
      "    - -880.1138490550387\n",
      "    - -886.426447064621\n",
      "    - -948.2913941590951\n",
      "    - -1326.676108558978\n",
      "    - -875.4382630036622\n",
      "    - -891.3796587454775\n",
      "    - -967.5340787294866\n",
      "    - -933.5631782104873\n",
      "    - -1007.787609233676\n",
      "    - -1088.0155662050568\n",
      "    - -1328.5767845285495\n",
      "    - -891.0017403668425\n",
      "    - -990.3850182941933\n",
      "    - -984.0661249560898\n",
      "    - -1692.1242970463609\n",
      "    - -967.7399538900056\n",
      "    - -1213.2733710417112\n",
      "    - -1076.6860260534916\n",
      "    - -1004.5471876855728\n",
      "    - -1066.6127767703151\n",
      "    - -1541.3795144380613\n",
      "    - -1413.918091289017\n",
      "    - -1307.2962783400908\n",
      "    - -887.836582855169\n",
      "    - -970.2761705607485\n",
      "    - -1288.4597108446965\n",
      "    - -1191.5066245782334\n",
      "    - -988.9388929034474\n",
      "    - -889.591181374075\n",
      "    - -1254.309131671519\n",
      "    - -891.4502020134249\n",
      "    - -944.2703336986546\n",
      "    - -1568.7941256390113\n",
      "    - -1357.2234183568264\n",
      "    - -1585.5944530283657\n",
      "    - -856.9132126276451\n",
      "    - -876.222605700912\n",
      "    - -978.214505385279\n",
      "    - -1681.8670508173316\n",
      "    - -974.6138529849075\n",
      "    - -889.4585742395514\n",
      "    - -1136.0841014936827\n",
      "    - -1160.839620030548\n",
      "    - -879.9965533736386\n",
      "    - -892.3005085799656\n",
      "    - -914.4775026289707\n",
      "    - -1080.4888819350956\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19126460353896832\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14747309733880073\n",
      "    mean_inference_ms: 0.9697542541630506\n",
      "    mean_raw_obs_processing_ms: 0.1207418605982155\n",
      "time_since_restore: 808.0942845344543\n",
      "time_this_iter_s: 10.947749853134155\n",
      "time_total_s: 808.0942845344543\n",
      "timers:\n",
      "  learn_throughput: 848.675\n",
      "  learn_time_ms: 4713.23\n",
      "  load_throughput: 16956959.774\n",
      "  load_time_ms: 0.236\n",
      "  training_iteration_time_ms: 10530.12\n",
      "  update_time_ms: 2.415\n",
      "timestamp: 1660564729\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 312000\n",
      "training_iteration: 78\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 316000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 316000\n",
      "  num_agent_steps_trained: 316000\n",
      "  num_env_steps_sampled: 316000\n",
      "  num_env_steps_trained: 316000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-59-03\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -746.6280828910684\n",
      "episode_reward_mean: -1109.903909253112\n",
      "episode_reward_min: -1705.7785939326368\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1580\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6081859469413757\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4933559894561768\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01395728625357151\n",
      "        model: {}\n",
      "        policy_loss: 0.011884117498993874\n",
      "        total_loss: 9.8276948928833\n",
      "        vf_explained_var: -0.021187027916312218\n",
      "        vf_loss: 9.80732250213623\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 316000\n",
      "  num_agent_steps_trained: 316000\n",
      "  num_env_steps_sampled: 316000\n",
      "  num_env_steps_trained: 316000\n",
      "iterations_since_restore: 79\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 316000\n",
      "num_agent_steps_trained: 316000\n",
      "num_env_steps_sampled: 316000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 316000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.110000000000003\n",
      "  ram_util_percent: 59.404999999999994\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19141329292894987\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14757945304598194\n",
      "  mean_inference_ms: 0.9705249023070932\n",
      "  mean_raw_obs_processing_ms: 0.12081412503427442\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -746.6280828910684\n",
      "  episode_reward_mean: -1109.903909253112\n",
      "  episode_reward_min: -1705.7785939326368\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -849.9684395557383\n",
      "    - -1612.3633146330887\n",
      "    - -993.236918815796\n",
      "    - -974.543603589715\n",
      "    - -872.1176016223859\n",
      "    - -1612.3123675259292\n",
      "    - -879.2120045669197\n",
      "    - -877.981288238297\n",
      "    - -1175.4196495436365\n",
      "    - -1051.8690345896007\n",
      "    - -859.7933309446175\n",
      "    - -877.8495887337622\n",
      "    - -1286.7428472962667\n",
      "    - -1168.7367868065319\n",
      "    - -1269.9904548816562\n",
      "    - -972.0667586558069\n",
      "    - -1173.251796590443\n",
      "    - -1051.6844569289726\n",
      "    - -1627.677202678112\n",
      "    - -1071.2021228151023\n",
      "    - -903.3545807773678\n",
      "    - -968.9655431690267\n",
      "    - -986.4044687687604\n",
      "    - -870.1392350870162\n",
      "    - -1402.0911551464774\n",
      "    - -1573.5995002243967\n",
      "    - -1064.2466263411136\n",
      "    - -957.9359894923897\n",
      "    - -1502.6962084840652\n",
      "    - -841.3415710792159\n",
      "    - -1525.026269044401\n",
      "    - -888.0713947627281\n",
      "    - -970.3527884120756\n",
      "    - -880.1138490550387\n",
      "    - -886.426447064621\n",
      "    - -948.2913941590951\n",
      "    - -1326.676108558978\n",
      "    - -875.4382630036622\n",
      "    - -891.3796587454775\n",
      "    - -967.5340787294866\n",
      "    - -933.5631782104873\n",
      "    - -1007.787609233676\n",
      "    - -1088.0155662050568\n",
      "    - -1328.5767845285495\n",
      "    - -891.0017403668425\n",
      "    - -990.3850182941933\n",
      "    - -984.0661249560898\n",
      "    - -1692.1242970463609\n",
      "    - -967.7399538900056\n",
      "    - -1213.2733710417112\n",
      "    - -1076.6860260534916\n",
      "    - -1004.5471876855728\n",
      "    - -1066.6127767703151\n",
      "    - -1541.3795144380613\n",
      "    - -1413.918091289017\n",
      "    - -1307.2962783400908\n",
      "    - -887.836582855169\n",
      "    - -970.2761705607485\n",
      "    - -1288.4597108446965\n",
      "    - -1191.5066245782334\n",
      "    - -988.9388929034474\n",
      "    - -889.591181374075\n",
      "    - -1254.309131671519\n",
      "    - -891.4502020134249\n",
      "    - -944.2703336986546\n",
      "    - -1568.7941256390113\n",
      "    - -1357.2234183568264\n",
      "    - -1585.5944530283657\n",
      "    - -856.9132126276451\n",
      "    - -876.222605700912\n",
      "    - -978.214505385279\n",
      "    - -1681.8670508173316\n",
      "    - -974.6138529849075\n",
      "    - -889.4585742395514\n",
      "    - -1136.0841014936827\n",
      "    - -1160.839620030548\n",
      "    - -879.9965533736386\n",
      "    - -892.3005085799656\n",
      "    - -914.4775026289707\n",
      "    - -1080.4888819350956\n",
      "    - -801.4124873348435\n",
      "    - -892.3047941069369\n",
      "    - -746.6280828910684\n",
      "    - -1443.350188269365\n",
      "    - -1705.7785939326368\n",
      "    - -851.2244455826894\n",
      "    - -903.3984512824578\n",
      "    - -1067.1173027171535\n",
      "    - -957.3476676016326\n",
      "    - -1649.826885161334\n",
      "    - -941.7935060090647\n",
      "    - -883.6120031704876\n",
      "    - -1328.8395979070938\n",
      "    - -878.6268246042007\n",
      "    - -976.2683700498517\n",
      "    - -1130.4226566157163\n",
      "    - -1469.4133461686092\n",
      "    - -1602.7193235999434\n",
      "    - -1481.9564423076429\n",
      "    - -1143.5439412094886\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19141329292894987\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14757945304598194\n",
      "    mean_inference_ms: 0.9705249023070932\n",
      "    mean_raw_obs_processing_ms: 0.12081412503427442\n",
      "time_since_restore: 821.9391767978668\n",
      "time_this_iter_s: 13.844892263412476\n",
      "time_total_s: 821.9391767978668\n",
      "timers:\n",
      "  learn_throughput: 801.184\n",
      "  learn_time_ms: 4992.614\n",
      "  load_throughput: 16730370.961\n",
      "  load_time_ms: 0.239\n",
      "  training_iteration_time_ms: 10968.209\n",
      "  update_time_ms: 2.403\n",
      "timestamp: 1660564743\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 316000\n",
      "training_iteration: 79\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 320000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 320000\n",
      "  num_agent_steps_trained: 320000\n",
      "  num_env_steps_sampled: 320000\n",
      "  num_env_steps_trained: 320000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-59-15\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -746.6280828910684\n",
      "episode_reward_mean: -1098.9705882585858\n",
      "episode_reward_min: -1705.7785939326368\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1600\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6081859469413757\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6885395646095276\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011003009043633938\n",
      "        model: {}\n",
      "        policy_loss: 0.010522891767323017\n",
      "        total_loss: 9.779546737670898\n",
      "        vf_explained_var: -0.025574928149580956\n",
      "        vf_loss: 9.762332916259766\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 320000\n",
      "  num_agent_steps_trained: 320000\n",
      "  num_env_steps_sampled: 320000\n",
      "  num_env_steps_trained: 320000\n",
      "iterations_since_restore: 80\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 320000\n",
      "num_agent_steps_trained: 320000\n",
      "num_env_steps_sampled: 320000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 320000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.96470588235293\n",
      "  ram_util_percent: 59.764705882352935\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19161396739663253\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.147731368595205\n",
      "  mean_inference_ms: 0.9716414522803981\n",
      "  mean_raw_obs_processing_ms: 0.12091917863351374\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -746.6280828910684\n",
      "  episode_reward_mean: -1098.9705882585858\n",
      "  episode_reward_min: -1705.7785939326368\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -903.3545807773678\n",
      "    - -968.9655431690267\n",
      "    - -986.4044687687604\n",
      "    - -870.1392350870162\n",
      "    - -1402.0911551464774\n",
      "    - -1573.5995002243967\n",
      "    - -1064.2466263411136\n",
      "    - -957.9359894923897\n",
      "    - -1502.6962084840652\n",
      "    - -841.3415710792159\n",
      "    - -1525.026269044401\n",
      "    - -888.0713947627281\n",
      "    - -970.3527884120756\n",
      "    - -880.1138490550387\n",
      "    - -886.426447064621\n",
      "    - -948.2913941590951\n",
      "    - -1326.676108558978\n",
      "    - -875.4382630036622\n",
      "    - -891.3796587454775\n",
      "    - -967.5340787294866\n",
      "    - -933.5631782104873\n",
      "    - -1007.787609233676\n",
      "    - -1088.0155662050568\n",
      "    - -1328.5767845285495\n",
      "    - -891.0017403668425\n",
      "    - -990.3850182941933\n",
      "    - -984.0661249560898\n",
      "    - -1692.1242970463609\n",
      "    - -967.7399538900056\n",
      "    - -1213.2733710417112\n",
      "    - -1076.6860260534916\n",
      "    - -1004.5471876855728\n",
      "    - -1066.6127767703151\n",
      "    - -1541.3795144380613\n",
      "    - -1413.918091289017\n",
      "    - -1307.2962783400908\n",
      "    - -887.836582855169\n",
      "    - -970.2761705607485\n",
      "    - -1288.4597108446965\n",
      "    - -1191.5066245782334\n",
      "    - -988.9388929034474\n",
      "    - -889.591181374075\n",
      "    - -1254.309131671519\n",
      "    - -891.4502020134249\n",
      "    - -944.2703336986546\n",
      "    - -1568.7941256390113\n",
      "    - -1357.2234183568264\n",
      "    - -1585.5944530283657\n",
      "    - -856.9132126276451\n",
      "    - -876.222605700912\n",
      "    - -978.214505385279\n",
      "    - -1681.8670508173316\n",
      "    - -974.6138529849075\n",
      "    - -889.4585742395514\n",
      "    - -1136.0841014936827\n",
      "    - -1160.839620030548\n",
      "    - -879.9965533736386\n",
      "    - -892.3005085799656\n",
      "    - -914.4775026289707\n",
      "    - -1080.4888819350956\n",
      "    - -801.4124873348435\n",
      "    - -892.3047941069369\n",
      "    - -746.6280828910684\n",
      "    - -1443.350188269365\n",
      "    - -1705.7785939326368\n",
      "    - -851.2244455826894\n",
      "    - -903.3984512824578\n",
      "    - -1067.1173027171535\n",
      "    - -957.3476676016326\n",
      "    - -1649.826885161334\n",
      "    - -941.7935060090647\n",
      "    - -883.6120031704876\n",
      "    - -1328.8395979070938\n",
      "    - -878.6268246042007\n",
      "    - -976.2683700498517\n",
      "    - -1130.4226566157163\n",
      "    - -1469.4133461686092\n",
      "    - -1602.7193235999434\n",
      "    - -1481.9564423076429\n",
      "    - -1143.5439412094886\n",
      "    - -1194.8546430206998\n",
      "    - -1373.0994433217895\n",
      "    - -977.006647877787\n",
      "    - -883.8319024983688\n",
      "    - -1692.4657532980304\n",
      "    - -1101.2274733848994\n",
      "    - -766.6555717740417\n",
      "    - -850.356288568123\n",
      "    - -1332.8494283636935\n",
      "    - -1033.9884209142026\n",
      "    - -1440.8621770631592\n",
      "    - -786.7103829002864\n",
      "    - -891.7201258036436\n",
      "    - -894.3172962703883\n",
      "    - -901.0932125708712\n",
      "    - -965.1289925751145\n",
      "    - -1015.2239505432806\n",
      "    - -1259.7374315174932\n",
      "    - -925.545876687541\n",
      "    - -878.0124506063515\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19161396739663253\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.147731368595205\n",
      "    mean_inference_ms: 0.9716414522803981\n",
      "    mean_raw_obs_processing_ms: 0.12091917863351374\n",
      "time_since_restore: 834.2170567512512\n",
      "time_this_iter_s: 12.2778799533844\n",
      "time_total_s: 834.2170567512512\n",
      "timers:\n",
      "  learn_throughput: 782.48\n",
      "  learn_time_ms: 5111.952\n",
      "  load_throughput: 16438581.227\n",
      "  load_time_ms: 0.243\n",
      "  training_iteration_time_ms: 11260.381\n",
      "  update_time_ms: 2.504\n",
      "timestamp: 1660564755\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 320000\n",
      "training_iteration: 80\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 324000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 324000\n",
      "  num_agent_steps_trained: 324000\n",
      "  num_env_steps_sampled: 324000\n",
      "  num_env_steps_trained: 324000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-59-25\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -746.6280828910684\n",
      "episode_reward_mean: -1119.6272234736869\n",
      "episode_reward_min: -1742.0334542593957\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1620\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6081859469413757\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6480358839035034\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012966898269951344\n",
      "        model: {}\n",
      "        policy_loss: 0.009587624110281467\n",
      "        total_loss: 9.766103744506836\n",
      "        vf_explained_var: -0.02733764611184597\n",
      "        vf_loss: 9.74863052368164\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 324000\n",
      "  num_agent_steps_trained: 324000\n",
      "  num_env_steps_sampled: 324000\n",
      "  num_env_steps_trained: 324000\n",
      "iterations_since_restore: 81\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 324000\n",
      "num_agent_steps_trained: 324000\n",
      "num_env_steps_sampled: 324000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 324000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 37.586666666666666\n",
      "  ram_util_percent: 59.453333333333326\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19182092055327943\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14789547702848882\n",
      "  mean_inference_ms: 0.9728161795749343\n",
      "  mean_raw_obs_processing_ms: 0.12102942239329183\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -746.6280828910684\n",
      "  episode_reward_mean: -1119.6272234736869\n",
      "  episode_reward_min: -1742.0334542593957\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -933.5631782104873\n",
      "    - -1007.787609233676\n",
      "    - -1088.0155662050568\n",
      "    - -1328.5767845285495\n",
      "    - -891.0017403668425\n",
      "    - -990.3850182941933\n",
      "    - -984.0661249560898\n",
      "    - -1692.1242970463609\n",
      "    - -967.7399538900056\n",
      "    - -1213.2733710417112\n",
      "    - -1076.6860260534916\n",
      "    - -1004.5471876855728\n",
      "    - -1066.6127767703151\n",
      "    - -1541.3795144380613\n",
      "    - -1413.918091289017\n",
      "    - -1307.2962783400908\n",
      "    - -887.836582855169\n",
      "    - -970.2761705607485\n",
      "    - -1288.4597108446965\n",
      "    - -1191.5066245782334\n",
      "    - -988.9388929034474\n",
      "    - -889.591181374075\n",
      "    - -1254.309131671519\n",
      "    - -891.4502020134249\n",
      "    - -944.2703336986546\n",
      "    - -1568.7941256390113\n",
      "    - -1357.2234183568264\n",
      "    - -1585.5944530283657\n",
      "    - -856.9132126276451\n",
      "    - -876.222605700912\n",
      "    - -978.214505385279\n",
      "    - -1681.8670508173316\n",
      "    - -974.6138529849075\n",
      "    - -889.4585742395514\n",
      "    - -1136.0841014936827\n",
      "    - -1160.839620030548\n",
      "    - -879.9965533736386\n",
      "    - -892.3005085799656\n",
      "    - -914.4775026289707\n",
      "    - -1080.4888819350956\n",
      "    - -801.4124873348435\n",
      "    - -892.3047941069369\n",
      "    - -746.6280828910684\n",
      "    - -1443.350188269365\n",
      "    - -1705.7785939326368\n",
      "    - -851.2244455826894\n",
      "    - -903.3984512824578\n",
      "    - -1067.1173027171535\n",
      "    - -957.3476676016326\n",
      "    - -1649.826885161334\n",
      "    - -941.7935060090647\n",
      "    - -883.6120031704876\n",
      "    - -1328.8395979070938\n",
      "    - -878.6268246042007\n",
      "    - -976.2683700498517\n",
      "    - -1130.4226566157163\n",
      "    - -1469.4133461686092\n",
      "    - -1602.7193235999434\n",
      "    - -1481.9564423076429\n",
      "    - -1143.5439412094886\n",
      "    - -1194.8546430206998\n",
      "    - -1373.0994433217895\n",
      "    - -977.006647877787\n",
      "    - -883.8319024983688\n",
      "    - -1692.4657532980304\n",
      "    - -1101.2274733848994\n",
      "    - -766.6555717740417\n",
      "    - -850.356288568123\n",
      "    - -1332.8494283636935\n",
      "    - -1033.9884209142026\n",
      "    - -1440.8621770631592\n",
      "    - -786.7103829002864\n",
      "    - -891.7201258036436\n",
      "    - -894.3172962703883\n",
      "    - -901.0932125708712\n",
      "    - -965.1289925751145\n",
      "    - -1015.2239505432806\n",
      "    - -1259.7374315174932\n",
      "    - -925.545876687541\n",
      "    - -878.0124506063515\n",
      "    - -1442.1086833224779\n",
      "    - -1062.0611157172168\n",
      "    - -891.3697860028017\n",
      "    - -983.5864133880118\n",
      "    - -1219.5543272419304\n",
      "    - -881.6012722672448\n",
      "    - -1625.5207116557226\n",
      "    - -879.6001427275254\n",
      "    - -1135.5114305184354\n",
      "    - -876.9216218189006\n",
      "    - -851.9262926366423\n",
      "    - -886.6385950502278\n",
      "    - -890.2721375562919\n",
      "    - -1273.8721849607089\n",
      "    - -765.0596741443054\n",
      "    - -1562.488654348273\n",
      "    - -1551.2632736959793\n",
      "    - -1629.6890830176906\n",
      "    - -1742.0334542593957\n",
      "    - -1144.6697972856903\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19182092055327943\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14789547702848882\n",
      "    mean_inference_ms: 0.9728161795749343\n",
      "    mean_raw_obs_processing_ms: 0.12102942239329183\n",
      "time_since_restore: 844.2332351207733\n",
      "time_this_iter_s: 10.016178369522095\n",
      "time_total_s: 844.2332351207733\n",
      "timers:\n",
      "  learn_throughput: 778.253\n",
      "  learn_time_ms: 5139.719\n",
      "  load_throughput: 16464392.542\n",
      "  load_time_ms: 0.243\n",
      "  training_iteration_time_ms: 11320.092\n",
      "  update_time_ms: 2.43\n",
      "timestamp: 1660564765\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 324000\n",
      "training_iteration: 81\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 328000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 328000\n",
      "  num_agent_steps_trained: 328000\n",
      "  num_env_steps_sampled: 328000\n",
      "  num_env_steps_trained: 328000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-59-35\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -746.6280828910684\n",
      "episode_reward_mean: -1118.6396523536753\n",
      "episode_reward_min: -1742.0334542593957\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1640\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6081859469413757\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2687398195266724\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008600540459156036\n",
      "        model: {}\n",
      "        policy_loss: 0.010645031929016113\n",
      "        total_loss: 9.866596221923828\n",
      "        vf_explained_var: -0.02765587903559208\n",
      "        vf_loss: 9.850720405578613\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 328000\n",
      "  num_agent_steps_trained: 328000\n",
      "  num_env_steps_sampled: 328000\n",
      "  num_env_steps_trained: 328000\n",
      "iterations_since_restore: 82\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 328000\n",
      "num_agent_steps_trained: 328000\n",
      "num_env_steps_sampled: 328000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 328000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 47.621428571428574\n",
      "  ram_util_percent: 59.39999999999999\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19199682249999703\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1480353673594157\n",
      "  mean_inference_ms: 0.9738016760876986\n",
      "  mean_raw_obs_processing_ms: 0.12112269177352297\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -746.6280828910684\n",
      "  episode_reward_mean: -1118.6396523536753\n",
      "  episode_reward_min: -1742.0334542593957\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -988.9388929034474\n",
      "    - -889.591181374075\n",
      "    - -1254.309131671519\n",
      "    - -891.4502020134249\n",
      "    - -944.2703336986546\n",
      "    - -1568.7941256390113\n",
      "    - -1357.2234183568264\n",
      "    - -1585.5944530283657\n",
      "    - -856.9132126276451\n",
      "    - -876.222605700912\n",
      "    - -978.214505385279\n",
      "    - -1681.8670508173316\n",
      "    - -974.6138529849075\n",
      "    - -889.4585742395514\n",
      "    - -1136.0841014936827\n",
      "    - -1160.839620030548\n",
      "    - -879.9965533736386\n",
      "    - -892.3005085799656\n",
      "    - -914.4775026289707\n",
      "    - -1080.4888819350956\n",
      "    - -801.4124873348435\n",
      "    - -892.3047941069369\n",
      "    - -746.6280828910684\n",
      "    - -1443.350188269365\n",
      "    - -1705.7785939326368\n",
      "    - -851.2244455826894\n",
      "    - -903.3984512824578\n",
      "    - -1067.1173027171535\n",
      "    - -957.3476676016326\n",
      "    - -1649.826885161334\n",
      "    - -941.7935060090647\n",
      "    - -883.6120031704876\n",
      "    - -1328.8395979070938\n",
      "    - -878.6268246042007\n",
      "    - -976.2683700498517\n",
      "    - -1130.4226566157163\n",
      "    - -1469.4133461686092\n",
      "    - -1602.7193235999434\n",
      "    - -1481.9564423076429\n",
      "    - -1143.5439412094886\n",
      "    - -1194.8546430206998\n",
      "    - -1373.0994433217895\n",
      "    - -977.006647877787\n",
      "    - -883.8319024983688\n",
      "    - -1692.4657532980304\n",
      "    - -1101.2274733848994\n",
      "    - -766.6555717740417\n",
      "    - -850.356288568123\n",
      "    - -1332.8494283636935\n",
      "    - -1033.9884209142026\n",
      "    - -1440.8621770631592\n",
      "    - -786.7103829002864\n",
      "    - -891.7201258036436\n",
      "    - -894.3172962703883\n",
      "    - -901.0932125708712\n",
      "    - -965.1289925751145\n",
      "    - -1015.2239505432806\n",
      "    - -1259.7374315174932\n",
      "    - -925.545876687541\n",
      "    - -878.0124506063515\n",
      "    - -1442.1086833224779\n",
      "    - -1062.0611157172168\n",
      "    - -891.3697860028017\n",
      "    - -983.5864133880118\n",
      "    - -1219.5543272419304\n",
      "    - -881.6012722672448\n",
      "    - -1625.5207116557226\n",
      "    - -879.6001427275254\n",
      "    - -1135.5114305184354\n",
      "    - -876.9216218189006\n",
      "    - -851.9262926366423\n",
      "    - -886.6385950502278\n",
      "    - -890.2721375562919\n",
      "    - -1273.8721849607089\n",
      "    - -765.0596741443054\n",
      "    - -1562.488654348273\n",
      "    - -1551.2632736959793\n",
      "    - -1629.6890830176906\n",
      "    - -1742.0334542593957\n",
      "    - -1144.6697972856903\n",
      "    - -883.556735365993\n",
      "    - -1508.3288062382744\n",
      "    - -889.2476669693838\n",
      "    - -1254.8301034817073\n",
      "    - -881.170582576497\n",
      "    - -968.1354659070768\n",
      "    - -1303.939544243615\n",
      "    - -1218.507235099593\n",
      "    - -880.1547161537726\n",
      "    - -882.8142290220337\n",
      "    - -888.486014835585\n",
      "    - -1480.3319310055376\n",
      "    - -1128.554138687754\n",
      "    - -890.2450051030783\n",
      "    - -887.2140761489518\n",
      "    - -1587.3606107302603\n",
      "    - -1318.0463394610288\n",
      "    - -1485.1642041456505\n",
      "    - -1094.9744285253\n",
      "    - -1315.2336614861304\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19199682249999703\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1480353673594157\n",
      "    mean_inference_ms: 0.9738016760876986\n",
      "    mean_raw_obs_processing_ms: 0.12112269177352297\n",
      "time_since_restore: 854.5598661899567\n",
      "time_this_iter_s: 10.32663106918335\n",
      "time_total_s: 854.5598661899567\n",
      "timers:\n",
      "  learn_throughput: 779.191\n",
      "  learn_time_ms: 5133.529\n",
      "  load_throughput: 16428922.836\n",
      "  load_time_ms: 0.243\n",
      "  training_iteration_time_ms: 11314.345\n",
      "  update_time_ms: 2.417\n",
      "timestamp: 1660564775\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 328000\n",
      "training_iteration: 82\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 332000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 332000\n",
      "  num_agent_steps_trained: 332000\n",
      "  num_env_steps_sampled: 332000\n",
      "  num_env_steps_trained: 332000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-59-47\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -746.6280828910684\n",
      "episode_reward_mean: -1107.3262343712279\n",
      "episode_reward_min: -1742.0334542593957\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1660\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6081859469413757\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.2143712341785431\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008436712436378002\n",
      "        model: {}\n",
      "        policy_loss: 0.010239699855446815\n",
      "        total_loss: 9.793153762817383\n",
      "        vf_explained_var: -0.022951578721404076\n",
      "        vf_loss: 9.777782440185547\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 332000\n",
      "  num_agent_steps_trained: 332000\n",
      "  num_env_steps_sampled: 332000\n",
      "  num_env_steps_trained: 332000\n",
      "iterations_since_restore: 83\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 332000\n",
      "num_agent_steps_trained: 332000\n",
      "num_env_steps_sampled: 332000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 332000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 46.082352941176474\n",
      "  ram_util_percent: 59.44705882352941\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19219249437099242\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14818303295747234\n",
      "  mean_inference_ms: 0.9748949323060322\n",
      "  mean_raw_obs_processing_ms: 0.12123305575021977\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -746.6280828910684\n",
      "  episode_reward_mean: -1107.3262343712279\n",
      "  episode_reward_min: -1742.0334542593957\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -801.4124873348435\n",
      "    - -892.3047941069369\n",
      "    - -746.6280828910684\n",
      "    - -1443.350188269365\n",
      "    - -1705.7785939326368\n",
      "    - -851.2244455826894\n",
      "    - -903.3984512824578\n",
      "    - -1067.1173027171535\n",
      "    - -957.3476676016326\n",
      "    - -1649.826885161334\n",
      "    - -941.7935060090647\n",
      "    - -883.6120031704876\n",
      "    - -1328.8395979070938\n",
      "    - -878.6268246042007\n",
      "    - -976.2683700498517\n",
      "    - -1130.4226566157163\n",
      "    - -1469.4133461686092\n",
      "    - -1602.7193235999434\n",
      "    - -1481.9564423076429\n",
      "    - -1143.5439412094886\n",
      "    - -1194.8546430206998\n",
      "    - -1373.0994433217895\n",
      "    - -977.006647877787\n",
      "    - -883.8319024983688\n",
      "    - -1692.4657532980304\n",
      "    - -1101.2274733848994\n",
      "    - -766.6555717740417\n",
      "    - -850.356288568123\n",
      "    - -1332.8494283636935\n",
      "    - -1033.9884209142026\n",
      "    - -1440.8621770631592\n",
      "    - -786.7103829002864\n",
      "    - -891.7201258036436\n",
      "    - -894.3172962703883\n",
      "    - -901.0932125708712\n",
      "    - -965.1289925751145\n",
      "    - -1015.2239505432806\n",
      "    - -1259.7374315174932\n",
      "    - -925.545876687541\n",
      "    - -878.0124506063515\n",
      "    - -1442.1086833224779\n",
      "    - -1062.0611157172168\n",
      "    - -891.3697860028017\n",
      "    - -983.5864133880118\n",
      "    - -1219.5543272419304\n",
      "    - -881.6012722672448\n",
      "    - -1625.5207116557226\n",
      "    - -879.6001427275254\n",
      "    - -1135.5114305184354\n",
      "    - -876.9216218189006\n",
      "    - -851.9262926366423\n",
      "    - -886.6385950502278\n",
      "    - -890.2721375562919\n",
      "    - -1273.8721849607089\n",
      "    - -765.0596741443054\n",
      "    - -1562.488654348273\n",
      "    - -1551.2632736959793\n",
      "    - -1629.6890830176906\n",
      "    - -1742.0334542593957\n",
      "    - -1144.6697972856903\n",
      "    - -883.556735365993\n",
      "    - -1508.3288062382744\n",
      "    - -889.2476669693838\n",
      "    - -1254.8301034817073\n",
      "    - -881.170582576497\n",
      "    - -968.1354659070768\n",
      "    - -1303.939544243615\n",
      "    - -1218.507235099593\n",
      "    - -880.1547161537726\n",
      "    - -882.8142290220337\n",
      "    - -888.486014835585\n",
      "    - -1480.3319310055376\n",
      "    - -1128.554138687754\n",
      "    - -890.2450051030783\n",
      "    - -887.2140761489518\n",
      "    - -1587.3606107302603\n",
      "    - -1318.0463394610288\n",
      "    - -1485.1642041456505\n",
      "    - -1094.9744285253\n",
      "    - -1315.2336614861304\n",
      "    - -969.648686564925\n",
      "    - -891.3241461439911\n",
      "    - -881.4509747089893\n",
      "    - -1264.86168979701\n",
      "    - -972.8400295549538\n",
      "    - -898.4367804134405\n",
      "    - -888.46501508695\n",
      "    - -1504.4904299195277\n",
      "    - -1671.0430978965935\n",
      "    - -1117.294072159728\n",
      "    - -887.1083025374603\n",
      "    - -1278.5491716509453\n",
      "    - -881.9627225548614\n",
      "    - -978.173596409212\n",
      "    - -967.5256265081474\n",
      "    - -1187.265729857066\n",
      "    - -884.6150537490266\n",
      "    - -889.3852372554409\n",
      "    - -896.7827385391819\n",
      "    - -759.0838089306685\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19219249437099242\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14818303295747234\n",
      "    mean_inference_ms: 0.9748949323060322\n",
      "    mean_raw_obs_processing_ms: 0.12123305575021977\n",
      "time_since_restore: 866.0012204647064\n",
      "time_this_iter_s: 11.441354274749756\n",
      "time_total_s: 866.0012204647064\n",
      "timers:\n",
      "  learn_throughput: 780.819\n",
      "  learn_time_ms: 5122.825\n",
      "  load_throughput: 16377602.499\n",
      "  load_time_ms: 0.244\n",
      "  training_iteration_time_ms: 11281.589\n",
      "  update_time_ms: 2.44\n",
      "timestamp: 1660564787\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 332000\n",
      "training_iteration: 83\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 336000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 336000\n",
      "  num_agent_steps_trained: 336000\n",
      "  num_env_steps_sampled: 336000\n",
      "  num_env_steps_trained: 336000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_13-59-57\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -759.0838089306685\n",
      "episode_reward_mean: -1093.5663971131116\n",
      "episode_reward_min: -1742.0334542593957\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1680\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6081859469413757\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6570658087730408\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011344085447490215\n",
      "        model: {}\n",
      "        policy_loss: 0.0070713902823626995\n",
      "        total_loss: 9.881972312927246\n",
      "        vf_explained_var: -0.02272059954702854\n",
      "        vf_loss: 9.868003845214844\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 336000\n",
      "  num_agent_steps_trained: 336000\n",
      "  num_env_steps_sampled: 336000\n",
      "  num_env_steps_trained: 336000\n",
      "iterations_since_restore: 84\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 336000\n",
      "num_agent_steps_trained: 336000\n",
      "num_env_steps_sampled: 336000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 336000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 51.77333333333333\n",
      "  ram_util_percent: 59.41333333333332\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19229440848810456\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14826189672373183\n",
      "  mean_inference_ms: 0.9754509076495153\n",
      "  mean_raw_obs_processing_ms: 0.12128599275616118\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -759.0838089306685\n",
      "  episode_reward_mean: -1093.5663971131116\n",
      "  episode_reward_min: -1742.0334542593957\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1194.8546430206998\n",
      "    - -1373.0994433217895\n",
      "    - -977.006647877787\n",
      "    - -883.8319024983688\n",
      "    - -1692.4657532980304\n",
      "    - -1101.2274733848994\n",
      "    - -766.6555717740417\n",
      "    - -850.356288568123\n",
      "    - -1332.8494283636935\n",
      "    - -1033.9884209142026\n",
      "    - -1440.8621770631592\n",
      "    - -786.7103829002864\n",
      "    - -891.7201258036436\n",
      "    - -894.3172962703883\n",
      "    - -901.0932125708712\n",
      "    - -965.1289925751145\n",
      "    - -1015.2239505432806\n",
      "    - -1259.7374315174932\n",
      "    - -925.545876687541\n",
      "    - -878.0124506063515\n",
      "    - -1442.1086833224779\n",
      "    - -1062.0611157172168\n",
      "    - -891.3697860028017\n",
      "    - -983.5864133880118\n",
      "    - -1219.5543272419304\n",
      "    - -881.6012722672448\n",
      "    - -1625.5207116557226\n",
      "    - -879.6001427275254\n",
      "    - -1135.5114305184354\n",
      "    - -876.9216218189006\n",
      "    - -851.9262926366423\n",
      "    - -886.6385950502278\n",
      "    - -890.2721375562919\n",
      "    - -1273.8721849607089\n",
      "    - -765.0596741443054\n",
      "    - -1562.488654348273\n",
      "    - -1551.2632736959793\n",
      "    - -1629.6890830176906\n",
      "    - -1742.0334542593957\n",
      "    - -1144.6697972856903\n",
      "    - -883.556735365993\n",
      "    - -1508.3288062382744\n",
      "    - -889.2476669693838\n",
      "    - -1254.8301034817073\n",
      "    - -881.170582576497\n",
      "    - -968.1354659070768\n",
      "    - -1303.939544243615\n",
      "    - -1218.507235099593\n",
      "    - -880.1547161537726\n",
      "    - -882.8142290220337\n",
      "    - -888.486014835585\n",
      "    - -1480.3319310055376\n",
      "    - -1128.554138687754\n",
      "    - -890.2450051030783\n",
      "    - -887.2140761489518\n",
      "    - -1587.3606107302603\n",
      "    - -1318.0463394610288\n",
      "    - -1485.1642041456505\n",
      "    - -1094.9744285253\n",
      "    - -1315.2336614861304\n",
      "    - -969.648686564925\n",
      "    - -891.3241461439911\n",
      "    - -881.4509747089893\n",
      "    - -1264.86168979701\n",
      "    - -972.8400295549538\n",
      "    - -898.4367804134405\n",
      "    - -888.46501508695\n",
      "    - -1504.4904299195277\n",
      "    - -1671.0430978965935\n",
      "    - -1117.294072159728\n",
      "    - -887.1083025374603\n",
      "    - -1278.5491716509453\n",
      "    - -881.9627225548614\n",
      "    - -978.173596409212\n",
      "    - -967.5256265081474\n",
      "    - -1187.265729857066\n",
      "    - -884.6150537490266\n",
      "    - -889.3852372554409\n",
      "    - -896.7827385391819\n",
      "    - -759.0838089306685\n",
      "    - -1326.8901767105417\n",
      "    - -1145.5122584660478\n",
      "    - -879.9299945957519\n",
      "    - -1303.8483441381472\n",
      "    - -870.9238084474417\n",
      "    - -1402.1811885004531\n",
      "    - -1188.7195309604792\n",
      "    - -990.7904535034722\n",
      "    - -1064.9874433855773\n",
      "    - -977.8830928414411\n",
      "    - -950.0375183007942\n",
      "    - -991.5020051077406\n",
      "    - -889.6507698671212\n",
      "    - -899.4724947711857\n",
      "    - -1618.075696888197\n",
      "    - -1169.9368331005821\n",
      "    - -1158.203633287486\n",
      "    - -878.6459877636482\n",
      "    - -885.1604914846239\n",
      "    - -887.2494625898381\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19229440848810456\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14826189672373183\n",
      "    mean_inference_ms: 0.9754509076495153\n",
      "    mean_raw_obs_processing_ms: 0.12128599275616118\n",
      "time_since_restore: 876.469114780426\n",
      "time_this_iter_s: 10.467894315719604\n",
      "time_total_s: 876.469114780426\n",
      "timers:\n",
      "  learn_throughput: 785.142\n",
      "  learn_time_ms: 5094.619\n",
      "  load_throughput: 16902292.968\n",
      "  load_time_ms: 0.237\n",
      "  training_iteration_time_ms: 11194.755\n",
      "  update_time_ms: 2.373\n",
      "timestamp: 1660564797\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 336000\n",
      "training_iteration: 84\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 340000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 340000\n",
      "  num_agent_steps_trained: 340000\n",
      "  num_env_steps_sampled: 340000\n",
      "  num_env_steps_trained: 340000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-00-07\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -759.0838089306685\n",
      "episode_reward_mean: -1088.907670598777\n",
      "episode_reward_min: -1742.0334542593957\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1700\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6081859469413757\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.21198789775371552\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0125906215980649\n",
      "        model: {}\n",
      "        policy_loss: 0.012784822843968868\n",
      "        total_loss: 9.920598030090332\n",
      "        vf_explained_var: -0.017440496012568474\n",
      "        vf_loss: 9.900155067443848\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 340000\n",
      "  num_agent_steps_trained: 340000\n",
      "  num_env_steps_sampled: 340000\n",
      "  num_env_steps_trained: 340000\n",
      "iterations_since_restore: 85\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 340000\n",
      "num_agent_steps_trained: 340000\n",
      "num_env_steps_sampled: 340000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 340000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.1\n",
      "  ram_util_percent: 59.39999999999999\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1922737227628937\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14825720987417176\n",
      "  mean_inference_ms: 0.9753459438203982\n",
      "  mean_raw_obs_processing_ms: 0.12127486162855497\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -759.0838089306685\n",
      "  episode_reward_mean: -1088.907670598777\n",
      "  episode_reward_min: -1742.0334542593957\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1442.1086833224779\n",
      "    - -1062.0611157172168\n",
      "    - -891.3697860028017\n",
      "    - -983.5864133880118\n",
      "    - -1219.5543272419304\n",
      "    - -881.6012722672448\n",
      "    - -1625.5207116557226\n",
      "    - -879.6001427275254\n",
      "    - -1135.5114305184354\n",
      "    - -876.9216218189006\n",
      "    - -851.9262926366423\n",
      "    - -886.6385950502278\n",
      "    - -890.2721375562919\n",
      "    - -1273.8721849607089\n",
      "    - -765.0596741443054\n",
      "    - -1562.488654348273\n",
      "    - -1551.2632736959793\n",
      "    - -1629.6890830176906\n",
      "    - -1742.0334542593957\n",
      "    - -1144.6697972856903\n",
      "    - -883.556735365993\n",
      "    - -1508.3288062382744\n",
      "    - -889.2476669693838\n",
      "    - -1254.8301034817073\n",
      "    - -881.170582576497\n",
      "    - -968.1354659070768\n",
      "    - -1303.939544243615\n",
      "    - -1218.507235099593\n",
      "    - -880.1547161537726\n",
      "    - -882.8142290220337\n",
      "    - -888.486014835585\n",
      "    - -1480.3319310055376\n",
      "    - -1128.554138687754\n",
      "    - -890.2450051030783\n",
      "    - -887.2140761489518\n",
      "    - -1587.3606107302603\n",
      "    - -1318.0463394610288\n",
      "    - -1485.1642041456505\n",
      "    - -1094.9744285253\n",
      "    - -1315.2336614861304\n",
      "    - -969.648686564925\n",
      "    - -891.3241461439911\n",
      "    - -881.4509747089893\n",
      "    - -1264.86168979701\n",
      "    - -972.8400295549538\n",
      "    - -898.4367804134405\n",
      "    - -888.46501508695\n",
      "    - -1504.4904299195277\n",
      "    - -1671.0430978965935\n",
      "    - -1117.294072159728\n",
      "    - -887.1083025374603\n",
      "    - -1278.5491716509453\n",
      "    - -881.9627225548614\n",
      "    - -978.173596409212\n",
      "    - -967.5256265081474\n",
      "    - -1187.265729857066\n",
      "    - -884.6150537490266\n",
      "    - -889.3852372554409\n",
      "    - -896.7827385391819\n",
      "    - -759.0838089306685\n",
      "    - -1326.8901767105417\n",
      "    - -1145.5122584660478\n",
      "    - -879.9299945957519\n",
      "    - -1303.8483441381472\n",
      "    - -870.9238084474417\n",
      "    - -1402.1811885004531\n",
      "    - -1188.7195309604792\n",
      "    - -990.7904535034722\n",
      "    - -1064.9874433855773\n",
      "    - -977.8830928414411\n",
      "    - -950.0375183007942\n",
      "    - -991.5020051077406\n",
      "    - -889.6507698671212\n",
      "    - -899.4724947711857\n",
      "    - -1618.075696888197\n",
      "    - -1169.9368331005821\n",
      "    - -1158.203633287486\n",
      "    - -878.6459877636482\n",
      "    - -885.1604914846239\n",
      "    - -887.2494625898381\n",
      "    - -1180.9625513816077\n",
      "    - -1081.0347681225153\n",
      "    - -957.0185730200232\n",
      "    - -1034.8694952765068\n",
      "    - -865.8289059677693\n",
      "    - -947.8432795349908\n",
      "    - -963.383760576483\n",
      "    - -887.2660845890838\n",
      "    - -1577.0514297721413\n",
      "    - -994.4532556129669\n",
      "    - -1469.9549912771358\n",
      "    - -960.4337975551077\n",
      "    - -1042.024069694262\n",
      "    - -887.5447403074454\n",
      "    - -1101.1841977338593\n",
      "    - -1090.7369120259439\n",
      "    - -901.0650041986893\n",
      "    - -973.0049786320795\n",
      "    - -896.1726762922627\n",
      "    - -886.9813465554437\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1922737227628937\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14825720987417176\n",
      "    mean_inference_ms: 0.9753459438203982\n",
      "    mean_raw_obs_processing_ms: 0.12127486162855497\n",
      "time_since_restore: 886.248788356781\n",
      "time_this_iter_s: 9.77967357635498\n",
      "time_total_s: 886.248788356781\n",
      "timers:\n",
      "  learn_throughput: 787.144\n",
      "  learn_time_ms: 5081.663\n",
      "  load_throughput: 17612026.034\n",
      "  load_time_ms: 0.227\n",
      "  training_iteration_time_ms: 11113.593\n",
      "  update_time_ms: 2.369\n",
      "timestamp: 1660564807\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 340000\n",
      "training_iteration: 85\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 344000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 344000\n",
      "  num_agent_steps_trained: 344000\n",
      "  num_env_steps_sampled: 344000\n",
      "  num_env_steps_trained: 344000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-00-18\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -759.0838089306685\n",
      "episode_reward_mean: -1073.4960380647317\n",
      "episode_reward_min: -1671.0430978965935\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1720\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6081859469413757\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7666614055633545\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00923447497189045\n",
      "        model: {}\n",
      "        policy_loss: 0.008768844418227673\n",
      "        total_loss: 9.78648853302002\n",
      "        vf_explained_var: -0.022059081122279167\n",
      "        vf_loss: 9.772104263305664\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 344000\n",
      "  num_agent_steps_trained: 344000\n",
      "  num_env_steps_sampled: 344000\n",
      "  num_env_steps_trained: 344000\n",
      "iterations_since_restore: 86\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 344000\n",
      "num_agent_steps_trained: 344000\n",
      "num_env_steps_sampled: 344000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 344000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.6\n",
      "  ram_util_percent: 59.43999999999999\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19227789570081494\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14825916709677692\n",
      "  mean_inference_ms: 0.9753393877428796\n",
      "  mean_raw_obs_processing_ms: 0.12127182179950317\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -759.0838089306685\n",
      "  episode_reward_mean: -1073.4960380647317\n",
      "  episode_reward_min: -1671.0430978965935\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -883.556735365993\n",
      "    - -1508.3288062382744\n",
      "    - -889.2476669693838\n",
      "    - -1254.8301034817073\n",
      "    - -881.170582576497\n",
      "    - -968.1354659070768\n",
      "    - -1303.939544243615\n",
      "    - -1218.507235099593\n",
      "    - -880.1547161537726\n",
      "    - -882.8142290220337\n",
      "    - -888.486014835585\n",
      "    - -1480.3319310055376\n",
      "    - -1128.554138687754\n",
      "    - -890.2450051030783\n",
      "    - -887.2140761489518\n",
      "    - -1587.3606107302603\n",
      "    - -1318.0463394610288\n",
      "    - -1485.1642041456505\n",
      "    - -1094.9744285253\n",
      "    - -1315.2336614861304\n",
      "    - -969.648686564925\n",
      "    - -891.3241461439911\n",
      "    - -881.4509747089893\n",
      "    - -1264.86168979701\n",
      "    - -972.8400295549538\n",
      "    - -898.4367804134405\n",
      "    - -888.46501508695\n",
      "    - -1504.4904299195277\n",
      "    - -1671.0430978965935\n",
      "    - -1117.294072159728\n",
      "    - -887.1083025374603\n",
      "    - -1278.5491716509453\n",
      "    - -881.9627225548614\n",
      "    - -978.173596409212\n",
      "    - -967.5256265081474\n",
      "    - -1187.265729857066\n",
      "    - -884.6150537490266\n",
      "    - -889.3852372554409\n",
      "    - -896.7827385391819\n",
      "    - -759.0838089306685\n",
      "    - -1326.8901767105417\n",
      "    - -1145.5122584660478\n",
      "    - -879.9299945957519\n",
      "    - -1303.8483441381472\n",
      "    - -870.9238084474417\n",
      "    - -1402.1811885004531\n",
      "    - -1188.7195309604792\n",
      "    - -990.7904535034722\n",
      "    - -1064.9874433855773\n",
      "    - -977.8830928414411\n",
      "    - -950.0375183007942\n",
      "    - -991.5020051077406\n",
      "    - -889.6507698671212\n",
      "    - -899.4724947711857\n",
      "    - -1618.075696888197\n",
      "    - -1169.9368331005821\n",
      "    - -1158.203633287486\n",
      "    - -878.6459877636482\n",
      "    - -885.1604914846239\n",
      "    - -887.2494625898381\n",
      "    - -1180.9625513816077\n",
      "    - -1081.0347681225153\n",
      "    - -957.0185730200232\n",
      "    - -1034.8694952765068\n",
      "    - -865.8289059677693\n",
      "    - -947.8432795349908\n",
      "    - -963.383760576483\n",
      "    - -887.2660845890838\n",
      "    - -1577.0514297721413\n",
      "    - -994.4532556129669\n",
      "    - -1469.9549912771358\n",
      "    - -960.4337975551077\n",
      "    - -1042.024069694262\n",
      "    - -887.5447403074454\n",
      "    - -1101.1841977338593\n",
      "    - -1090.7369120259439\n",
      "    - -901.0650041986893\n",
      "    - -973.0049786320795\n",
      "    - -896.1726762922627\n",
      "    - -886.9813465554437\n",
      "    - -891.9898834843978\n",
      "    - -880.3419002202107\n",
      "    - -978.6622463220772\n",
      "    - -886.814368006055\n",
      "    - -877.1096946009174\n",
      "    - -884.3004411438211\n",
      "    - -881.7961347597735\n",
      "    - -954.2946880536321\n",
      "    - -982.3700997159035\n",
      "    - -913.3843269874521\n",
      "    - -1013.3217094337701\n",
      "    - -1484.9304060425395\n",
      "    - -1196.1195809169412\n",
      "    - -1234.5939866658643\n",
      "    - -1204.047465474066\n",
      "    - -1075.9629550511677\n",
      "    - -1364.5592777861748\n",
      "    - -1492.8975128037507\n",
      "    - -886.6603882145702\n",
      "    - -1670.4283325278323\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19227789570081494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14825916709677692\n",
      "    mean_inference_ms: 0.9753393877428796\n",
      "    mean_raw_obs_processing_ms: 0.12127182179950317\n",
      "time_since_restore: 896.8207306861877\n",
      "time_this_iter_s: 10.571942329406738\n",
      "time_total_s: 896.8207306861877\n",
      "timers:\n",
      "  learn_throughput: 790.931\n",
      "  learn_time_ms: 5057.33\n",
      "  load_throughput: 17663946.094\n",
      "  load_time_ms: 0.226\n",
      "  training_iteration_time_ms: 11127.038\n",
      "  update_time_ms: 2.374\n",
      "timestamp: 1660564818\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 344000\n",
      "training_iteration: 86\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 348000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 348000\n",
      "  num_agent_steps_trained: 348000\n",
      "  num_env_steps_sampled: 348000\n",
      "  num_env_steps_trained: 348000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-00-28\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -758.9635219082306\n",
      "episode_reward_mean: -1051.7937754211694\n",
      "episode_reward_min: -1671.0430978965935\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1740\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6081859469413757\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.1171569973230362\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01897292770445347\n",
      "        model: {}\n",
      "        policy_loss: 0.011422176845371723\n",
      "        total_loss: 9.906730651855469\n",
      "        vf_explained_var: -0.01767970249056816\n",
      "        vf_loss: 9.883769035339355\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 348000\n",
      "  num_agent_steps_trained: 348000\n",
      "  num_env_steps_sampled: 348000\n",
      "  num_env_steps_trained: 348000\n",
      "iterations_since_restore: 87\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 348000\n",
      "num_agent_steps_trained: 348000\n",
      "num_env_steps_sampled: 348000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 348000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 66.81428571428572\n",
      "  ram_util_percent: 59.414285714285704\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1922787865474354\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14826629654522802\n",
      "  mean_inference_ms: 0.9753556355790051\n",
      "  mean_raw_obs_processing_ms: 0.12126889476264774\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -758.9635219082306\n",
      "  episode_reward_mean: -1051.7937754211694\n",
      "  episode_reward_min: -1671.0430978965935\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -969.648686564925\n",
      "    - -891.3241461439911\n",
      "    - -881.4509747089893\n",
      "    - -1264.86168979701\n",
      "    - -972.8400295549538\n",
      "    - -898.4367804134405\n",
      "    - -888.46501508695\n",
      "    - -1504.4904299195277\n",
      "    - -1671.0430978965935\n",
      "    - -1117.294072159728\n",
      "    - -887.1083025374603\n",
      "    - -1278.5491716509453\n",
      "    - -881.9627225548614\n",
      "    - -978.173596409212\n",
      "    - -967.5256265081474\n",
      "    - -1187.265729857066\n",
      "    - -884.6150537490266\n",
      "    - -889.3852372554409\n",
      "    - -896.7827385391819\n",
      "    - -759.0838089306685\n",
      "    - -1326.8901767105417\n",
      "    - -1145.5122584660478\n",
      "    - -879.9299945957519\n",
      "    - -1303.8483441381472\n",
      "    - -870.9238084474417\n",
      "    - -1402.1811885004531\n",
      "    - -1188.7195309604792\n",
      "    - -990.7904535034722\n",
      "    - -1064.9874433855773\n",
      "    - -977.8830928414411\n",
      "    - -950.0375183007942\n",
      "    - -991.5020051077406\n",
      "    - -889.6507698671212\n",
      "    - -899.4724947711857\n",
      "    - -1618.075696888197\n",
      "    - -1169.9368331005821\n",
      "    - -1158.203633287486\n",
      "    - -878.6459877636482\n",
      "    - -885.1604914846239\n",
      "    - -887.2494625898381\n",
      "    - -1180.9625513816077\n",
      "    - -1081.0347681225153\n",
      "    - -957.0185730200232\n",
      "    - -1034.8694952765068\n",
      "    - -865.8289059677693\n",
      "    - -947.8432795349908\n",
      "    - -963.383760576483\n",
      "    - -887.2660845890838\n",
      "    - -1577.0514297721413\n",
      "    - -994.4532556129669\n",
      "    - -1469.9549912771358\n",
      "    - -960.4337975551077\n",
      "    - -1042.024069694262\n",
      "    - -887.5447403074454\n",
      "    - -1101.1841977338593\n",
      "    - -1090.7369120259439\n",
      "    - -901.0650041986893\n",
      "    - -973.0049786320795\n",
      "    - -896.1726762922627\n",
      "    - -886.9813465554437\n",
      "    - -891.9898834843978\n",
      "    - -880.3419002202107\n",
      "    - -978.6622463220772\n",
      "    - -886.814368006055\n",
      "    - -877.1096946009174\n",
      "    - -884.3004411438211\n",
      "    - -881.7961347597735\n",
      "    - -954.2946880536321\n",
      "    - -982.3700997159035\n",
      "    - -913.3843269874521\n",
      "    - -1013.3217094337701\n",
      "    - -1484.9304060425395\n",
      "    - -1196.1195809169412\n",
      "    - -1234.5939866658643\n",
      "    - -1204.047465474066\n",
      "    - -1075.9629550511677\n",
      "    - -1364.5592777861748\n",
      "    - -1492.8975128037507\n",
      "    - -886.6603882145702\n",
      "    - -1670.4283325278323\n",
      "    - -907.2148238191675\n",
      "    - -880.7979246031698\n",
      "    - -1103.207104206183\n",
      "    - -1300.9662486558361\n",
      "    - -829.676256519081\n",
      "    - -1180.3130440997882\n",
      "    - -983.2410252719413\n",
      "    - -890.6848408655075\n",
      "    - -973.593543532378\n",
      "    - -1024.1131853044972\n",
      "    - -1010.1085962710681\n",
      "    - -1151.4903965709993\n",
      "    - -1283.0054681849015\n",
      "    - -926.138899127666\n",
      "    - -758.9635219082306\n",
      "    - -903.7975434173607\n",
      "    - -971.5249153746006\n",
      "    - -982.5970409114152\n",
      "    - -963.8052280179895\n",
      "    - -1550.829624169229\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1922787865474354\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14826629654522802\n",
      "    mean_inference_ms: 0.9753556355790051\n",
      "    mean_raw_obs_processing_ms: 0.12126889476264774\n",
      "time_since_restore: 907.0339455604553\n",
      "time_this_iter_s: 10.213214874267578\n",
      "time_total_s: 907.0339455604553\n",
      "timers:\n",
      "  learn_throughput: 808.514\n",
      "  learn_time_ms: 4947.349\n",
      "  load_throughput: 18176832.069\n",
      "  load_time_ms: 0.22\n",
      "  training_iteration_time_ms: 10981.634\n",
      "  update_time_ms: 2.29\n",
      "timestamp: 1660564828\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 348000\n",
      "training_iteration: 87\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 352000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 352000\n",
      "  num_agent_steps_trained: 352000\n",
      "  num_env_steps_sampled: 352000\n",
      "  num_env_steps_trained: 352000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-00-39\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -758.9635219082306\n",
      "episode_reward_mean: -1081.862384488504\n",
      "episode_reward_min: -1670.4283325278323\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1760\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6081859469413757\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6797711849212646\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00476910499855876\n",
      "        model: {}\n",
      "        policy_loss: 0.010910519398748875\n",
      "        total_loss: 9.880566596984863\n",
      "        vf_explained_var: -0.022935794666409492\n",
      "        vf_loss: 9.866754531860352\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 352000\n",
      "  num_agent_steps_trained: 352000\n",
      "  num_env_steps_sampled: 352000\n",
      "  num_env_steps_trained: 352000\n",
      "iterations_since_restore: 88\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 352000\n",
      "num_agent_steps_trained: 352000\n",
      "num_env_steps_sampled: 352000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 352000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.4375\n",
      "  ram_util_percent: 59.34374999999999\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1922667102078049\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14826618816995604\n",
      "  mean_inference_ms: 0.9753064290037048\n",
      "  mean_raw_obs_processing_ms: 0.12125097649531388\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -758.9635219082306\n",
      "  episode_reward_mean: -1081.862384488504\n",
      "  episode_reward_min: -1670.4283325278323\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1326.8901767105417\n",
      "    - -1145.5122584660478\n",
      "    - -879.9299945957519\n",
      "    - -1303.8483441381472\n",
      "    - -870.9238084474417\n",
      "    - -1402.1811885004531\n",
      "    - -1188.7195309604792\n",
      "    - -990.7904535034722\n",
      "    - -1064.9874433855773\n",
      "    - -977.8830928414411\n",
      "    - -950.0375183007942\n",
      "    - -991.5020051077406\n",
      "    - -889.6507698671212\n",
      "    - -899.4724947711857\n",
      "    - -1618.075696888197\n",
      "    - -1169.9368331005821\n",
      "    - -1158.203633287486\n",
      "    - -878.6459877636482\n",
      "    - -885.1604914846239\n",
      "    - -887.2494625898381\n",
      "    - -1180.9625513816077\n",
      "    - -1081.0347681225153\n",
      "    - -957.0185730200232\n",
      "    - -1034.8694952765068\n",
      "    - -865.8289059677693\n",
      "    - -947.8432795349908\n",
      "    - -963.383760576483\n",
      "    - -887.2660845890838\n",
      "    - -1577.0514297721413\n",
      "    - -994.4532556129669\n",
      "    - -1469.9549912771358\n",
      "    - -960.4337975551077\n",
      "    - -1042.024069694262\n",
      "    - -887.5447403074454\n",
      "    - -1101.1841977338593\n",
      "    - -1090.7369120259439\n",
      "    - -901.0650041986893\n",
      "    - -973.0049786320795\n",
      "    - -896.1726762922627\n",
      "    - -886.9813465554437\n",
      "    - -891.9898834843978\n",
      "    - -880.3419002202107\n",
      "    - -978.6622463220772\n",
      "    - -886.814368006055\n",
      "    - -877.1096946009174\n",
      "    - -884.3004411438211\n",
      "    - -881.7961347597735\n",
      "    - -954.2946880536321\n",
      "    - -982.3700997159035\n",
      "    - -913.3843269874521\n",
      "    - -1013.3217094337701\n",
      "    - -1484.9304060425395\n",
      "    - -1196.1195809169412\n",
      "    - -1234.5939866658643\n",
      "    - -1204.047465474066\n",
      "    - -1075.9629550511677\n",
      "    - -1364.5592777861748\n",
      "    - -1492.8975128037507\n",
      "    - -886.6603882145702\n",
      "    - -1670.4283325278323\n",
      "    - -907.2148238191675\n",
      "    - -880.7979246031698\n",
      "    - -1103.207104206183\n",
      "    - -1300.9662486558361\n",
      "    - -829.676256519081\n",
      "    - -1180.3130440997882\n",
      "    - -983.2410252719413\n",
      "    - -890.6848408655075\n",
      "    - -973.593543532378\n",
      "    - -1024.1131853044972\n",
      "    - -1010.1085962710681\n",
      "    - -1151.4903965709993\n",
      "    - -1283.0054681849015\n",
      "    - -926.138899127666\n",
      "    - -758.9635219082306\n",
      "    - -903.7975434173607\n",
      "    - -971.5249153746006\n",
      "    - -982.5970409114152\n",
      "    - -963.8052280179895\n",
      "    - -1550.829624169229\n",
      "    - -880.2205004120614\n",
      "    - -886.1091920948105\n",
      "    - -1200.500595156374\n",
      "    - -887.6877159473175\n",
      "    - -1563.3799454432678\n",
      "    - -881.8713019666033\n",
      "    - -974.4857844527929\n",
      "    - -981.0798338517301\n",
      "    - -1436.3433107653582\n",
      "    - -1096.041955154261\n",
      "    - -1598.4032839535255\n",
      "    - -976.0909391986878\n",
      "    - -882.6715033610795\n",
      "    - -1003.5058497488783\n",
      "    - -1587.1913574522252\n",
      "    - -1599.0470974113198\n",
      "    - -1396.1141984000521\n",
      "    - -1175.9327944957745\n",
      "    - -1306.8723453536704\n",
      "    - -1363.6183123517874\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1922667102078049\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14826618816995604\n",
      "    mean_inference_ms: 0.9753064290037048\n",
      "    mean_raw_obs_processing_ms: 0.12125097649531388\n",
      "time_since_restore: 917.6060435771942\n",
      "time_this_iter_s: 10.572098016738892\n",
      "time_total_s: 917.6060435771942\n",
      "timers:\n",
      "  learn_throughput: 816.857\n",
      "  learn_time_ms: 4896.818\n",
      "  load_throughput: 18454753.052\n",
      "  load_time_ms: 0.217\n",
      "  training_iteration_time_ms: 10944.282\n",
      "  update_time_ms: 2.241\n",
      "timestamp: 1660564839\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 352000\n",
      "training_iteration: 88\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 356000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 356000\n",
      "  num_agent_steps_trained: 356000\n",
      "  num_env_steps_sampled: 356000\n",
      "  num_env_steps_trained: 356000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-00-51\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -758.9635219082306\n",
      "episode_reward_mean: -1100.311012354005\n",
      "episode_reward_min: -1670.4283325278323\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1780\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30409297347068787\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4937400817871094\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013522570952773094\n",
      "        model: {}\n",
      "        policy_loss: 0.011211030185222626\n",
      "        total_loss: 9.88967514038086\n",
      "        vf_explained_var: -0.009809940122067928\n",
      "        vf_loss: 9.874351501464844\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 356000\n",
      "  num_agent_steps_trained: 356000\n",
      "  num_env_steps_sampled: 356000\n",
      "  num_env_steps_trained: 356000\n",
      "iterations_since_restore: 89\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 356000\n",
      "num_agent_steps_trained: 356000\n",
      "num_env_steps_sampled: 356000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 356000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 65.39411764705882\n",
      "  ram_util_percent: 59.376470588235286\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.192286468626462\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1482958770741491\n",
      "  mean_inference_ms: 0.9755036554560875\n",
      "  mean_raw_obs_processing_ms: 0.12125561488432916\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -758.9635219082306\n",
      "  episode_reward_mean: -1100.311012354005\n",
      "  episode_reward_min: -1670.4283325278323\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1180.9625513816077\n",
      "    - -1081.0347681225153\n",
      "    - -957.0185730200232\n",
      "    - -1034.8694952765068\n",
      "    - -865.8289059677693\n",
      "    - -947.8432795349908\n",
      "    - -963.383760576483\n",
      "    - -887.2660845890838\n",
      "    - -1577.0514297721413\n",
      "    - -994.4532556129669\n",
      "    - -1469.9549912771358\n",
      "    - -960.4337975551077\n",
      "    - -1042.024069694262\n",
      "    - -887.5447403074454\n",
      "    - -1101.1841977338593\n",
      "    - -1090.7369120259439\n",
      "    - -901.0650041986893\n",
      "    - -973.0049786320795\n",
      "    - -896.1726762922627\n",
      "    - -886.9813465554437\n",
      "    - -891.9898834843978\n",
      "    - -880.3419002202107\n",
      "    - -978.6622463220772\n",
      "    - -886.814368006055\n",
      "    - -877.1096946009174\n",
      "    - -884.3004411438211\n",
      "    - -881.7961347597735\n",
      "    - -954.2946880536321\n",
      "    - -982.3700997159035\n",
      "    - -913.3843269874521\n",
      "    - -1013.3217094337701\n",
      "    - -1484.9304060425395\n",
      "    - -1196.1195809169412\n",
      "    - -1234.5939866658643\n",
      "    - -1204.047465474066\n",
      "    - -1075.9629550511677\n",
      "    - -1364.5592777861748\n",
      "    - -1492.8975128037507\n",
      "    - -886.6603882145702\n",
      "    - -1670.4283325278323\n",
      "    - -907.2148238191675\n",
      "    - -880.7979246031698\n",
      "    - -1103.207104206183\n",
      "    - -1300.9662486558361\n",
      "    - -829.676256519081\n",
      "    - -1180.3130440997882\n",
      "    - -983.2410252719413\n",
      "    - -890.6848408655075\n",
      "    - -973.593543532378\n",
      "    - -1024.1131853044972\n",
      "    - -1010.1085962710681\n",
      "    - -1151.4903965709993\n",
      "    - -1283.0054681849015\n",
      "    - -926.138899127666\n",
      "    - -758.9635219082306\n",
      "    - -903.7975434173607\n",
      "    - -971.5249153746006\n",
      "    - -982.5970409114152\n",
      "    - -963.8052280179895\n",
      "    - -1550.829624169229\n",
      "    - -880.2205004120614\n",
      "    - -886.1091920948105\n",
      "    - -1200.500595156374\n",
      "    - -887.6877159473175\n",
      "    - -1563.3799454432678\n",
      "    - -881.8713019666033\n",
      "    - -974.4857844527929\n",
      "    - -981.0798338517301\n",
      "    - -1436.3433107653582\n",
      "    - -1096.041955154261\n",
      "    - -1598.4032839535255\n",
      "    - -976.0909391986878\n",
      "    - -882.6715033610795\n",
      "    - -1003.5058497488783\n",
      "    - -1587.1913574522252\n",
      "    - -1599.0470974113198\n",
      "    - -1396.1141984000521\n",
      "    - -1175.9327944957745\n",
      "    - -1306.8723453536704\n",
      "    - -1363.6183123517874\n",
      "    - -1146.865178314985\n",
      "    - -1357.0889622372954\n",
      "    - -1000.2786742270557\n",
      "    - -1388.7541181979807\n",
      "    - -900.3363314557296\n",
      "    - -962.9717063965082\n",
      "    - -1100.849172911863\n",
      "    - -1195.7962245997478\n",
      "    - -1394.4430120164645\n",
      "    - -876.904228033334\n",
      "    - -1209.2930862119736\n",
      "    - -1545.5728739026551\n",
      "    - -1410.9698543952065\n",
      "    - -1063.7924794735368\n",
      "    - -970.5481946328141\n",
      "    - -1161.8162095733708\n",
      "    - -1349.4573070132833\n",
      "    - -972.4101108699955\n",
      "    - -1366.115272344067\n",
      "    - -950.2009744528096\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.192286468626462\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1482958770741491\n",
      "    mean_inference_ms: 0.9755036554560875\n",
      "    mean_raw_obs_processing_ms: 0.12125561488432916\n",
      "time_since_restore: 929.9103627204895\n",
      "time_this_iter_s: 12.304319143295288\n",
      "time_total_s: 929.9103627204895\n",
      "timers:\n",
      "  learn_throughput: 833.145\n",
      "  learn_time_ms: 4801.085\n",
      "  load_throughput: 17880439.092\n",
      "  load_time_ms: 0.224\n",
      "  training_iteration_time_ms: 10790.246\n",
      "  update_time_ms: 2.232\n",
      "timestamp: 1660564851\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 356000\n",
      "training_iteration: 89\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 360000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 360000\n",
      "  num_agent_steps_trained: 360000\n",
      "  num_env_steps_sampled: 360000\n",
      "  num_env_steps_trained: 360000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-01-05\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -758.9635219082306\n",
      "episode_reward_mean: -1119.3848751119438\n",
      "episode_reward_min: -1714.5803635783698\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1800\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30409297347068787\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1744048595428467\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019020305946469307\n",
      "        model: {}\n",
      "        policy_loss: 0.00470900209620595\n",
      "        total_loss: 9.8379545211792\n",
      "        vf_explained_var: -0.024066133424639702\n",
      "        vf_loss: 9.827461242675781\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 360000\n",
      "  num_agent_steps_trained: 360000\n",
      "  num_env_steps_sampled: 360000\n",
      "  num_env_steps_trained: 360000\n",
      "iterations_since_restore: 90\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 360000\n",
      "num_agent_steps_trained: 360000\n",
      "num_env_steps_sampled: 360000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 360000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 36.67\n",
      "  ram_util_percent: 59.53000000000001\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19239102998467358\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14837439259585408\n",
      "  mean_inference_ms: 0.9760969273769837\n",
      "  mean_raw_obs_processing_ms: 0.12129631640346367\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -758.9635219082306\n",
      "  episode_reward_mean: -1119.3848751119438\n",
      "  episode_reward_min: -1714.5803635783698\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -891.9898834843978\n",
      "    - -880.3419002202107\n",
      "    - -978.6622463220772\n",
      "    - -886.814368006055\n",
      "    - -877.1096946009174\n",
      "    - -884.3004411438211\n",
      "    - -881.7961347597735\n",
      "    - -954.2946880536321\n",
      "    - -982.3700997159035\n",
      "    - -913.3843269874521\n",
      "    - -1013.3217094337701\n",
      "    - -1484.9304060425395\n",
      "    - -1196.1195809169412\n",
      "    - -1234.5939866658643\n",
      "    - -1204.047465474066\n",
      "    - -1075.9629550511677\n",
      "    - -1364.5592777861748\n",
      "    - -1492.8975128037507\n",
      "    - -886.6603882145702\n",
      "    - -1670.4283325278323\n",
      "    - -907.2148238191675\n",
      "    - -880.7979246031698\n",
      "    - -1103.207104206183\n",
      "    - -1300.9662486558361\n",
      "    - -829.676256519081\n",
      "    - -1180.3130440997882\n",
      "    - -983.2410252719413\n",
      "    - -890.6848408655075\n",
      "    - -973.593543532378\n",
      "    - -1024.1131853044972\n",
      "    - -1010.1085962710681\n",
      "    - -1151.4903965709993\n",
      "    - -1283.0054681849015\n",
      "    - -926.138899127666\n",
      "    - -758.9635219082306\n",
      "    - -903.7975434173607\n",
      "    - -971.5249153746006\n",
      "    - -982.5970409114152\n",
      "    - -963.8052280179895\n",
      "    - -1550.829624169229\n",
      "    - -880.2205004120614\n",
      "    - -886.1091920948105\n",
      "    - -1200.500595156374\n",
      "    - -887.6877159473175\n",
      "    - -1563.3799454432678\n",
      "    - -881.8713019666033\n",
      "    - -974.4857844527929\n",
      "    - -981.0798338517301\n",
      "    - -1436.3433107653582\n",
      "    - -1096.041955154261\n",
      "    - -1598.4032839535255\n",
      "    - -976.0909391986878\n",
      "    - -882.6715033610795\n",
      "    - -1003.5058497488783\n",
      "    - -1587.1913574522252\n",
      "    - -1599.0470974113198\n",
      "    - -1396.1141984000521\n",
      "    - -1175.9327944957745\n",
      "    - -1306.8723453536704\n",
      "    - -1363.6183123517874\n",
      "    - -1146.865178314985\n",
      "    - -1357.0889622372954\n",
      "    - -1000.2786742270557\n",
      "    - -1388.7541181979807\n",
      "    - -900.3363314557296\n",
      "    - -962.9717063965082\n",
      "    - -1100.849172911863\n",
      "    - -1195.7962245997478\n",
      "    - -1394.4430120164645\n",
      "    - -876.904228033334\n",
      "    - -1209.2930862119736\n",
      "    - -1545.5728739026551\n",
      "    - -1410.9698543952065\n",
      "    - -1063.7924794735368\n",
      "    - -970.5481946328141\n",
      "    - -1161.8162095733708\n",
      "    - -1349.4573070132833\n",
      "    - -972.4101108699955\n",
      "    - -1366.115272344067\n",
      "    - -950.2009744528096\n",
      "    - -1338.573328131694\n",
      "    - -933.749297659643\n",
      "    - -1714.5803635783698\n",
      "    - -993.2437204317123\n",
      "    - -847.6091506608261\n",
      "    - -995.0413157172924\n",
      "    - -889.5313391037448\n",
      "    - -899.3382743403305\n",
      "    - -896.6088950306065\n",
      "    - -904.1124577253781\n",
      "    - -888.9047465779698\n",
      "    - -1594.5090293718795\n",
      "    - -1400.8044526964845\n",
      "    - -981.84232586847\n",
      "    - -1195.1076725155053\n",
      "    - -1165.1068014538373\n",
      "    - -974.0220361970739\n",
      "    - -888.3381593727971\n",
      "    - -1534.7020969688313\n",
      "    - -1570.4756305177455\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19239102998467358\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14837439259585408\n",
      "    mean_inference_ms: 0.9760969273769837\n",
      "    mean_raw_obs_processing_ms: 0.12129631640346367\n",
      "time_since_restore: 943.5511956214905\n",
      "time_this_iter_s: 13.640832901000977\n",
      "time_total_s: 943.5511956214905\n",
      "timers:\n",
      "  learn_throughput: 802.191\n",
      "  learn_time_ms: 4986.346\n",
      "  load_throughput: 16435360.502\n",
      "  load_time_ms: 0.243\n",
      "  training_iteration_time_ms: 10926.994\n",
      "  update_time_ms: 2.118\n",
      "timestamp: 1660564865\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 360000\n",
      "training_iteration: 90\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 364000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 364000\n",
      "  num_agent_steps_trained: 364000\n",
      "  num_env_steps_sampled: 364000\n",
      "  num_env_steps_trained: 364000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-01-16\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -758.9635219082306\n",
      "episode_reward_mean: -1147.588942570357\n",
      "episode_reward_min: -1714.5803635783698\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1820\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30409297347068787\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1512703895568848\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010976549237966537\n",
      "        model: {}\n",
      "        policy_loss: 0.008419170044362545\n",
      "        total_loss: 9.814874649047852\n",
      "        vf_explained_var: -0.01224015187472105\n",
      "        vf_loss: 9.803116798400879\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 364000\n",
      "  num_agent_steps_trained: 364000\n",
      "  num_env_steps_sampled: 364000\n",
      "  num_env_steps_trained: 364000\n",
      "iterations_since_restore: 91\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 364000\n",
      "num_agent_steps_trained: 364000\n",
      "num_env_steps_sampled: 364000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 364000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 62.9375\n",
      "  ram_util_percent: 59.50625\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1925423808597845\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14849149980216517\n",
      "  mean_inference_ms: 0.9769617875854604\n",
      "  mean_raw_obs_processing_ms: 0.12136721508077247\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -758.9635219082306\n",
      "  episode_reward_mean: -1147.588942570357\n",
      "  episode_reward_min: -1714.5803635783698\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -907.2148238191675\n",
      "    - -880.7979246031698\n",
      "    - -1103.207104206183\n",
      "    - -1300.9662486558361\n",
      "    - -829.676256519081\n",
      "    - -1180.3130440997882\n",
      "    - -983.2410252719413\n",
      "    - -890.6848408655075\n",
      "    - -973.593543532378\n",
      "    - -1024.1131853044972\n",
      "    - -1010.1085962710681\n",
      "    - -1151.4903965709993\n",
      "    - -1283.0054681849015\n",
      "    - -926.138899127666\n",
      "    - -758.9635219082306\n",
      "    - -903.7975434173607\n",
      "    - -971.5249153746006\n",
      "    - -982.5970409114152\n",
      "    - -963.8052280179895\n",
      "    - -1550.829624169229\n",
      "    - -880.2205004120614\n",
      "    - -886.1091920948105\n",
      "    - -1200.500595156374\n",
      "    - -887.6877159473175\n",
      "    - -1563.3799454432678\n",
      "    - -881.8713019666033\n",
      "    - -974.4857844527929\n",
      "    - -981.0798338517301\n",
      "    - -1436.3433107653582\n",
      "    - -1096.041955154261\n",
      "    - -1598.4032839535255\n",
      "    - -976.0909391986878\n",
      "    - -882.6715033610795\n",
      "    - -1003.5058497488783\n",
      "    - -1587.1913574522252\n",
      "    - -1599.0470974113198\n",
      "    - -1396.1141984000521\n",
      "    - -1175.9327944957745\n",
      "    - -1306.8723453536704\n",
      "    - -1363.6183123517874\n",
      "    - -1146.865178314985\n",
      "    - -1357.0889622372954\n",
      "    - -1000.2786742270557\n",
      "    - -1388.7541181979807\n",
      "    - -900.3363314557296\n",
      "    - -962.9717063965082\n",
      "    - -1100.849172911863\n",
      "    - -1195.7962245997478\n",
      "    - -1394.4430120164645\n",
      "    - -876.904228033334\n",
      "    - -1209.2930862119736\n",
      "    - -1545.5728739026551\n",
      "    - -1410.9698543952065\n",
      "    - -1063.7924794735368\n",
      "    - -970.5481946328141\n",
      "    - -1161.8162095733708\n",
      "    - -1349.4573070132833\n",
      "    - -972.4101108699955\n",
      "    - -1366.115272344067\n",
      "    - -950.2009744528096\n",
      "    - -1338.573328131694\n",
      "    - -933.749297659643\n",
      "    - -1714.5803635783698\n",
      "    - -993.2437204317123\n",
      "    - -847.6091506608261\n",
      "    - -995.0413157172924\n",
      "    - -889.5313391037448\n",
      "    - -899.3382743403305\n",
      "    - -896.6088950306065\n",
      "    - -904.1124577253781\n",
      "    - -888.9047465779698\n",
      "    - -1594.5090293718795\n",
      "    - -1400.8044526964845\n",
      "    - -981.84232586847\n",
      "    - -1195.1076725155053\n",
      "    - -1165.1068014538373\n",
      "    - -974.0220361970739\n",
      "    - -888.3381593727971\n",
      "    - -1534.7020969688313\n",
      "    - -1570.4756305177455\n",
      "    - -894.5575613646927\n",
      "    - -1640.609270349434\n",
      "    - -1189.1688674967634\n",
      "    - -895.0365624778362\n",
      "    - -1059.883688490796\n",
      "    - -891.0030909543219\n",
      "    - -893.9925356606442\n",
      "    - -1653.3086638879956\n",
      "    - -1612.5813445976862\n",
      "    - -947.586649657372\n",
      "    - -1042.7565123999534\n",
      "    - -889.5384839712053\n",
      "    - -1578.74794488972\n",
      "    - -1624.251808321668\n",
      "    - -1306.2780084323656\n",
      "    - -1016.8664977298341\n",
      "    - -968.1176660320776\n",
      "    - -1356.1976085364852\n",
      "    - -1518.002782693267\n",
      "    - -1596.5065961081207\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1925423808597845\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14849149980216517\n",
      "    mean_inference_ms: 0.9769617875854604\n",
      "    mean_raw_obs_processing_ms: 0.12136721508077247\n",
      "time_since_restore: 954.8108947277069\n",
      "time_this_iter_s: 11.25969910621643\n",
      "time_total_s: 954.8108947277069\n",
      "timers:\n",
      "  learn_throughput: 797.613\n",
      "  learn_time_ms: 5014.96\n",
      "  load_throughput: 13818644.263\n",
      "  load_time_ms: 0.289\n",
      "  training_iteration_time_ms: 11051.415\n",
      "  update_time_ms: 2.138\n",
      "timestamp: 1660564876\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 364000\n",
      "training_iteration: 91\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 368000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 368000\n",
      "  num_agent_steps_trained: 368000\n",
      "  num_env_steps_sampled: 368000\n",
      "  num_env_steps_trained: 368000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-01-31\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -770.8375153482173\n",
      "episode_reward_mean: -1155.742272529778\n",
      "episode_reward_min: -1714.5803635783698\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1840\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30409297347068787\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6848481297492981\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015710363164544106\n",
      "        model: {}\n",
      "        policy_loss: 0.007365034427493811\n",
      "        total_loss: 9.791253089904785\n",
      "        vf_explained_var: -0.01998208835721016\n",
      "        vf_loss: 9.7791109085083\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 368000\n",
      "  num_agent_steps_trained: 368000\n",
      "  num_env_steps_sampled: 368000\n",
      "  num_env_steps_trained: 368000\n",
      "iterations_since_restore: 92\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 368000\n",
      "num_agent_steps_trained: 368000\n",
      "num_env_steps_sampled: 368000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 368000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 81.32380952380952\n",
      "  ram_util_percent: 59.74761904761905\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19290887066442622\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14877444697151293\n",
      "  mean_inference_ms: 0.9790539577542173\n",
      "  mean_raw_obs_processing_ms: 0.12156728074744456\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -770.8375153482173\n",
      "  episode_reward_mean: -1155.742272529778\n",
      "  episode_reward_min: -1714.5803635783698\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -880.2205004120614\n",
      "    - -886.1091920948105\n",
      "    - -1200.500595156374\n",
      "    - -887.6877159473175\n",
      "    - -1563.3799454432678\n",
      "    - -881.8713019666033\n",
      "    - -974.4857844527929\n",
      "    - -981.0798338517301\n",
      "    - -1436.3433107653582\n",
      "    - -1096.041955154261\n",
      "    - -1598.4032839535255\n",
      "    - -976.0909391986878\n",
      "    - -882.6715033610795\n",
      "    - -1003.5058497488783\n",
      "    - -1587.1913574522252\n",
      "    - -1599.0470974113198\n",
      "    - -1396.1141984000521\n",
      "    - -1175.9327944957745\n",
      "    - -1306.8723453536704\n",
      "    - -1363.6183123517874\n",
      "    - -1146.865178314985\n",
      "    - -1357.0889622372954\n",
      "    - -1000.2786742270557\n",
      "    - -1388.7541181979807\n",
      "    - -900.3363314557296\n",
      "    - -962.9717063965082\n",
      "    - -1100.849172911863\n",
      "    - -1195.7962245997478\n",
      "    - -1394.4430120164645\n",
      "    - -876.904228033334\n",
      "    - -1209.2930862119736\n",
      "    - -1545.5728739026551\n",
      "    - -1410.9698543952065\n",
      "    - -1063.7924794735368\n",
      "    - -970.5481946328141\n",
      "    - -1161.8162095733708\n",
      "    - -1349.4573070132833\n",
      "    - -972.4101108699955\n",
      "    - -1366.115272344067\n",
      "    - -950.2009744528096\n",
      "    - -1338.573328131694\n",
      "    - -933.749297659643\n",
      "    - -1714.5803635783698\n",
      "    - -993.2437204317123\n",
      "    - -847.6091506608261\n",
      "    - -995.0413157172924\n",
      "    - -889.5313391037448\n",
      "    - -899.3382743403305\n",
      "    - -896.6088950306065\n",
      "    - -904.1124577253781\n",
      "    - -888.9047465779698\n",
      "    - -1594.5090293718795\n",
      "    - -1400.8044526964845\n",
      "    - -981.84232586847\n",
      "    - -1195.1076725155053\n",
      "    - -1165.1068014538373\n",
      "    - -974.0220361970739\n",
      "    - -888.3381593727971\n",
      "    - -1534.7020969688313\n",
      "    - -1570.4756305177455\n",
      "    - -894.5575613646927\n",
      "    - -1640.609270349434\n",
      "    - -1189.1688674967634\n",
      "    - -895.0365624778362\n",
      "    - -1059.883688490796\n",
      "    - -891.0030909543219\n",
      "    - -893.9925356606442\n",
      "    - -1653.3086638879956\n",
      "    - -1612.5813445976862\n",
      "    - -947.586649657372\n",
      "    - -1042.7565123999534\n",
      "    - -889.5384839712053\n",
      "    - -1578.74794488972\n",
      "    - -1624.251808321668\n",
      "    - -1306.2780084323656\n",
      "    - -1016.8664977298341\n",
      "    - -968.1176660320776\n",
      "    - -1356.1976085364852\n",
      "    - -1518.002782693267\n",
      "    - -1596.5065961081207\n",
      "    - -1465.4114162303076\n",
      "    - -1185.0028161420776\n",
      "    - -913.533017378915\n",
      "    - -1077.6247679720161\n",
      "    - -1102.974577403255\n",
      "    - -770.8375153482173\n",
      "    - -1343.1967224686596\n",
      "    - -904.5036243526765\n",
      "    - -883.9833856096111\n",
      "    - -895.4441544889614\n",
      "    - -1079.389824680436\n",
      "    - -1662.498491730786\n",
      "    - -1004.4639362808934\n",
      "    - -890.969036650151\n",
      "    - -972.4538830542899\n",
      "    - -882.6984369820484\n",
      "    - -890.5379525436927\n",
      "    - -882.8611318667041\n",
      "    - -888.8786396624079\n",
      "    - -1694.138895926993\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19290887066442622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14877444697151293\n",
      "    mean_inference_ms: 0.9790539577542173\n",
      "    mean_raw_obs_processing_ms: 0.12156728074744456\n",
      "time_since_restore: 969.4940338134766\n",
      "time_this_iter_s: 14.683139085769653\n",
      "time_total_s: 969.4940338134766\n",
      "timers:\n",
      "  learn_throughput: 780.609\n",
      "  learn_time_ms: 5124.204\n",
      "  load_throughput: 12878802.487\n",
      "  load_time_ms: 0.311\n",
      "  training_iteration_time_ms: 11487.087\n",
      "  update_time_ms: 2.174\n",
      "timestamp: 1660564891\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 368000\n",
      "training_iteration: 92\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 372000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 372000\n",
      "  num_agent_steps_trained: 372000\n",
      "  num_env_steps_sampled: 372000\n",
      "  num_env_steps_trained: 372000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-01-46\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -770.8375153482173\n",
      "episode_reward_mean: -1143.813677244821\n",
      "episode_reward_min: -1714.5803635783698\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1860\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30409297347068787\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.968798041343689\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019508054479956627\n",
      "        model: {}\n",
      "        policy_loss: 0.009716782718896866\n",
      "        total_loss: 9.829899787902832\n",
      "        vf_explained_var: -0.020592229440808296\n",
      "        vf_loss: 9.814249992370605\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 372000\n",
      "  num_agent_steps_trained: 372000\n",
      "  num_env_steps_sampled: 372000\n",
      "  num_env_steps_trained: 372000\n",
      "iterations_since_restore: 93\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 372000\n",
      "num_agent_steps_trained: 372000\n",
      "num_env_steps_sampled: 372000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 372000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 72.32857142857144\n",
      "  ram_util_percent: 60.46190476190476\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19328798821767368\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1490661279167691\n",
      "  mean_inference_ms: 0.9811559042606587\n",
      "  mean_raw_obs_processing_ms: 0.12177994530396631\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -770.8375153482173\n",
      "  episode_reward_mean: -1143.813677244821\n",
      "  episode_reward_min: -1714.5803635783698\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1146.865178314985\n",
      "    - -1357.0889622372954\n",
      "    - -1000.2786742270557\n",
      "    - -1388.7541181979807\n",
      "    - -900.3363314557296\n",
      "    - -962.9717063965082\n",
      "    - -1100.849172911863\n",
      "    - -1195.7962245997478\n",
      "    - -1394.4430120164645\n",
      "    - -876.904228033334\n",
      "    - -1209.2930862119736\n",
      "    - -1545.5728739026551\n",
      "    - -1410.9698543952065\n",
      "    - -1063.7924794735368\n",
      "    - -970.5481946328141\n",
      "    - -1161.8162095733708\n",
      "    - -1349.4573070132833\n",
      "    - -972.4101108699955\n",
      "    - -1366.115272344067\n",
      "    - -950.2009744528096\n",
      "    - -1338.573328131694\n",
      "    - -933.749297659643\n",
      "    - -1714.5803635783698\n",
      "    - -993.2437204317123\n",
      "    - -847.6091506608261\n",
      "    - -995.0413157172924\n",
      "    - -889.5313391037448\n",
      "    - -899.3382743403305\n",
      "    - -896.6088950306065\n",
      "    - -904.1124577253781\n",
      "    - -888.9047465779698\n",
      "    - -1594.5090293718795\n",
      "    - -1400.8044526964845\n",
      "    - -981.84232586847\n",
      "    - -1195.1076725155053\n",
      "    - -1165.1068014538373\n",
      "    - -974.0220361970739\n",
      "    - -888.3381593727971\n",
      "    - -1534.7020969688313\n",
      "    - -1570.4756305177455\n",
      "    - -894.5575613646927\n",
      "    - -1640.609270349434\n",
      "    - -1189.1688674967634\n",
      "    - -895.0365624778362\n",
      "    - -1059.883688490796\n",
      "    - -891.0030909543219\n",
      "    - -893.9925356606442\n",
      "    - -1653.3086638879956\n",
      "    - -1612.5813445976862\n",
      "    - -947.586649657372\n",
      "    - -1042.7565123999534\n",
      "    - -889.5384839712053\n",
      "    - -1578.74794488972\n",
      "    - -1624.251808321668\n",
      "    - -1306.2780084323656\n",
      "    - -1016.8664977298341\n",
      "    - -968.1176660320776\n",
      "    - -1356.1976085364852\n",
      "    - -1518.002782693267\n",
      "    - -1596.5065961081207\n",
      "    - -1465.4114162303076\n",
      "    - -1185.0028161420776\n",
      "    - -913.533017378915\n",
      "    - -1077.6247679720161\n",
      "    - -1102.974577403255\n",
      "    - -770.8375153482173\n",
      "    - -1343.1967224686596\n",
      "    - -904.5036243526765\n",
      "    - -883.9833856096111\n",
      "    - -895.4441544889614\n",
      "    - -1079.389824680436\n",
      "    - -1662.498491730786\n",
      "    - -1004.4639362808934\n",
      "    - -890.969036650151\n",
      "    - -972.4538830542899\n",
      "    - -882.6984369820484\n",
      "    - -890.5379525436927\n",
      "    - -882.8611318667041\n",
      "    - -888.8786396624079\n",
      "    - -1694.138895926993\n",
      "    - -1560.2655117913023\n",
      "    - -1071.197488328061\n",
      "    - -1558.6655820915344\n",
      "    - -887.8580670767633\n",
      "    - -911.9735178088548\n",
      "    - -891.4883752554432\n",
      "    - -978.0222291357157\n",
      "    - -1000.6711017599531\n",
      "    - -982.5628738706824\n",
      "    - -1115.818702826627\n",
      "    - -1267.8375167605084\n",
      "    - -1191.2262539790718\n",
      "    - -1211.4918436677751\n",
      "    - -1153.0038082012372\n",
      "    - -998.2219125275282\n",
      "    - -894.939336743039\n",
      "    - -893.4021046158807\n",
      "    - -901.0391295159286\n",
      "    - -1493.3680796690842\n",
      "    - -1521.254852850886\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19328798821767368\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1490661279167691\n",
      "    mean_inference_ms: 0.9811559042606587\n",
      "    mean_raw_obs_processing_ms: 0.12177994530396631\n",
      "time_since_restore: 984.5482110977173\n",
      "time_this_iter_s: 15.054177284240723\n",
      "time_total_s: 984.5482110977173\n",
      "timers:\n",
      "  learn_throughput: 729.087\n",
      "  learn_time_ms: 5486.313\n",
      "  load_throughput: 13096967.994\n",
      "  load_time_ms: 0.305\n",
      "  training_iteration_time_ms: 11848.503\n",
      "  update_time_ms: 2.192\n",
      "timestamp: 1660564906\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 372000\n",
      "training_iteration: 93\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 376000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 376000\n",
      "  num_agent_steps_trained: 376000\n",
      "  num_env_steps_sampled: 376000\n",
      "  num_env_steps_trained: 376000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-01-57\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -770.8375153482173\n",
      "episode_reward_mean: -1129.0955501957426\n",
      "episode_reward_min: -1725.4512779277143\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1880\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30409297347068787\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7702544927597046\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013021638616919518\n",
      "        model: {}\n",
      "        policy_loss: 0.01006059069186449\n",
      "        total_loss: 9.768631935119629\n",
      "        vf_explained_var: -0.015901226550340652\n",
      "        vf_loss: 9.75461196899414\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 376000\n",
      "  num_agent_steps_trained: 376000\n",
      "  num_env_steps_sampled: 376000\n",
      "  num_env_steps_trained: 376000\n",
      "iterations_since_restore: 94\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 376000\n",
      "num_agent_steps_trained: 376000\n",
      "num_env_steps_sampled: 376000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 376000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.10625\n",
      "  ram_util_percent: 61.0125\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1936597589934953\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1493434478621133\n",
      "  mean_inference_ms: 0.9831804064837695\n",
      "  mean_raw_obs_processing_ms: 0.12198942909879365\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -770.8375153482173\n",
      "  episode_reward_mean: -1129.0955501957426\n",
      "  episode_reward_min: -1725.4512779277143\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1338.573328131694\n",
      "    - -933.749297659643\n",
      "    - -1714.5803635783698\n",
      "    - -993.2437204317123\n",
      "    - -847.6091506608261\n",
      "    - -995.0413157172924\n",
      "    - -889.5313391037448\n",
      "    - -899.3382743403305\n",
      "    - -896.6088950306065\n",
      "    - -904.1124577253781\n",
      "    - -888.9047465779698\n",
      "    - -1594.5090293718795\n",
      "    - -1400.8044526964845\n",
      "    - -981.84232586847\n",
      "    - -1195.1076725155053\n",
      "    - -1165.1068014538373\n",
      "    - -974.0220361970739\n",
      "    - -888.3381593727971\n",
      "    - -1534.7020969688313\n",
      "    - -1570.4756305177455\n",
      "    - -894.5575613646927\n",
      "    - -1640.609270349434\n",
      "    - -1189.1688674967634\n",
      "    - -895.0365624778362\n",
      "    - -1059.883688490796\n",
      "    - -891.0030909543219\n",
      "    - -893.9925356606442\n",
      "    - -1653.3086638879956\n",
      "    - -1612.5813445976862\n",
      "    - -947.586649657372\n",
      "    - -1042.7565123999534\n",
      "    - -889.5384839712053\n",
      "    - -1578.74794488972\n",
      "    - -1624.251808321668\n",
      "    - -1306.2780084323656\n",
      "    - -1016.8664977298341\n",
      "    - -968.1176660320776\n",
      "    - -1356.1976085364852\n",
      "    - -1518.002782693267\n",
      "    - -1596.5065961081207\n",
      "    - -1465.4114162303076\n",
      "    - -1185.0028161420776\n",
      "    - -913.533017378915\n",
      "    - -1077.6247679720161\n",
      "    - -1102.974577403255\n",
      "    - -770.8375153482173\n",
      "    - -1343.1967224686596\n",
      "    - -904.5036243526765\n",
      "    - -883.9833856096111\n",
      "    - -895.4441544889614\n",
      "    - -1079.389824680436\n",
      "    - -1662.498491730786\n",
      "    - -1004.4639362808934\n",
      "    - -890.969036650151\n",
      "    - -972.4538830542899\n",
      "    - -882.6984369820484\n",
      "    - -890.5379525436927\n",
      "    - -882.8611318667041\n",
      "    - -888.8786396624079\n",
      "    - -1694.138895926993\n",
      "    - -1560.2655117913023\n",
      "    - -1071.197488328061\n",
      "    - -1558.6655820915344\n",
      "    - -887.8580670767633\n",
      "    - -911.9735178088548\n",
      "    - -891.4883752554432\n",
      "    - -978.0222291357157\n",
      "    - -1000.6711017599531\n",
      "    - -982.5628738706824\n",
      "    - -1115.818702826627\n",
      "    - -1267.8375167605084\n",
      "    - -1191.2262539790718\n",
      "    - -1211.4918436677751\n",
      "    - -1153.0038082012372\n",
      "    - -998.2219125275282\n",
      "    - -894.939336743039\n",
      "    - -893.4021046158807\n",
      "    - -901.0391295159286\n",
      "    - -1493.3680796690842\n",
      "    - -1521.254852850886\n",
      "    - -973.4787296364138\n",
      "    - -1335.073507290561\n",
      "    - -1390.9005722670208\n",
      "    - -914.1656657196421\n",
      "    - -866.8275303557024\n",
      "    - -1076.2120384471364\n",
      "    - -1006.3602157343887\n",
      "    - -893.9662987170954\n",
      "    - -1220.9673511729127\n",
      "    - -985.5419616005233\n",
      "    - -1725.4512779277143\n",
      "    - -971.7516637376553\n",
      "    - -1008.4013095955463\n",
      "    - -1000.328276465357\n",
      "    - -884.7961953446668\n",
      "    - -893.1418456646037\n",
      "    - -1162.4071773001438\n",
      "    - -1243.606038782751\n",
      "    - -1016.3539289595069\n",
      "    - -1282.919681633522\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1936597589934953\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1493434478621133\n",
      "    mean_inference_ms: 0.9831804064837695\n",
      "    mean_raw_obs_processing_ms: 0.12198942909879365\n",
      "time_since_restore: 995.8411519527435\n",
      "time_this_iter_s: 11.292940855026245\n",
      "time_total_s: 995.8411519527435\n",
      "timers:\n",
      "  learn_throughput: 724.73\n",
      "  learn_time_ms: 5519.294\n",
      "  load_throughput: 13017703.29\n",
      "  load_time_ms: 0.307\n",
      "  training_iteration_time_ms: 11930.932\n",
      "  update_time_ms: 2.197\n",
      "timestamp: 1660564917\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 376000\n",
      "training_iteration: 94\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 380000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 380000\n",
      "  num_agent_steps_trained: 380000\n",
      "  num_env_steps_sampled: 380000\n",
      "  num_env_steps_trained: 380000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-02-08\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -770.8375153482173\n",
      "episode_reward_mean: -1126.9146324551414\n",
      "episode_reward_min: -1725.4512779277143\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1900\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30409297347068787\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1433048248291016\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.026412233710289\n",
      "        model: {}\n",
      "        policy_loss: 0.001568724517710507\n",
      "        total_loss: 9.817826271057129\n",
      "        vf_explained_var: -0.016336970031261444\n",
      "        vf_loss: 9.808226585388184\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 380000\n",
      "  num_agent_steps_trained: 380000\n",
      "  num_env_steps_sampled: 380000\n",
      "  num_env_steps_trained: 380000\n",
      "iterations_since_restore: 95\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 380000\n",
      "num_agent_steps_trained: 380000\n",
      "num_env_steps_sampled: 380000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 380000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 63.818749999999994\n",
      "  ram_util_percent: 60.25\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19401882142320112\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14961649704899638\n",
      "  mean_inference_ms: 0.9851866696033111\n",
      "  mean_raw_obs_processing_ms: 0.12219666368636496\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -770.8375153482173\n",
      "  episode_reward_mean: -1126.9146324551414\n",
      "  episode_reward_min: -1725.4512779277143\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -894.5575613646927\n",
      "    - -1640.609270349434\n",
      "    - -1189.1688674967634\n",
      "    - -895.0365624778362\n",
      "    - -1059.883688490796\n",
      "    - -891.0030909543219\n",
      "    - -893.9925356606442\n",
      "    - -1653.3086638879956\n",
      "    - -1612.5813445976862\n",
      "    - -947.586649657372\n",
      "    - -1042.7565123999534\n",
      "    - -889.5384839712053\n",
      "    - -1578.74794488972\n",
      "    - -1624.251808321668\n",
      "    - -1306.2780084323656\n",
      "    - -1016.8664977298341\n",
      "    - -968.1176660320776\n",
      "    - -1356.1976085364852\n",
      "    - -1518.002782693267\n",
      "    - -1596.5065961081207\n",
      "    - -1465.4114162303076\n",
      "    - -1185.0028161420776\n",
      "    - -913.533017378915\n",
      "    - -1077.6247679720161\n",
      "    - -1102.974577403255\n",
      "    - -770.8375153482173\n",
      "    - -1343.1967224686596\n",
      "    - -904.5036243526765\n",
      "    - -883.9833856096111\n",
      "    - -895.4441544889614\n",
      "    - -1079.389824680436\n",
      "    - -1662.498491730786\n",
      "    - -1004.4639362808934\n",
      "    - -890.969036650151\n",
      "    - -972.4538830542899\n",
      "    - -882.6984369820484\n",
      "    - -890.5379525436927\n",
      "    - -882.8611318667041\n",
      "    - -888.8786396624079\n",
      "    - -1694.138895926993\n",
      "    - -1560.2655117913023\n",
      "    - -1071.197488328061\n",
      "    - -1558.6655820915344\n",
      "    - -887.8580670767633\n",
      "    - -911.9735178088548\n",
      "    - -891.4883752554432\n",
      "    - -978.0222291357157\n",
      "    - -1000.6711017599531\n",
      "    - -982.5628738706824\n",
      "    - -1115.818702826627\n",
      "    - -1267.8375167605084\n",
      "    - -1191.2262539790718\n",
      "    - -1211.4918436677751\n",
      "    - -1153.0038082012372\n",
      "    - -998.2219125275282\n",
      "    - -894.939336743039\n",
      "    - -893.4021046158807\n",
      "    - -901.0391295159286\n",
      "    - -1493.3680796690842\n",
      "    - -1521.254852850886\n",
      "    - -973.4787296364138\n",
      "    - -1335.073507290561\n",
      "    - -1390.9005722670208\n",
      "    - -914.1656657196421\n",
      "    - -866.8275303557024\n",
      "    - -1076.2120384471364\n",
      "    - -1006.3602157343887\n",
      "    - -893.9662987170954\n",
      "    - -1220.9673511729127\n",
      "    - -985.5419616005233\n",
      "    - -1725.4512779277143\n",
      "    - -971.7516637376553\n",
      "    - -1008.4013095955463\n",
      "    - -1000.328276465357\n",
      "    - -884.7961953446668\n",
      "    - -893.1418456646037\n",
      "    - -1162.4071773001438\n",
      "    - -1243.606038782751\n",
      "    - -1016.3539289595069\n",
      "    - -1282.919681633522\n",
      "    - -884.2070008678758\n",
      "    - -1441.5043090105228\n",
      "    - -1392.2159561391143\n",
      "    - -896.2683851521327\n",
      "    - -882.6019504374544\n",
      "    - -891.1714620113711\n",
      "    - -1166.2948458956728\n",
      "    - -1528.7682890891774\n",
      "    - -1291.710290442621\n",
      "    - -891.3083542929927\n",
      "    - -1149.4310933968009\n",
      "    - -1645.7477588909912\n",
      "    - -890.4561370643327\n",
      "    - -894.0365181334805\n",
      "    - -883.888935788521\n",
      "    - -983.3869785253232\n",
      "    - -1448.2503592677883\n",
      "    - -1266.2969846544722\n",
      "    - -976.8556180212225\n",
      "    - -983.7080927782016\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19401882142320112\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14961649704899638\n",
      "    mean_inference_ms: 0.9851866696033111\n",
      "    mean_raw_obs_processing_ms: 0.12219666368636496\n",
      "time_since_restore: 1006.8856678009033\n",
      "time_this_iter_s: 11.04451584815979\n",
      "time_total_s: 1006.8856678009033\n",
      "timers:\n",
      "  learn_throughput: 721.773\n",
      "  learn_time_ms: 5541.906\n",
      "  load_throughput: 12923444.77\n",
      "  load_time_ms: 0.31\n",
      "  training_iteration_time_ms: 12057.447\n",
      "  update_time_ms: 2.208\n",
      "timestamp: 1660564928\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 380000\n",
      "training_iteration: 95\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 384000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 384000\n",
      "  num_agent_steps_trained: 384000\n",
      "  num_env_steps_sampled: 384000\n",
      "  num_env_steps_trained: 384000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-02-18\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -770.8375153482173\n",
      "episode_reward_mean: -1095.9758307225588\n",
      "episode_reward_min: -1725.4512779277143\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1920\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.456139475107193\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5824272632598877\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020503710955381393\n",
      "        model: {}\n",
      "        policy_loss: 0.0008807601407170296\n",
      "        total_loss: 9.853414535522461\n",
      "        vf_explained_var: -0.013526408933103085\n",
      "        vf_loss: 9.843181610107422\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 384000\n",
      "  num_agent_steps_trained: 384000\n",
      "  num_env_steps_sampled: 384000\n",
      "  num_env_steps_trained: 384000\n",
      "iterations_since_restore: 96\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 384000\n",
      "num_agent_steps_trained: 384000\n",
      "num_env_steps_sampled: 384000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 384000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.20666666666667\n",
      "  ram_util_percent: 60.0\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19430634529676868\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14983828952706468\n",
      "  mean_inference_ms: 0.9868263456672817\n",
      "  mean_raw_obs_processing_ms: 0.12236612284244432\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -770.8375153482173\n",
      "  episode_reward_mean: -1095.9758307225588\n",
      "  episode_reward_min: -1725.4512779277143\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1465.4114162303076\n",
      "    - -1185.0028161420776\n",
      "    - -913.533017378915\n",
      "    - -1077.6247679720161\n",
      "    - -1102.974577403255\n",
      "    - -770.8375153482173\n",
      "    - -1343.1967224686596\n",
      "    - -904.5036243526765\n",
      "    - -883.9833856096111\n",
      "    - -895.4441544889614\n",
      "    - -1079.389824680436\n",
      "    - -1662.498491730786\n",
      "    - -1004.4639362808934\n",
      "    - -890.969036650151\n",
      "    - -972.4538830542899\n",
      "    - -882.6984369820484\n",
      "    - -890.5379525436927\n",
      "    - -882.8611318667041\n",
      "    - -888.8786396624079\n",
      "    - -1694.138895926993\n",
      "    - -1560.2655117913023\n",
      "    - -1071.197488328061\n",
      "    - -1558.6655820915344\n",
      "    - -887.8580670767633\n",
      "    - -911.9735178088548\n",
      "    - -891.4883752554432\n",
      "    - -978.0222291357157\n",
      "    - -1000.6711017599531\n",
      "    - -982.5628738706824\n",
      "    - -1115.818702826627\n",
      "    - -1267.8375167605084\n",
      "    - -1191.2262539790718\n",
      "    - -1211.4918436677751\n",
      "    - -1153.0038082012372\n",
      "    - -998.2219125275282\n",
      "    - -894.939336743039\n",
      "    - -893.4021046158807\n",
      "    - -901.0391295159286\n",
      "    - -1493.3680796690842\n",
      "    - -1521.254852850886\n",
      "    - -973.4787296364138\n",
      "    - -1335.073507290561\n",
      "    - -1390.9005722670208\n",
      "    - -914.1656657196421\n",
      "    - -866.8275303557024\n",
      "    - -1076.2120384471364\n",
      "    - -1006.3602157343887\n",
      "    - -893.9662987170954\n",
      "    - -1220.9673511729127\n",
      "    - -985.5419616005233\n",
      "    - -1725.4512779277143\n",
      "    - -971.7516637376553\n",
      "    - -1008.4013095955463\n",
      "    - -1000.328276465357\n",
      "    - -884.7961953446668\n",
      "    - -893.1418456646037\n",
      "    - -1162.4071773001438\n",
      "    - -1243.606038782751\n",
      "    - -1016.3539289595069\n",
      "    - -1282.919681633522\n",
      "    - -884.2070008678758\n",
      "    - -1441.5043090105228\n",
      "    - -1392.2159561391143\n",
      "    - -896.2683851521327\n",
      "    - -882.6019504374544\n",
      "    - -891.1714620113711\n",
      "    - -1166.2948458956728\n",
      "    - -1528.7682890891774\n",
      "    - -1291.710290442621\n",
      "    - -891.3083542929927\n",
      "    - -1149.4310933968009\n",
      "    - -1645.7477588909912\n",
      "    - -890.4561370643327\n",
      "    - -894.0365181334805\n",
      "    - -883.888935788521\n",
      "    - -983.3869785253232\n",
      "    - -1448.2503592677883\n",
      "    - -1266.2969846544722\n",
      "    - -976.8556180212225\n",
      "    - -983.7080927782016\n",
      "    - -990.8567426710517\n",
      "    - -1534.2081281245903\n",
      "    - -1090.5570478994669\n",
      "    - -903.7276457499495\n",
      "    - -1509.8550236819112\n",
      "    - -1135.8744621214187\n",
      "    - -916.2758070341552\n",
      "    - -981.4673113637484\n",
      "    - -1167.4279804192697\n",
      "    - -887.6106709242704\n",
      "    - -984.5363953569738\n",
      "    - -982.0581833363628\n",
      "    - -910.1313890444167\n",
      "    - -913.5595241903576\n",
      "    - -1003.1922530526542\n",
      "    - -903.3162007310815\n",
      "    - -1114.9247803884982\n",
      "    - -1558.022210784814\n",
      "    - -1089.7063084399783\n",
      "    - -903.8039054789984\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19430634529676868\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14983828952706468\n",
      "    mean_inference_ms: 0.9868263456672817\n",
      "    mean_raw_obs_processing_ms: 0.12236612284244432\n",
      "time_since_restore: 1016.8950970172882\n",
      "time_this_iter_s: 10.009429216384888\n",
      "time_total_s: 1016.8950970172882\n",
      "timers:\n",
      "  learn_throughput: 726.758\n",
      "  learn_time_ms: 5503.894\n",
      "  load_throughput: 12915485.758\n",
      "  load_time_ms: 0.31\n",
      "  training_iteration_time_ms: 12001.233\n",
      "  update_time_ms: 2.22\n",
      "timestamp: 1660564938\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 384000\n",
      "training_iteration: 96\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 388000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 388000\n",
      "  num_agent_steps_trained: 388000\n",
      "  num_env_steps_sampled: 388000\n",
      "  num_env_steps_trained: 388000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-02-28\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -866.8275303557024\n",
      "episode_reward_mean: -1110.5569949602127\n",
      "episode_reward_min: -1725.4512779277143\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1940\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6842092275619507\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.589308738708496\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012723664753139019\n",
      "        model: {}\n",
      "        policy_loss: 0.00827705953270197\n",
      "        total_loss: 9.830615997314453\n",
      "        vf_explained_var: -0.02316582389175892\n",
      "        vf_loss: 9.813633918762207\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 388000\n",
      "  num_agent_steps_trained: 388000\n",
      "  num_env_steps_sampled: 388000\n",
      "  num_env_steps_trained: 388000\n",
      "iterations_since_restore: 97\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 388000\n",
      "num_agent_steps_trained: 388000\n",
      "num_env_steps_sampled: 388000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 388000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.01538461538462\n",
      "  ram_util_percent: 60.01538461538461\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19436872672836109\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1498897495311042\n",
      "  mean_inference_ms: 0.9871869410268872\n",
      "  mean_raw_obs_processing_ms: 0.12240381085178029\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -866.8275303557024\n",
      "  episode_reward_mean: -1110.5569949602127\n",
      "  episode_reward_min: -1725.4512779277143\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1560.2655117913023\n",
      "    - -1071.197488328061\n",
      "    - -1558.6655820915344\n",
      "    - -887.8580670767633\n",
      "    - -911.9735178088548\n",
      "    - -891.4883752554432\n",
      "    - -978.0222291357157\n",
      "    - -1000.6711017599531\n",
      "    - -982.5628738706824\n",
      "    - -1115.818702826627\n",
      "    - -1267.8375167605084\n",
      "    - -1191.2262539790718\n",
      "    - -1211.4918436677751\n",
      "    - -1153.0038082012372\n",
      "    - -998.2219125275282\n",
      "    - -894.939336743039\n",
      "    - -893.4021046158807\n",
      "    - -901.0391295159286\n",
      "    - -1493.3680796690842\n",
      "    - -1521.254852850886\n",
      "    - -973.4787296364138\n",
      "    - -1335.073507290561\n",
      "    - -1390.9005722670208\n",
      "    - -914.1656657196421\n",
      "    - -866.8275303557024\n",
      "    - -1076.2120384471364\n",
      "    - -1006.3602157343887\n",
      "    - -893.9662987170954\n",
      "    - -1220.9673511729127\n",
      "    - -985.5419616005233\n",
      "    - -1725.4512779277143\n",
      "    - -971.7516637376553\n",
      "    - -1008.4013095955463\n",
      "    - -1000.328276465357\n",
      "    - -884.7961953446668\n",
      "    - -893.1418456646037\n",
      "    - -1162.4071773001438\n",
      "    - -1243.606038782751\n",
      "    - -1016.3539289595069\n",
      "    - -1282.919681633522\n",
      "    - -884.2070008678758\n",
      "    - -1441.5043090105228\n",
      "    - -1392.2159561391143\n",
      "    - -896.2683851521327\n",
      "    - -882.6019504374544\n",
      "    - -891.1714620113711\n",
      "    - -1166.2948458956728\n",
      "    - -1528.7682890891774\n",
      "    - -1291.710290442621\n",
      "    - -891.3083542929927\n",
      "    - -1149.4310933968009\n",
      "    - -1645.7477588909912\n",
      "    - -890.4561370643327\n",
      "    - -894.0365181334805\n",
      "    - -883.888935788521\n",
      "    - -983.3869785253232\n",
      "    - -1448.2503592677883\n",
      "    - -1266.2969846544722\n",
      "    - -976.8556180212225\n",
      "    - -983.7080927782016\n",
      "    - -990.8567426710517\n",
      "    - -1534.2081281245903\n",
      "    - -1090.5570478994669\n",
      "    - -903.7276457499495\n",
      "    - -1509.8550236819112\n",
      "    - -1135.8744621214187\n",
      "    - -916.2758070341552\n",
      "    - -981.4673113637484\n",
      "    - -1167.4279804192697\n",
      "    - -887.6106709242704\n",
      "    - -984.5363953569738\n",
      "    - -982.0581833363628\n",
      "    - -910.1313890444167\n",
      "    - -913.5595241903576\n",
      "    - -1003.1922530526542\n",
      "    - -903.3162007310815\n",
      "    - -1114.9247803884982\n",
      "    - -1558.022210784814\n",
      "    - -1089.7063084399783\n",
      "    - -903.8039054789984\n",
      "    - -980.0510984681674\n",
      "    - -980.2250175641725\n",
      "    - -966.0744845002997\n",
      "    - -889.9531423408133\n",
      "    - -979.3585383712483\n",
      "    - -1189.0307488669177\n",
      "    - -1708.2264714291616\n",
      "    - -1579.9415264514935\n",
      "    - -1597.0353861037784\n",
      "    - -885.2762178965107\n",
      "    - -987.0743679542172\n",
      "    - -882.566999485488\n",
      "    - -1010.556918169529\n",
      "    - -1177.3922593414406\n",
      "    - -898.0452807172393\n",
      "    - -894.447789817357\n",
      "    - -890.7980019999437\n",
      "    - -1513.8506308796016\n",
      "    - -1576.8449199335546\n",
      "    - -1262.7688502475685\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19436872672836109\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1498897495311042\n",
      "    mean_inference_ms: 0.9871869410268872\n",
      "    mean_raw_obs_processing_ms: 0.12240381085178029\n",
      "time_since_restore: 1026.619332075119\n",
      "time_this_iter_s: 9.72423505783081\n",
      "time_total_s: 1026.619332075119\n",
      "timers:\n",
      "  learn_throughput: 732.183\n",
      "  learn_time_ms: 5463.115\n",
      "  load_throughput: 12680232.787\n",
      "  load_time_ms: 0.315\n",
      "  training_iteration_time_ms: 11952.356\n",
      "  update_time_ms: 2.202\n",
      "timestamp: 1660564948\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 388000\n",
      "training_iteration: 97\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 392000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 392000\n",
      "  num_agent_steps_trained: 392000\n",
      "  num_env_steps_sampled: 392000\n",
      "  num_env_steps_trained: 392000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-02-38\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -866.8275303557024\n",
      "episode_reward_mean: -1116.680833249895\n",
      "episode_reward_min: -1748.3689209084\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1960\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6842092275619507\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7417948246002197\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007450660690665245\n",
      "        model: {}\n",
      "        policy_loss: 0.013705038465559483\n",
      "        total_loss: 9.900286674499512\n",
      "        vf_explained_var: -0.029224084690213203\n",
      "        vf_loss: 9.88148307800293\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 392000\n",
      "  num_agent_steps_trained: 392000\n",
      "  num_env_steps_sampled: 392000\n",
      "  num_env_steps_trained: 392000\n",
      "iterations_since_restore: 98\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 392000\n",
      "num_agent_steps_trained: 392000\n",
      "num_env_steps_sampled: 392000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 392000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.819999999999986\n",
      "  ram_util_percent: 60.013333333333335\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19437758325848178\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14989856303755225\n",
      "  mean_inference_ms: 0.9872860914859777\n",
      "  mean_raw_obs_processing_ms: 0.12240808046854167\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -866.8275303557024\n",
      "  episode_reward_mean: -1116.680833249895\n",
      "  episode_reward_min: -1748.3689209084\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -973.4787296364138\n",
      "    - -1335.073507290561\n",
      "    - -1390.9005722670208\n",
      "    - -914.1656657196421\n",
      "    - -866.8275303557024\n",
      "    - -1076.2120384471364\n",
      "    - -1006.3602157343887\n",
      "    - -893.9662987170954\n",
      "    - -1220.9673511729127\n",
      "    - -985.5419616005233\n",
      "    - -1725.4512779277143\n",
      "    - -971.7516637376553\n",
      "    - -1008.4013095955463\n",
      "    - -1000.328276465357\n",
      "    - -884.7961953446668\n",
      "    - -893.1418456646037\n",
      "    - -1162.4071773001438\n",
      "    - -1243.606038782751\n",
      "    - -1016.3539289595069\n",
      "    - -1282.919681633522\n",
      "    - -884.2070008678758\n",
      "    - -1441.5043090105228\n",
      "    - -1392.2159561391143\n",
      "    - -896.2683851521327\n",
      "    - -882.6019504374544\n",
      "    - -891.1714620113711\n",
      "    - -1166.2948458956728\n",
      "    - -1528.7682890891774\n",
      "    - -1291.710290442621\n",
      "    - -891.3083542929927\n",
      "    - -1149.4310933968009\n",
      "    - -1645.7477588909912\n",
      "    - -890.4561370643327\n",
      "    - -894.0365181334805\n",
      "    - -883.888935788521\n",
      "    - -983.3869785253232\n",
      "    - -1448.2503592677883\n",
      "    - -1266.2969846544722\n",
      "    - -976.8556180212225\n",
      "    - -983.7080927782016\n",
      "    - -990.8567426710517\n",
      "    - -1534.2081281245903\n",
      "    - -1090.5570478994669\n",
      "    - -903.7276457499495\n",
      "    - -1509.8550236819112\n",
      "    - -1135.8744621214187\n",
      "    - -916.2758070341552\n",
      "    - -981.4673113637484\n",
      "    - -1167.4279804192697\n",
      "    - -887.6106709242704\n",
      "    - -984.5363953569738\n",
      "    - -982.0581833363628\n",
      "    - -910.1313890444167\n",
      "    - -913.5595241903576\n",
      "    - -1003.1922530526542\n",
      "    - -903.3162007310815\n",
      "    - -1114.9247803884982\n",
      "    - -1558.022210784814\n",
      "    - -1089.7063084399783\n",
      "    - -903.8039054789984\n",
      "    - -980.0510984681674\n",
      "    - -980.2250175641725\n",
      "    - -966.0744845002997\n",
      "    - -889.9531423408133\n",
      "    - -979.3585383712483\n",
      "    - -1189.0307488669177\n",
      "    - -1708.2264714291616\n",
      "    - -1579.9415264514935\n",
      "    - -1597.0353861037784\n",
      "    - -885.2762178965107\n",
      "    - -987.0743679542172\n",
      "    - -882.566999485488\n",
      "    - -1010.556918169529\n",
      "    - -1177.3922593414406\n",
      "    - -898.0452807172393\n",
      "    - -894.447789817357\n",
      "    - -890.7980019999437\n",
      "    - -1513.8506308796016\n",
      "    - -1576.8449199335546\n",
      "    - -1262.7688502475685\n",
      "    - -905.9336076499162\n",
      "    - -984.3174632462775\n",
      "    - -880.9122069113342\n",
      "    - -896.5464250026383\n",
      "    - -915.9945903600141\n",
      "    - -882.4025854325118\n",
      "    - -1748.3689209084\n",
      "    - -885.5755562495534\n",
      "    - -1016.3325769140794\n",
      "    - -975.3243661428637\n",
      "    - -1317.2807268769027\n",
      "    - -1062.8766076783509\n",
      "    - -1312.401114011994\n",
      "    - -1277.4582928686204\n",
      "    - -1512.291503718301\n",
      "    - -1230.5035545426206\n",
      "    - -876.4438065101048\n",
      "    - -1668.6465248626923\n",
      "    - -1534.286627616815\n",
      "    - -1212.7950599401124\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19437758325848178\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14989856303755225\n",
      "    mean_inference_ms: 0.9872860914859777\n",
      "    mean_raw_obs_processing_ms: 0.12240808046854167\n",
      "time_since_restore: 1036.7022972106934\n",
      "time_this_iter_s: 10.08296513557434\n",
      "time_total_s: 1036.7022972106934\n",
      "timers:\n",
      "  learn_throughput: 730.816\n",
      "  learn_time_ms: 5473.334\n",
      "  load_throughput: 12606865.044\n",
      "  load_time_ms: 0.317\n",
      "  training_iteration_time_ms: 11903.159\n",
      "  update_time_ms: 2.23\n",
      "timestamp: 1660564958\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 392000\n",
      "training_iteration: 98\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 396000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 396000\n",
      "  num_agent_steps_trained: 396000\n",
      "  num_env_steps_sampled: 396000\n",
      "  num_env_steps_trained: 396000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-02-48\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -876.4438065101048\n",
      "episode_reward_mean: -1150.8928761828201\n",
      "episode_reward_min: -1771.0757179395987\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 1980\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6842092275619507\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.713184118270874\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009634608402848244\n",
      "        model: {}\n",
      "        policy_loss: 0.010567031800746918\n",
      "        total_loss: 9.891409873962402\n",
      "        vf_explained_var: -0.01573994569480419\n",
      "        vf_loss: 9.874250411987305\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 396000\n",
      "  num_agent_steps_trained: 396000\n",
      "  num_env_steps_sampled: 396000\n",
      "  num_env_steps_trained: 396000\n",
      "iterations_since_restore: 99\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 396000\n",
      "num_agent_steps_trained: 396000\n",
      "num_env_steps_sampled: 396000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 396000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.971428571428575\n",
      "  ram_util_percent: 60.021428571428565\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1943270180298976\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1498746842746545\n",
      "  mean_inference_ms: 0.987125201603451\n",
      "  mean_raw_obs_processing_ms: 0.1223808227086457\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -876.4438065101048\n",
      "  episode_reward_mean: -1150.8928761828201\n",
      "  episode_reward_min: -1771.0757179395987\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -884.2070008678758\n",
      "    - -1441.5043090105228\n",
      "    - -1392.2159561391143\n",
      "    - -896.2683851521327\n",
      "    - -882.6019504374544\n",
      "    - -891.1714620113711\n",
      "    - -1166.2948458956728\n",
      "    - -1528.7682890891774\n",
      "    - -1291.710290442621\n",
      "    - -891.3083542929927\n",
      "    - -1149.4310933968009\n",
      "    - -1645.7477588909912\n",
      "    - -890.4561370643327\n",
      "    - -894.0365181334805\n",
      "    - -883.888935788521\n",
      "    - -983.3869785253232\n",
      "    - -1448.2503592677883\n",
      "    - -1266.2969846544722\n",
      "    - -976.8556180212225\n",
      "    - -983.7080927782016\n",
      "    - -990.8567426710517\n",
      "    - -1534.2081281245903\n",
      "    - -1090.5570478994669\n",
      "    - -903.7276457499495\n",
      "    - -1509.8550236819112\n",
      "    - -1135.8744621214187\n",
      "    - -916.2758070341552\n",
      "    - -981.4673113637484\n",
      "    - -1167.4279804192697\n",
      "    - -887.6106709242704\n",
      "    - -984.5363953569738\n",
      "    - -982.0581833363628\n",
      "    - -910.1313890444167\n",
      "    - -913.5595241903576\n",
      "    - -1003.1922530526542\n",
      "    - -903.3162007310815\n",
      "    - -1114.9247803884982\n",
      "    - -1558.022210784814\n",
      "    - -1089.7063084399783\n",
      "    - -903.8039054789984\n",
      "    - -980.0510984681674\n",
      "    - -980.2250175641725\n",
      "    - -966.0744845002997\n",
      "    - -889.9531423408133\n",
      "    - -979.3585383712483\n",
      "    - -1189.0307488669177\n",
      "    - -1708.2264714291616\n",
      "    - -1579.9415264514935\n",
      "    - -1597.0353861037784\n",
      "    - -885.2762178965107\n",
      "    - -987.0743679542172\n",
      "    - -882.566999485488\n",
      "    - -1010.556918169529\n",
      "    - -1177.3922593414406\n",
      "    - -898.0452807172393\n",
      "    - -894.447789817357\n",
      "    - -890.7980019999437\n",
      "    - -1513.8506308796016\n",
      "    - -1576.8449199335546\n",
      "    - -1262.7688502475685\n",
      "    - -905.9336076499162\n",
      "    - -984.3174632462775\n",
      "    - -880.9122069113342\n",
      "    - -896.5464250026383\n",
      "    - -915.9945903600141\n",
      "    - -882.4025854325118\n",
      "    - -1748.3689209084\n",
      "    - -885.5755562495534\n",
      "    - -1016.3325769140794\n",
      "    - -975.3243661428637\n",
      "    - -1317.2807268769027\n",
      "    - -1062.8766076783509\n",
      "    - -1312.401114011994\n",
      "    - -1277.4582928686204\n",
      "    - -1512.291503718301\n",
      "    - -1230.5035545426206\n",
      "    - -876.4438065101048\n",
      "    - -1668.6465248626923\n",
      "    - -1534.286627616815\n",
      "    - -1212.7950599401124\n",
      "    - -1174.5856292686651\n",
      "    - -1093.245630586604\n",
      "    - -1075.4563121659676\n",
      "    - -1181.0117796286006\n",
      "    - -881.3297084989453\n",
      "    - -1272.6770051042056\n",
      "    - -886.7540886355748\n",
      "    - -894.7916841819359\n",
      "    - -1565.798533099399\n",
      "    - -900.3637546421902\n",
      "    - -1713.5776686254471\n",
      "    - -1771.0757179395987\n",
      "    - -909.5875707946304\n",
      "    - -1493.4889932227595\n",
      "    - -1581.665780525652\n",
      "    - -977.7647244966485\n",
      "    - -1738.2362893097122\n",
      "    - -1536.5747319999234\n",
      "    - -891.3754361463488\n",
      "    - -1734.494520772572\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1943270180298976\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1498746842746545\n",
      "    mean_inference_ms: 0.987125201603451\n",
      "    mean_raw_obs_processing_ms: 0.1223808227086457\n",
      "time_since_restore: 1046.393175125122\n",
      "time_this_iter_s: 9.690877914428711\n",
      "time_total_s: 1046.393175125122\n",
      "timers:\n",
      "  learn_throughput: 755.285\n",
      "  learn_time_ms: 5296.013\n",
      "  load_throughput: 13021744.8\n",
      "  load_time_ms: 0.307\n",
      "  training_iteration_time_ms: 11641.858\n",
      "  update_time_ms: 2.243\n",
      "timestamp: 1660564968\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 396000\n",
      "training_iteration: 99\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 400000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 400000\n",
      "  num_agent_steps_trained: 400000\n",
      "  num_env_steps_sampled: 400000\n",
      "  num_env_steps_trained: 400000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-02-57\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -876.4438065101048\n",
      "episode_reward_mean: -1156.520324605411\n",
      "episode_reward_min: -1771.0757179395987\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2000\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6842092275619507\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4085898399353027\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008598264306783676\n",
      "        model: {}\n",
      "        policy_loss: 0.008809366263449192\n",
      "        total_loss: 9.893471717834473\n",
      "        vf_explained_var: -0.028669776394963264\n",
      "        vf_loss: 9.878779411315918\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 400000\n",
      "  num_agent_steps_trained: 400000\n",
      "  num_env_steps_sampled: 400000\n",
      "  num_env_steps_trained: 400000\n",
      "iterations_since_restore: 100\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 400000\n",
      "num_agent_steps_trained: 400000\n",
      "num_env_steps_sampled: 400000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 400000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 52.03076923076924\n",
      "  ram_util_percent: 59.9153846153846\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1942033852050176\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14980162025648283\n",
      "  mean_inference_ms: 0.9865536486628372\n",
      "  mean_raw_obs_processing_ms: 0.12231222104457523\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -876.4438065101048\n",
      "  episode_reward_mean: -1156.520324605411\n",
      "  episode_reward_min: -1771.0757179395987\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -990.8567426710517\n",
      "    - -1534.2081281245903\n",
      "    - -1090.5570478994669\n",
      "    - -903.7276457499495\n",
      "    - -1509.8550236819112\n",
      "    - -1135.8744621214187\n",
      "    - -916.2758070341552\n",
      "    - -981.4673113637484\n",
      "    - -1167.4279804192697\n",
      "    - -887.6106709242704\n",
      "    - -984.5363953569738\n",
      "    - -982.0581833363628\n",
      "    - -910.1313890444167\n",
      "    - -913.5595241903576\n",
      "    - -1003.1922530526542\n",
      "    - -903.3162007310815\n",
      "    - -1114.9247803884982\n",
      "    - -1558.022210784814\n",
      "    - -1089.7063084399783\n",
      "    - -903.8039054789984\n",
      "    - -980.0510984681674\n",
      "    - -980.2250175641725\n",
      "    - -966.0744845002997\n",
      "    - -889.9531423408133\n",
      "    - -979.3585383712483\n",
      "    - -1189.0307488669177\n",
      "    - -1708.2264714291616\n",
      "    - -1579.9415264514935\n",
      "    - -1597.0353861037784\n",
      "    - -885.2762178965107\n",
      "    - -987.0743679542172\n",
      "    - -882.566999485488\n",
      "    - -1010.556918169529\n",
      "    - -1177.3922593414406\n",
      "    - -898.0452807172393\n",
      "    - -894.447789817357\n",
      "    - -890.7980019999437\n",
      "    - -1513.8506308796016\n",
      "    - -1576.8449199335546\n",
      "    - -1262.7688502475685\n",
      "    - -905.9336076499162\n",
      "    - -984.3174632462775\n",
      "    - -880.9122069113342\n",
      "    - -896.5464250026383\n",
      "    - -915.9945903600141\n",
      "    - -882.4025854325118\n",
      "    - -1748.3689209084\n",
      "    - -885.5755562495534\n",
      "    - -1016.3325769140794\n",
      "    - -975.3243661428637\n",
      "    - -1317.2807268769027\n",
      "    - -1062.8766076783509\n",
      "    - -1312.401114011994\n",
      "    - -1277.4582928686204\n",
      "    - -1512.291503718301\n",
      "    - -1230.5035545426206\n",
      "    - -876.4438065101048\n",
      "    - -1668.6465248626923\n",
      "    - -1534.286627616815\n",
      "    - -1212.7950599401124\n",
      "    - -1174.5856292686651\n",
      "    - -1093.245630586604\n",
      "    - -1075.4563121659676\n",
      "    - -1181.0117796286006\n",
      "    - -881.3297084989453\n",
      "    - -1272.6770051042056\n",
      "    - -886.7540886355748\n",
      "    - -894.7916841819359\n",
      "    - -1565.798533099399\n",
      "    - -900.3637546421902\n",
      "    - -1713.5776686254471\n",
      "    - -1771.0757179395987\n",
      "    - -909.5875707946304\n",
      "    - -1493.4889932227595\n",
      "    - -1581.665780525652\n",
      "    - -977.7647244966485\n",
      "    - -1738.2362893097122\n",
      "    - -1536.5747319999234\n",
      "    - -891.3754361463488\n",
      "    - -1734.494520772572\n",
      "    - -993.9522514422853\n",
      "    - -1513.6348747376305\n",
      "    - -1279.5672605606762\n",
      "    - -1740.8770281962052\n",
      "    - -996.3771832336434\n",
      "    - -1167.019173661132\n",
      "    - -889.2642226859435\n",
      "    - -1099.1611711332223\n",
      "    - -1698.4781847121503\n",
      "    - -1010.5517614345367\n",
      "    - -897.7817192366671\n",
      "    - -880.7568217347724\n",
      "    - -1126.6659659708273\n",
      "    - -1458.761771436671\n",
      "    - -888.1011880164049\n",
      "    - -1101.9578081273748\n",
      "    - -907.788370614518\n",
      "    - -877.4186185030833\n",
      "    - -902.8130172348226\n",
      "    - -1519.9257694465834\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1942033852050176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14980162025648283\n",
      "    mean_inference_ms: 0.9865536486628372\n",
      "    mean_raw_obs_processing_ms: 0.12231222104457523\n",
      "time_since_restore: 1055.9522542953491\n",
      "time_this_iter_s: 9.55907917022705\n",
      "time_total_s: 1055.9522542953491\n",
      "timers:\n",
      "  learn_throughput: 800.298\n",
      "  learn_time_ms: 4998.135\n",
      "  load_throughput: 14106798.957\n",
      "  load_time_ms: 0.284\n",
      "  training_iteration_time_ms: 11233.726\n",
      "  update_time_ms: 2.295\n",
      "timestamp: 1660564977\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 400000\n",
      "training_iteration: 100\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 404000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 404000\n",
      "  num_agent_steps_trained: 404000\n",
      "  num_env_steps_sampled: 404000\n",
      "  num_env_steps_trained: 404000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-03-07\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -871.2729633551248\n",
      "episode_reward_mean: -1168.470823070388\n",
      "episode_reward_min: -1771.0757179395987\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2020\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6842092275619507\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4454745054244995\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013149382546544075\n",
      "        model: {}\n",
      "        policy_loss: 0.00903193186968565\n",
      "        total_loss: 9.874897003173828\n",
      "        vf_explained_var: -0.015039874240756035\n",
      "        vf_loss: 9.856868743896484\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 404000\n",
      "  num_agent_steps_trained: 404000\n",
      "  num_env_steps_sampled: 404000\n",
      "  num_env_steps_trained: 404000\n",
      "iterations_since_restore: 101\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 404000\n",
      "num_agent_steps_trained: 404000\n",
      "num_env_steps_sampled: 404000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 404000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 42.557142857142864\n",
      "  ram_util_percent: 59.821428571428555\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1940621420023971\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14972196304693702\n",
      "  mean_inference_ms: 0.9859113879161677\n",
      "  mean_raw_obs_processing_ms: 0.12223686592406448\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -871.2729633551248\n",
      "  episode_reward_mean: -1168.470823070388\n",
      "  episode_reward_min: -1771.0757179395987\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -980.0510984681674\n",
      "    - -980.2250175641725\n",
      "    - -966.0744845002997\n",
      "    - -889.9531423408133\n",
      "    - -979.3585383712483\n",
      "    - -1189.0307488669177\n",
      "    - -1708.2264714291616\n",
      "    - -1579.9415264514935\n",
      "    - -1597.0353861037784\n",
      "    - -885.2762178965107\n",
      "    - -987.0743679542172\n",
      "    - -882.566999485488\n",
      "    - -1010.556918169529\n",
      "    - -1177.3922593414406\n",
      "    - -898.0452807172393\n",
      "    - -894.447789817357\n",
      "    - -890.7980019999437\n",
      "    - -1513.8506308796016\n",
      "    - -1576.8449199335546\n",
      "    - -1262.7688502475685\n",
      "    - -905.9336076499162\n",
      "    - -984.3174632462775\n",
      "    - -880.9122069113342\n",
      "    - -896.5464250026383\n",
      "    - -915.9945903600141\n",
      "    - -882.4025854325118\n",
      "    - -1748.3689209084\n",
      "    - -885.5755562495534\n",
      "    - -1016.3325769140794\n",
      "    - -975.3243661428637\n",
      "    - -1317.2807268769027\n",
      "    - -1062.8766076783509\n",
      "    - -1312.401114011994\n",
      "    - -1277.4582928686204\n",
      "    - -1512.291503718301\n",
      "    - -1230.5035545426206\n",
      "    - -876.4438065101048\n",
      "    - -1668.6465248626923\n",
      "    - -1534.286627616815\n",
      "    - -1212.7950599401124\n",
      "    - -1174.5856292686651\n",
      "    - -1093.245630586604\n",
      "    - -1075.4563121659676\n",
      "    - -1181.0117796286006\n",
      "    - -881.3297084989453\n",
      "    - -1272.6770051042056\n",
      "    - -886.7540886355748\n",
      "    - -894.7916841819359\n",
      "    - -1565.798533099399\n",
      "    - -900.3637546421902\n",
      "    - -1713.5776686254471\n",
      "    - -1771.0757179395987\n",
      "    - -909.5875707946304\n",
      "    - -1493.4889932227595\n",
      "    - -1581.665780525652\n",
      "    - -977.7647244966485\n",
      "    - -1738.2362893097122\n",
      "    - -1536.5747319999234\n",
      "    - -891.3754361463488\n",
      "    - -1734.494520772572\n",
      "    - -993.9522514422853\n",
      "    - -1513.6348747376305\n",
      "    - -1279.5672605606762\n",
      "    - -1740.8770281962052\n",
      "    - -996.3771832336434\n",
      "    - -1167.019173661132\n",
      "    - -889.2642226859435\n",
      "    - -1099.1611711332223\n",
      "    - -1698.4781847121503\n",
      "    - -1010.5517614345367\n",
      "    - -897.7817192366671\n",
      "    - -880.7568217347724\n",
      "    - -1126.6659659708273\n",
      "    - -1458.761771436671\n",
      "    - -888.1011880164049\n",
      "    - -1101.9578081273748\n",
      "    - -907.788370614518\n",
      "    - -877.4186185030833\n",
      "    - -902.8130172348226\n",
      "    - -1519.9257694465834\n",
      "    - -1423.2335737507196\n",
      "    - -888.6283495522852\n",
      "    - -1094.6646665210544\n",
      "    - -1151.2257505498992\n",
      "    - -872.2450201222802\n",
      "    - -1310.3557350448339\n",
      "    - -1081.4032992496607\n",
      "    - -1555.7271090804963\n",
      "    - -871.2729633551248\n",
      "    - -1072.8720935763065\n",
      "    - -1019.321516080107\n",
      "    - -1176.3202287665817\n",
      "    - -1163.0273921860812\n",
      "    - -1254.8182099257974\n",
      "    - -1046.419034394811\n",
      "    - -1520.082064867255\n",
      "    - -889.0814990715619\n",
      "    - -1309.196122243225\n",
      "    - -889.1171221156327\n",
      "    - -1087.1500668379508\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1940621420023971\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14972196304693702\n",
      "    mean_inference_ms: 0.9859113879161677\n",
      "    mean_raw_obs_processing_ms: 0.12223686592406448\n",
      "time_since_restore: 1065.5318899154663\n",
      "time_this_iter_s: 9.579635620117188\n",
      "time_total_s: 1065.5318899154663\n",
      "timers:\n",
      "  learn_throughput: 808.858\n",
      "  learn_time_ms: 4945.243\n",
      "  load_throughput: 16680469.278\n",
      "  load_time_ms: 0.24\n",
      "  training_iteration_time_ms: 11065.64\n",
      "  update_time_ms: 2.29\n",
      "timestamp: 1660564987\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 404000\n",
      "training_iteration: 101\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "Iteration  100\n",
      "checkpoint saved at  /home/zet4/ray_results/PPOTrainer_Pendulum-v0_2022-08-15_13-45-117w98j8mg/checkpoint_000101/checkpoint-101\n",
      "agent_timesteps_total: 408000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 408000\n",
      "  num_agent_steps_trained: 408000\n",
      "  num_env_steps_sampled: 408000\n",
      "  num_env_steps_trained: 408000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-03-18\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -871.2729633551248\n",
      "episode_reward_mean: -1159.2648059760395\n",
      "episode_reward_min: -1771.0757179395987\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2040\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6842092275619507\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9743595123291016\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011407120153307915\n",
      "        model: {}\n",
      "        policy_loss: 0.010007888078689575\n",
      "        total_loss: 9.716084480285645\n",
      "        vf_explained_var: -0.028354858979582787\n",
      "        vf_loss: 9.698271751403809\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 408000\n",
      "  num_agent_steps_trained: 408000\n",
      "  num_env_steps_sampled: 408000\n",
      "  num_env_steps_trained: 408000\n",
      "iterations_since_restore: 102\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 408000\n",
      "num_agent_steps_trained: 408000\n",
      "num_env_steps_sampled: 408000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 408000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 52.60625\n",
      "  ram_util_percent: 60.3\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19401390060050352\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14970017908186806\n",
      "  mean_inference_ms: 0.9856996664527818\n",
      "  mean_raw_obs_processing_ms: 0.12220571611425102\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -871.2729633551248\n",
      "  episode_reward_mean: -1159.2648059760395\n",
      "  episode_reward_min: -1771.0757179395987\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -905.9336076499162\n",
      "    - -984.3174632462775\n",
      "    - -880.9122069113342\n",
      "    - -896.5464250026383\n",
      "    - -915.9945903600141\n",
      "    - -882.4025854325118\n",
      "    - -1748.3689209084\n",
      "    - -885.5755562495534\n",
      "    - -1016.3325769140794\n",
      "    - -975.3243661428637\n",
      "    - -1317.2807268769027\n",
      "    - -1062.8766076783509\n",
      "    - -1312.401114011994\n",
      "    - -1277.4582928686204\n",
      "    - -1512.291503718301\n",
      "    - -1230.5035545426206\n",
      "    - -876.4438065101048\n",
      "    - -1668.6465248626923\n",
      "    - -1534.286627616815\n",
      "    - -1212.7950599401124\n",
      "    - -1174.5856292686651\n",
      "    - -1093.245630586604\n",
      "    - -1075.4563121659676\n",
      "    - -1181.0117796286006\n",
      "    - -881.3297084989453\n",
      "    - -1272.6770051042056\n",
      "    - -886.7540886355748\n",
      "    - -894.7916841819359\n",
      "    - -1565.798533099399\n",
      "    - -900.3637546421902\n",
      "    - -1713.5776686254471\n",
      "    - -1771.0757179395987\n",
      "    - -909.5875707946304\n",
      "    - -1493.4889932227595\n",
      "    - -1581.665780525652\n",
      "    - -977.7647244966485\n",
      "    - -1738.2362893097122\n",
      "    - -1536.5747319999234\n",
      "    - -891.3754361463488\n",
      "    - -1734.494520772572\n",
      "    - -993.9522514422853\n",
      "    - -1513.6348747376305\n",
      "    - -1279.5672605606762\n",
      "    - -1740.8770281962052\n",
      "    - -996.3771832336434\n",
      "    - -1167.019173661132\n",
      "    - -889.2642226859435\n",
      "    - -1099.1611711332223\n",
      "    - -1698.4781847121503\n",
      "    - -1010.5517614345367\n",
      "    - -897.7817192366671\n",
      "    - -880.7568217347724\n",
      "    - -1126.6659659708273\n",
      "    - -1458.761771436671\n",
      "    - -888.1011880164049\n",
      "    - -1101.9578081273748\n",
      "    - -907.788370614518\n",
      "    - -877.4186185030833\n",
      "    - -902.8130172348226\n",
      "    - -1519.9257694465834\n",
      "    - -1423.2335737507196\n",
      "    - -888.6283495522852\n",
      "    - -1094.6646665210544\n",
      "    - -1151.2257505498992\n",
      "    - -872.2450201222802\n",
      "    - -1310.3557350448339\n",
      "    - -1081.4032992496607\n",
      "    - -1555.7271090804963\n",
      "    - -871.2729633551248\n",
      "    - -1072.8720935763065\n",
      "    - -1019.321516080107\n",
      "    - -1176.3202287665817\n",
      "    - -1163.0273921860812\n",
      "    - -1254.8182099257974\n",
      "    - -1046.419034394811\n",
      "    - -1520.082064867255\n",
      "    - -889.0814990715619\n",
      "    - -1309.196122243225\n",
      "    - -889.1171221156327\n",
      "    - -1087.1500668379508\n",
      "    - -884.299152598188\n",
      "    - -1073.9687557636184\n",
      "    - -1117.4760775228665\n",
      "    - -1508.2800543169596\n",
      "    - -982.460048571365\n",
      "    - -894.6205572649865\n",
      "    - -1085.4097275053061\n",
      "    - -1171.627332987225\n",
      "    - -876.5071694825665\n",
      "    - -1077.1649143529964\n",
      "    - -884.4291264417192\n",
      "    - -1325.8050260952605\n",
      "    - -893.3654635306698\n",
      "    - -873.7018841072467\n",
      "    - -983.7441535329499\n",
      "    - -1201.2618741410063\n",
      "    - -1204.803342665429\n",
      "    - -1617.9376768903478\n",
      "    - -883.3710815872658\n",
      "    - -1388.6835217456776\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19401390060050352\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14970017908186806\n",
      "    mean_inference_ms: 0.9856996664527818\n",
      "    mean_raw_obs_processing_ms: 0.12220571611425102\n",
      "time_since_restore: 1076.8533897399902\n",
      "time_this_iter_s: 11.321499824523926\n",
      "time_total_s: 1076.8533897399902\n",
      "timers:\n",
      "  learn_throughput: 830.542\n",
      "  learn_time_ms: 4816.134\n",
      "  load_throughput: 17658368.593\n",
      "  load_time_ms: 0.227\n",
      "  training_iteration_time_ms: 10729.532\n",
      "  update_time_ms: 2.301\n",
      "timestamp: 1660564998\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 408000\n",
      "training_iteration: 102\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 412000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 412000\n",
      "  num_agent_steps_trained: 412000\n",
      "  num_env_steps_sampled: 412000\n",
      "  num_env_steps_trained: 412000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-03-28\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -871.2729633551248\n",
      "episode_reward_mean: -1151.5810965200508\n",
      "episode_reward_min: -1771.0757179395987\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2060\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6842092275619507\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3345379829406738\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00958217028528452\n",
      "        model: {}\n",
      "        policy_loss: 0.012288523837924004\n",
      "        total_loss: 9.726953506469727\n",
      "        vf_explained_var: -0.011011980473995209\n",
      "        vf_loss: 9.708108901977539\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 412000\n",
      "  num_agent_steps_trained: 412000\n",
      "  num_env_steps_sampled: 412000\n",
      "  num_env_steps_trained: 412000\n",
      "iterations_since_restore: 103\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 412000\n",
      "num_agent_steps_trained: 412000\n",
      "num_env_steps_sampled: 412000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 412000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 34.52142857142857\n",
      "  ram_util_percent: 60.19285714285716\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19396665003787683\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14968639064230568\n",
      "  mean_inference_ms: 0.9855212836106375\n",
      "  mean_raw_obs_processing_ms: 0.12218290484719407\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -871.2729633551248\n",
      "  episode_reward_mean: -1151.5810965200508\n",
      "  episode_reward_min: -1771.0757179395987\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1174.5856292686651\n",
      "    - -1093.245630586604\n",
      "    - -1075.4563121659676\n",
      "    - -1181.0117796286006\n",
      "    - -881.3297084989453\n",
      "    - -1272.6770051042056\n",
      "    - -886.7540886355748\n",
      "    - -894.7916841819359\n",
      "    - -1565.798533099399\n",
      "    - -900.3637546421902\n",
      "    - -1713.5776686254471\n",
      "    - -1771.0757179395987\n",
      "    - -909.5875707946304\n",
      "    - -1493.4889932227595\n",
      "    - -1581.665780525652\n",
      "    - -977.7647244966485\n",
      "    - -1738.2362893097122\n",
      "    - -1536.5747319999234\n",
      "    - -891.3754361463488\n",
      "    - -1734.494520772572\n",
      "    - -993.9522514422853\n",
      "    - -1513.6348747376305\n",
      "    - -1279.5672605606762\n",
      "    - -1740.8770281962052\n",
      "    - -996.3771832336434\n",
      "    - -1167.019173661132\n",
      "    - -889.2642226859435\n",
      "    - -1099.1611711332223\n",
      "    - -1698.4781847121503\n",
      "    - -1010.5517614345367\n",
      "    - -897.7817192366671\n",
      "    - -880.7568217347724\n",
      "    - -1126.6659659708273\n",
      "    - -1458.761771436671\n",
      "    - -888.1011880164049\n",
      "    - -1101.9578081273748\n",
      "    - -907.788370614518\n",
      "    - -877.4186185030833\n",
      "    - -902.8130172348226\n",
      "    - -1519.9257694465834\n",
      "    - -1423.2335737507196\n",
      "    - -888.6283495522852\n",
      "    - -1094.6646665210544\n",
      "    - -1151.2257505498992\n",
      "    - -872.2450201222802\n",
      "    - -1310.3557350448339\n",
      "    - -1081.4032992496607\n",
      "    - -1555.7271090804963\n",
      "    - -871.2729633551248\n",
      "    - -1072.8720935763065\n",
      "    - -1019.321516080107\n",
      "    - -1176.3202287665817\n",
      "    - -1163.0273921860812\n",
      "    - -1254.8182099257974\n",
      "    - -1046.419034394811\n",
      "    - -1520.082064867255\n",
      "    - -889.0814990715619\n",
      "    - -1309.196122243225\n",
      "    - -889.1171221156327\n",
      "    - -1087.1500668379508\n",
      "    - -884.299152598188\n",
      "    - -1073.9687557636184\n",
      "    - -1117.4760775228665\n",
      "    - -1508.2800543169596\n",
      "    - -982.460048571365\n",
      "    - -894.6205572649865\n",
      "    - -1085.4097275053061\n",
      "    - -1171.627332987225\n",
      "    - -876.5071694825665\n",
      "    - -1077.1649143529964\n",
      "    - -884.4291264417192\n",
      "    - -1325.8050260952605\n",
      "    - -893.3654635306698\n",
      "    - -873.7018841072467\n",
      "    - -983.7441535329499\n",
      "    - -1201.2618741410063\n",
      "    - -1204.803342665429\n",
      "    - -1617.9376768903478\n",
      "    - -883.3710815872658\n",
      "    - -1388.6835217456776\n",
      "    - -1350.8253019457725\n",
      "    - -1157.488503665623\n",
      "    - -1078.7854644252566\n",
      "    - -889.7891211869925\n",
      "    - -1081.9141551741225\n",
      "    - -1167.814697729041\n",
      "    - -1014.6685274094095\n",
      "    - -1175.2087669485763\n",
      "    - -1004.8692017492865\n",
      "    - -875.7850049655289\n",
      "    - -1253.6849900215698\n",
      "    - -891.151647887124\n",
      "    - -904.6585772469543\n",
      "    - -893.5162918430466\n",
      "    - -1445.6440324501043\n",
      "    - -974.2450850942965\n",
      "    - -1490.9315566240089\n",
      "    - -1449.5755337936164\n",
      "    - -885.705301506563\n",
      "    - -1342.0594101783436\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19396665003787683\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14968639064230568\n",
      "    mean_inference_ms: 0.9855212836106375\n",
      "    mean_raw_obs_processing_ms: 0.12218290484719407\n",
      "time_since_restore: 1086.403740644455\n",
      "time_this_iter_s: 9.550350904464722\n",
      "time_total_s: 1086.403740644455\n",
      "timers:\n",
      "  learn_throughput: 923.095\n",
      "  learn_time_ms: 4333.251\n",
      "  load_throughput: 17658368.593\n",
      "  load_time_ms: 0.227\n",
      "  training_iteration_time_ms: 10179.2\n",
      "  update_time_ms: 2.242\n",
      "timestamp: 1660565008\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 412000\n",
      "training_iteration: 103\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 416000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 416000\n",
      "  num_agent_steps_trained: 416000\n",
      "  num_env_steps_sampled: 416000\n",
      "  num_env_steps_trained: 416000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-03-38\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -862.9231676724705\n",
      "episode_reward_mean: -1125.2588242907507\n",
      "episode_reward_min: -1740.8770281962052\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2080\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6842092275619507\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.450485348701477\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008467834442853928\n",
      "        model: {}\n",
      "        policy_loss: 0.012123442254960537\n",
      "        total_loss: 9.824604988098145\n",
      "        vf_explained_var: -0.02287735417485237\n",
      "        vf_loss: 9.80668830871582\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 416000\n",
      "  num_agent_steps_trained: 416000\n",
      "  num_env_steps_sampled: 416000\n",
      "  num_env_steps_trained: 416000\n",
      "iterations_since_restore: 104\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 416000\n",
      "num_agent_steps_trained: 416000\n",
      "num_env_steps_sampled: 416000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 416000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 41.67142857142858\n",
      "  ram_util_percent: 60.12857142857144\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19391989757034062\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14966926793023108\n",
      "  mean_inference_ms: 0.9852735551522227\n",
      "  mean_raw_obs_processing_ms: 0.1221554374933725\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -862.9231676724705\n",
      "  episode_reward_mean: -1125.2588242907507\n",
      "  episode_reward_min: -1740.8770281962052\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -993.9522514422853\n",
      "    - -1513.6348747376305\n",
      "    - -1279.5672605606762\n",
      "    - -1740.8770281962052\n",
      "    - -996.3771832336434\n",
      "    - -1167.019173661132\n",
      "    - -889.2642226859435\n",
      "    - -1099.1611711332223\n",
      "    - -1698.4781847121503\n",
      "    - -1010.5517614345367\n",
      "    - -897.7817192366671\n",
      "    - -880.7568217347724\n",
      "    - -1126.6659659708273\n",
      "    - -1458.761771436671\n",
      "    - -888.1011880164049\n",
      "    - -1101.9578081273748\n",
      "    - -907.788370614518\n",
      "    - -877.4186185030833\n",
      "    - -902.8130172348226\n",
      "    - -1519.9257694465834\n",
      "    - -1423.2335737507196\n",
      "    - -888.6283495522852\n",
      "    - -1094.6646665210544\n",
      "    - -1151.2257505498992\n",
      "    - -872.2450201222802\n",
      "    - -1310.3557350448339\n",
      "    - -1081.4032992496607\n",
      "    - -1555.7271090804963\n",
      "    - -871.2729633551248\n",
      "    - -1072.8720935763065\n",
      "    - -1019.321516080107\n",
      "    - -1176.3202287665817\n",
      "    - -1163.0273921860812\n",
      "    - -1254.8182099257974\n",
      "    - -1046.419034394811\n",
      "    - -1520.082064867255\n",
      "    - -889.0814990715619\n",
      "    - -1309.196122243225\n",
      "    - -889.1171221156327\n",
      "    - -1087.1500668379508\n",
      "    - -884.299152598188\n",
      "    - -1073.9687557636184\n",
      "    - -1117.4760775228665\n",
      "    - -1508.2800543169596\n",
      "    - -982.460048571365\n",
      "    - -894.6205572649865\n",
      "    - -1085.4097275053061\n",
      "    - -1171.627332987225\n",
      "    - -876.5071694825665\n",
      "    - -1077.1649143529964\n",
      "    - -884.4291264417192\n",
      "    - -1325.8050260952605\n",
      "    - -893.3654635306698\n",
      "    - -873.7018841072467\n",
      "    - -983.7441535329499\n",
      "    - -1201.2618741410063\n",
      "    - -1204.803342665429\n",
      "    - -1617.9376768903478\n",
      "    - -883.3710815872658\n",
      "    - -1388.6835217456776\n",
      "    - -1350.8253019457725\n",
      "    - -1157.488503665623\n",
      "    - -1078.7854644252566\n",
      "    - -889.7891211869925\n",
      "    - -1081.9141551741225\n",
      "    - -1167.814697729041\n",
      "    - -1014.6685274094095\n",
      "    - -1175.2087669485763\n",
      "    - -1004.8692017492865\n",
      "    - -875.7850049655289\n",
      "    - -1253.6849900215698\n",
      "    - -891.151647887124\n",
      "    - -904.6585772469543\n",
      "    - -893.5162918430466\n",
      "    - -1445.6440324501043\n",
      "    - -974.2450850942965\n",
      "    - -1490.9315566240089\n",
      "    - -1449.5755337936164\n",
      "    - -885.705301506563\n",
      "    - -1342.0594101783436\n",
      "    - -892.9951544093361\n",
      "    - -889.3840193908972\n",
      "    - -889.2652893853035\n",
      "    - -896.2292012167951\n",
      "    - -1418.1567327232908\n",
      "    - -862.9231676724705\n",
      "    - -1670.723279063938\n",
      "    - -1441.7153789945721\n",
      "    - -1059.1949542420055\n",
      "    - -880.4737612814191\n",
      "    - -1656.4470259234408\n",
      "    - -974.249361381913\n",
      "    - -1533.0434416911125\n",
      "    - -1728.9523287016316\n",
      "    - -1241.9743762726155\n",
      "    - -936.7707725177372\n",
      "    - -887.161561197872\n",
      "    - -876.1505720148685\n",
      "    - -1007.5926375938154\n",
      "    - -898.2253210403072\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19391989757034062\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14966926793023108\n",
      "    mean_inference_ms: 0.9852735551522227\n",
      "    mean_raw_obs_processing_ms: 0.1221554374933725\n",
      "time_since_restore: 1095.8696835041046\n",
      "time_this_iter_s: 9.465942859649658\n",
      "time_total_s: 1095.8696835041046\n",
      "timers:\n",
      "  learn_throughput: 942.887\n",
      "  learn_time_ms: 4242.289\n",
      "  load_throughput: 17763066.173\n",
      "  load_time_ms: 0.225\n",
      "  training_iteration_time_ms: 9996.569\n",
      "  update_time_ms: 2.252\n",
      "timestamp: 1660565018\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 416000\n",
      "training_iteration: 104\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 420000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 420000\n",
      "  num_agent_steps_trained: 420000\n",
      "  num_env_steps_sampled: 420000\n",
      "  num_env_steps_trained: 420000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-03-47\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -862.9231676724705\n",
      "episode_reward_mean: -1104.215130077473\n",
      "episode_reward_min: -1728.9523287016316\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2100\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6842092275619507\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3034251630306244\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011312201619148254\n",
      "        model: {}\n",
      "        policy_loss: 0.011178851127624512\n",
      "        total_loss: 9.729470252990723\n",
      "        vf_explained_var: -0.03874713554978371\n",
      "        vf_loss: 9.710551261901855\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 420000\n",
      "  num_agent_steps_trained: 420000\n",
      "  num_env_steps_sampled: 420000\n",
      "  num_env_steps_trained: 420000\n",
      "iterations_since_restore: 105\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 420000\n",
      "num_agent_steps_trained: 420000\n",
      "num_env_steps_sampled: 420000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 420000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.335714285714282\n",
      "  ram_util_percent: 60.050000000000004\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1938773084464935\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14965834728753488\n",
      "  mean_inference_ms: 0.9850836568094283\n",
      "  mean_raw_obs_processing_ms: 0.12213432656808568\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -862.9231676724705\n",
      "  episode_reward_mean: -1104.215130077473\n",
      "  episode_reward_min: -1728.9523287016316\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1423.2335737507196\n",
      "    - -888.6283495522852\n",
      "    - -1094.6646665210544\n",
      "    - -1151.2257505498992\n",
      "    - -872.2450201222802\n",
      "    - -1310.3557350448339\n",
      "    - -1081.4032992496607\n",
      "    - -1555.7271090804963\n",
      "    - -871.2729633551248\n",
      "    - -1072.8720935763065\n",
      "    - -1019.321516080107\n",
      "    - -1176.3202287665817\n",
      "    - -1163.0273921860812\n",
      "    - -1254.8182099257974\n",
      "    - -1046.419034394811\n",
      "    - -1520.082064867255\n",
      "    - -889.0814990715619\n",
      "    - -1309.196122243225\n",
      "    - -889.1171221156327\n",
      "    - -1087.1500668379508\n",
      "    - -884.299152598188\n",
      "    - -1073.9687557636184\n",
      "    - -1117.4760775228665\n",
      "    - -1508.2800543169596\n",
      "    - -982.460048571365\n",
      "    - -894.6205572649865\n",
      "    - -1085.4097275053061\n",
      "    - -1171.627332987225\n",
      "    - -876.5071694825665\n",
      "    - -1077.1649143529964\n",
      "    - -884.4291264417192\n",
      "    - -1325.8050260952605\n",
      "    - -893.3654635306698\n",
      "    - -873.7018841072467\n",
      "    - -983.7441535329499\n",
      "    - -1201.2618741410063\n",
      "    - -1204.803342665429\n",
      "    - -1617.9376768903478\n",
      "    - -883.3710815872658\n",
      "    - -1388.6835217456776\n",
      "    - -1350.8253019457725\n",
      "    - -1157.488503665623\n",
      "    - -1078.7854644252566\n",
      "    - -889.7891211869925\n",
      "    - -1081.9141551741225\n",
      "    - -1167.814697729041\n",
      "    - -1014.6685274094095\n",
      "    - -1175.2087669485763\n",
      "    - -1004.8692017492865\n",
      "    - -875.7850049655289\n",
      "    - -1253.6849900215698\n",
      "    - -891.151647887124\n",
      "    - -904.6585772469543\n",
      "    - -893.5162918430466\n",
      "    - -1445.6440324501043\n",
      "    - -974.2450850942965\n",
      "    - -1490.9315566240089\n",
      "    - -1449.5755337936164\n",
      "    - -885.705301506563\n",
      "    - -1342.0594101783436\n",
      "    - -892.9951544093361\n",
      "    - -889.3840193908972\n",
      "    - -889.2652893853035\n",
      "    - -896.2292012167951\n",
      "    - -1418.1567327232908\n",
      "    - -862.9231676724705\n",
      "    - -1670.723279063938\n",
      "    - -1441.7153789945721\n",
      "    - -1059.1949542420055\n",
      "    - -880.4737612814191\n",
      "    - -1656.4470259234408\n",
      "    - -974.249361381913\n",
      "    - -1533.0434416911125\n",
      "    - -1728.9523287016316\n",
      "    - -1241.9743762726155\n",
      "    - -936.7707725177372\n",
      "    - -887.161561197872\n",
      "    - -876.1505720148685\n",
      "    - -1007.5926375938154\n",
      "    - -898.2253210403072\n",
      "    - -1190.7416692696738\n",
      "    - -1198.385431535943\n",
      "    - -1056.4020312639202\n",
      "    - -965.7024541611122\n",
      "    - -886.8940957982213\n",
      "    - -881.721003261111\n",
      "    - -1295.7981027579444\n",
      "    - -908.9576175680749\n",
      "    - -995.8006754836854\n",
      "    - -989.0437255312385\n",
      "    - -1021.2458696484814\n",
      "    - -888.6885614505264\n",
      "    - -1066.8235413872308\n",
      "    - -1060.6086375520554\n",
      "    - -981.451822437378\n",
      "    - -990.371520339672\n",
      "    - -1107.8093434580721\n",
      "    - -881.2478064456144\n",
      "    - -1591.4750093020477\n",
      "    - -887.3158221394325\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1938773084464935\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14965834728753488\n",
      "    mean_inference_ms: 0.9850836568094283\n",
      "    mean_raw_obs_processing_ms: 0.12213432656808568\n",
      "time_since_restore: 1105.5958380699158\n",
      "time_this_iter_s: 9.726154565811157\n",
      "time_total_s: 1105.5958380699158\n",
      "timers:\n",
      "  learn_throughput: 950.196\n",
      "  learn_time_ms: 4209.657\n",
      "  load_throughput: 17886157.783\n",
      "  load_time_ms: 0.224\n",
      "  training_iteration_time_ms: 9864.72\n",
      "  update_time_ms: 2.24\n",
      "timestamp: 1660565027\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 420000\n",
      "training_iteration: 105\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 424000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 424000\n",
      "  num_agent_steps_trained: 424000\n",
      "  num_env_steps_sampled: 424000\n",
      "  num_env_steps_trained: 424000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-03-57\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -862.9231676724705\n",
      "episode_reward_mean: -1112.8789556727197\n",
      "episode_reward_min: -1728.9523287016316\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2120\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6842092275619507\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8698679208755493\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005731367971748114\n",
      "        model: {}\n",
      "        policy_loss: 0.009649270214140415\n",
      "        total_loss: 9.83207893371582\n",
      "        vf_explained_var: -0.015246439725160599\n",
      "        vf_loss: 9.81850814819336\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 424000\n",
      "  num_agent_steps_trained: 424000\n",
      "  num_env_steps_sampled: 424000\n",
      "  num_env_steps_trained: 424000\n",
      "iterations_since_restore: 106\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 424000\n",
      "num_agent_steps_trained: 424000\n",
      "num_env_steps_sampled: 424000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 424000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.39285714285715\n",
      "  ram_util_percent: 60.028571428571425\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19384465299794876\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14965154795977179\n",
      "  mean_inference_ms: 0.9849431219660594\n",
      "  mean_raw_obs_processing_ms: 0.12211613772906502\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -862.9231676724705\n",
      "  episode_reward_mean: -1112.8789556727197\n",
      "  episode_reward_min: -1728.9523287016316\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -884.299152598188\n",
      "    - -1073.9687557636184\n",
      "    - -1117.4760775228665\n",
      "    - -1508.2800543169596\n",
      "    - -982.460048571365\n",
      "    - -894.6205572649865\n",
      "    - -1085.4097275053061\n",
      "    - -1171.627332987225\n",
      "    - -876.5071694825665\n",
      "    - -1077.1649143529964\n",
      "    - -884.4291264417192\n",
      "    - -1325.8050260952605\n",
      "    - -893.3654635306698\n",
      "    - -873.7018841072467\n",
      "    - -983.7441535329499\n",
      "    - -1201.2618741410063\n",
      "    - -1204.803342665429\n",
      "    - -1617.9376768903478\n",
      "    - -883.3710815872658\n",
      "    - -1388.6835217456776\n",
      "    - -1350.8253019457725\n",
      "    - -1157.488503665623\n",
      "    - -1078.7854644252566\n",
      "    - -889.7891211869925\n",
      "    - -1081.9141551741225\n",
      "    - -1167.814697729041\n",
      "    - -1014.6685274094095\n",
      "    - -1175.2087669485763\n",
      "    - -1004.8692017492865\n",
      "    - -875.7850049655289\n",
      "    - -1253.6849900215698\n",
      "    - -891.151647887124\n",
      "    - -904.6585772469543\n",
      "    - -893.5162918430466\n",
      "    - -1445.6440324501043\n",
      "    - -974.2450850942965\n",
      "    - -1490.9315566240089\n",
      "    - -1449.5755337936164\n",
      "    - -885.705301506563\n",
      "    - -1342.0594101783436\n",
      "    - -892.9951544093361\n",
      "    - -889.3840193908972\n",
      "    - -889.2652893853035\n",
      "    - -896.2292012167951\n",
      "    - -1418.1567327232908\n",
      "    - -862.9231676724705\n",
      "    - -1670.723279063938\n",
      "    - -1441.7153789945721\n",
      "    - -1059.1949542420055\n",
      "    - -880.4737612814191\n",
      "    - -1656.4470259234408\n",
      "    - -974.249361381913\n",
      "    - -1533.0434416911125\n",
      "    - -1728.9523287016316\n",
      "    - -1241.9743762726155\n",
      "    - -936.7707725177372\n",
      "    - -887.161561197872\n",
      "    - -876.1505720148685\n",
      "    - -1007.5926375938154\n",
      "    - -898.2253210403072\n",
      "    - -1190.7416692696738\n",
      "    - -1198.385431535943\n",
      "    - -1056.4020312639202\n",
      "    - -965.7024541611122\n",
      "    - -886.8940957982213\n",
      "    - -881.721003261111\n",
      "    - -1295.7981027579444\n",
      "    - -908.9576175680749\n",
      "    - -995.8006754836854\n",
      "    - -989.0437255312385\n",
      "    - -1021.2458696484814\n",
      "    - -888.6885614505264\n",
      "    - -1066.8235413872308\n",
      "    - -1060.6086375520554\n",
      "    - -981.451822437378\n",
      "    - -990.371520339672\n",
      "    - -1107.8093434580721\n",
      "    - -881.2478064456144\n",
      "    - -1591.4750093020477\n",
      "    - -887.3158221394325\n",
      "    - -883.4626623739798\n",
      "    - -1422.9207823779116\n",
      "    - -1004.565043878547\n",
      "    - -898.018712930283\n",
      "    - -1616.0992003330616\n",
      "    - -1712.7389585199792\n",
      "    - -1308.1070555880074\n",
      "    - -1019.9180277120048\n",
      "    - -1080.986543361857\n",
      "    - -955.2666581905783\n",
      "    - -887.6065550827565\n",
      "    - -1085.359776709376\n",
      "    - -894.1467875345668\n",
      "    - -1359.7449683295868\n",
      "    - -887.7843213536712\n",
      "    - -1074.6284746267015\n",
      "    - -894.8763571538504\n",
      "    - -1291.351343927059\n",
      "    - -1649.9277290043065\n",
      "    - -1615.0344178282305\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19384465299794876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14965154795977179\n",
      "    mean_inference_ms: 0.9849431219660594\n",
      "    mean_raw_obs_processing_ms: 0.12211613772906502\n",
      "time_since_restore: 1115.662782907486\n",
      "time_this_iter_s: 10.06694483757019\n",
      "time_total_s: 1115.662782907486\n",
      "timers:\n",
      "  learn_throughput: 946.356\n",
      "  learn_time_ms: 4226.74\n",
      "  load_throughput: 17901425.523\n",
      "  load_time_ms: 0.223\n",
      "  training_iteration_time_ms: 9870.42\n",
      "  update_time_ms: 2.267\n",
      "timestamp: 1660565037\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 424000\n",
      "training_iteration: 106\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 428000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 428000\n",
      "  num_agent_steps_trained: 428000\n",
      "  num_env_steps_sampled: 428000\n",
      "  num_env_steps_trained: 428000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-04-07\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -862.9231676724705\n",
      "episode_reward_mean: -1129.180015343611\n",
      "episode_reward_min: -1728.9523287016316\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2140\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6842092275619507\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8319529294967651\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011473282240331173\n",
      "        model: {}\n",
      "        policy_loss: 0.012899546884000301\n",
      "        total_loss: 9.880725860595703\n",
      "        vf_explained_var: -0.01965564675629139\n",
      "        vf_loss: 9.859976768493652\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 428000\n",
      "  num_agent_steps_trained: 428000\n",
      "  num_env_steps_sampled: 428000\n",
      "  num_env_steps_trained: 428000\n",
      "iterations_since_restore: 107\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 428000\n",
      "num_agent_steps_trained: 428000\n",
      "num_env_steps_sampled: 428000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 428000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.52857142857143\n",
      "  ram_util_percent: 59.95714285714285\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19372720929687742\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1495933398166769\n",
      "  mean_inference_ms: 0.9844442811249533\n",
      "  mean_raw_obs_processing_ms: 0.12205663314847331\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -862.9231676724705\n",
      "  episode_reward_mean: -1129.180015343611\n",
      "  episode_reward_min: -1728.9523287016316\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1350.8253019457725\n",
      "    - -1157.488503665623\n",
      "    - -1078.7854644252566\n",
      "    - -889.7891211869925\n",
      "    - -1081.9141551741225\n",
      "    - -1167.814697729041\n",
      "    - -1014.6685274094095\n",
      "    - -1175.2087669485763\n",
      "    - -1004.8692017492865\n",
      "    - -875.7850049655289\n",
      "    - -1253.6849900215698\n",
      "    - -891.151647887124\n",
      "    - -904.6585772469543\n",
      "    - -893.5162918430466\n",
      "    - -1445.6440324501043\n",
      "    - -974.2450850942965\n",
      "    - -1490.9315566240089\n",
      "    - -1449.5755337936164\n",
      "    - -885.705301506563\n",
      "    - -1342.0594101783436\n",
      "    - -892.9951544093361\n",
      "    - -889.3840193908972\n",
      "    - -889.2652893853035\n",
      "    - -896.2292012167951\n",
      "    - -1418.1567327232908\n",
      "    - -862.9231676724705\n",
      "    - -1670.723279063938\n",
      "    - -1441.7153789945721\n",
      "    - -1059.1949542420055\n",
      "    - -880.4737612814191\n",
      "    - -1656.4470259234408\n",
      "    - -974.249361381913\n",
      "    - -1533.0434416911125\n",
      "    - -1728.9523287016316\n",
      "    - -1241.9743762726155\n",
      "    - -936.7707725177372\n",
      "    - -887.161561197872\n",
      "    - -876.1505720148685\n",
      "    - -1007.5926375938154\n",
      "    - -898.2253210403072\n",
      "    - -1190.7416692696738\n",
      "    - -1198.385431535943\n",
      "    - -1056.4020312639202\n",
      "    - -965.7024541611122\n",
      "    - -886.8940957982213\n",
      "    - -881.721003261111\n",
      "    - -1295.7981027579444\n",
      "    - -908.9576175680749\n",
      "    - -995.8006754836854\n",
      "    - -989.0437255312385\n",
      "    - -1021.2458696484814\n",
      "    - -888.6885614505264\n",
      "    - -1066.8235413872308\n",
      "    - -1060.6086375520554\n",
      "    - -981.451822437378\n",
      "    - -990.371520339672\n",
      "    - -1107.8093434580721\n",
      "    - -881.2478064456144\n",
      "    - -1591.4750093020477\n",
      "    - -887.3158221394325\n",
      "    - -883.4626623739798\n",
      "    - -1422.9207823779116\n",
      "    - -1004.565043878547\n",
      "    - -898.018712930283\n",
      "    - -1616.0992003330616\n",
      "    - -1712.7389585199792\n",
      "    - -1308.1070555880074\n",
      "    - -1019.9180277120048\n",
      "    - -1080.986543361857\n",
      "    - -955.2666581905783\n",
      "    - -887.6065550827565\n",
      "    - -1085.359776709376\n",
      "    - -894.1467875345668\n",
      "    - -1359.7449683295868\n",
      "    - -887.7843213536712\n",
      "    - -1074.6284746267015\n",
      "    - -894.8763571538504\n",
      "    - -1291.351343927059\n",
      "    - -1649.9277290043065\n",
      "    - -1615.0344178282305\n",
      "    - -998.3712884852137\n",
      "    - -897.5245018444549\n",
      "    - -1498.040512913482\n",
      "    - -1298.5067570425508\n",
      "    - -1459.022948613478\n",
      "    - -1295.2904403276727\n",
      "    - -1324.3481312860938\n",
      "    - -912.5604495397141\n",
      "    - -873.1155185933668\n",
      "    - -1455.3550309824439\n",
      "    - -1207.9991067522478\n",
      "    - -1146.6347604144505\n",
      "    - -889.3213826619135\n",
      "    - -965.289992598452\n",
      "    - -1080.8071659628383\n",
      "    - -1625.4550475682474\n",
      "    - -1654.7630076216994\n",
      "    - -1113.6820014953644\n",
      "    - -973.3541229390318\n",
      "    - -889.5807405500692\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19372720929687742\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1495933398166769\n",
      "    mean_inference_ms: 0.9844442811249533\n",
      "    mean_raw_obs_processing_ms: 0.12205663314847331\n",
      "time_since_restore: 1125.552329301834\n",
      "time_this_iter_s: 9.889546394348145\n",
      "time_total_s: 1125.552329301834\n",
      "timers:\n",
      "  learn_throughput: 945.61\n",
      "  learn_time_ms: 4230.075\n",
      "  load_throughput: 18446636.614\n",
      "  load_time_ms: 0.217\n",
      "  training_iteration_time_ms: 9886.843\n",
      "  update_time_ms: 2.269\n",
      "timestamp: 1660565047\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 428000\n",
      "training_iteration: 107\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 432000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 432000\n",
      "  num_agent_steps_trained: 432000\n",
      "  num_env_steps_sampled: 432000\n",
      "  num_env_steps_trained: 432000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-04-17\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -862.9231676724705\n",
      "episode_reward_mean: -1125.721554754959\n",
      "episode_reward_min: -1728.9523287016316\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2160\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6842092275619507\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0425742864608765\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00956598948687315\n",
      "        model: {}\n",
      "        policy_loss: 0.01101875863969326\n",
      "        total_loss: 9.816632270812988\n",
      "        vf_explained_var: -0.02634657733142376\n",
      "        vf_loss: 9.799068450927734\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 432000\n",
      "  num_agent_steps_trained: 432000\n",
      "  num_env_steps_sampled: 432000\n",
      "  num_env_steps_trained: 432000\n",
      "iterations_since_restore: 108\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 432000\n",
      "num_agent_steps_trained: 432000\n",
      "num_env_steps_sampled: 432000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 432000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.05\n",
      "  ram_util_percent: 60.02857142857143\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19361491053070773\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1495333757194951\n",
      "  mean_inference_ms: 0.9839541401522086\n",
      "  mean_raw_obs_processing_ms: 0.12199429465356858\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -862.9231676724705\n",
      "  episode_reward_mean: -1125.721554754959\n",
      "  episode_reward_min: -1728.9523287016316\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -892.9951544093361\n",
      "    - -889.3840193908972\n",
      "    - -889.2652893853035\n",
      "    - -896.2292012167951\n",
      "    - -1418.1567327232908\n",
      "    - -862.9231676724705\n",
      "    - -1670.723279063938\n",
      "    - -1441.7153789945721\n",
      "    - -1059.1949542420055\n",
      "    - -880.4737612814191\n",
      "    - -1656.4470259234408\n",
      "    - -974.249361381913\n",
      "    - -1533.0434416911125\n",
      "    - -1728.9523287016316\n",
      "    - -1241.9743762726155\n",
      "    - -936.7707725177372\n",
      "    - -887.161561197872\n",
      "    - -876.1505720148685\n",
      "    - -1007.5926375938154\n",
      "    - -898.2253210403072\n",
      "    - -1190.7416692696738\n",
      "    - -1198.385431535943\n",
      "    - -1056.4020312639202\n",
      "    - -965.7024541611122\n",
      "    - -886.8940957982213\n",
      "    - -881.721003261111\n",
      "    - -1295.7981027579444\n",
      "    - -908.9576175680749\n",
      "    - -995.8006754836854\n",
      "    - -989.0437255312385\n",
      "    - -1021.2458696484814\n",
      "    - -888.6885614505264\n",
      "    - -1066.8235413872308\n",
      "    - -1060.6086375520554\n",
      "    - -981.451822437378\n",
      "    - -990.371520339672\n",
      "    - -1107.8093434580721\n",
      "    - -881.2478064456144\n",
      "    - -1591.4750093020477\n",
      "    - -887.3158221394325\n",
      "    - -883.4626623739798\n",
      "    - -1422.9207823779116\n",
      "    - -1004.565043878547\n",
      "    - -898.018712930283\n",
      "    - -1616.0992003330616\n",
      "    - -1712.7389585199792\n",
      "    - -1308.1070555880074\n",
      "    - -1019.9180277120048\n",
      "    - -1080.986543361857\n",
      "    - -955.2666581905783\n",
      "    - -887.6065550827565\n",
      "    - -1085.359776709376\n",
      "    - -894.1467875345668\n",
      "    - -1359.7449683295868\n",
      "    - -887.7843213536712\n",
      "    - -1074.6284746267015\n",
      "    - -894.8763571538504\n",
      "    - -1291.351343927059\n",
      "    - -1649.9277290043065\n",
      "    - -1615.0344178282305\n",
      "    - -998.3712884852137\n",
      "    - -897.5245018444549\n",
      "    - -1498.040512913482\n",
      "    - -1298.5067570425508\n",
      "    - -1459.022948613478\n",
      "    - -1295.2904403276727\n",
      "    - -1324.3481312860938\n",
      "    - -912.5604495397141\n",
      "    - -873.1155185933668\n",
      "    - -1455.3550309824439\n",
      "    - -1207.9991067522478\n",
      "    - -1146.6347604144505\n",
      "    - -889.3213826619135\n",
      "    - -965.289992598452\n",
      "    - -1080.8071659628383\n",
      "    - -1625.4550475682474\n",
      "    - -1654.7630076216994\n",
      "    - -1113.6820014953644\n",
      "    - -973.3541229390318\n",
      "    - -889.5807405500692\n",
      "    - -882.0209352777715\n",
      "    - -1017.8774465654021\n",
      "    - -894.4398151287013\n",
      "    - -1645.4330576107184\n",
      "    - -892.7045029667809\n",
      "    - -1175.990852392987\n",
      "    - -1295.753410873304\n",
      "    - -1420.006433524892\n",
      "    - -870.862579859964\n",
      "    - -1024.6180672112591\n",
      "    - -1354.5219372280392\n",
      "    - -1586.0347576188865\n",
      "    - -890.2344549259167\n",
      "    - -969.6598612403997\n",
      "    - -976.5861117808253\n",
      "    - -888.0891255679197\n",
      "    - -888.3034193486543\n",
      "    - -889.4690132936356\n",
      "    - -893.5705495561834\n",
      "    - -1526.2987810077857\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19361491053070773\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1495333757194951\n",
      "    mean_inference_ms: 0.9839541401522086\n",
      "    mean_raw_obs_processing_ms: 0.12199429465356858\n",
      "time_since_restore: 1135.1420414447784\n",
      "time_this_iter_s: 9.589712142944336\n",
      "time_total_s: 1135.1420414447784\n",
      "timers:\n",
      "  learn_throughput: 958.145\n",
      "  learn_time_ms: 4174.736\n",
      "  load_throughput: 18440553.968\n",
      "  load_time_ms: 0.217\n",
      "  training_iteration_time_ms: 9837.716\n",
      "  update_time_ms: 2.266\n",
      "timestamp: 1660565057\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 432000\n",
      "training_iteration: 108\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 436000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 436000\n",
      "  num_agent_steps_trained: 436000\n",
      "  num_env_steps_sampled: 436000\n",
      "  num_env_steps_trained: 436000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-04-27\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -870.862579859964\n",
      "episode_reward_mean: -1132.3192506426403\n",
      "episode_reward_min: -1712.7389585199792\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2180\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6842092275619507\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6585670709609985\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012233319692313671\n",
      "        model: {}\n",
      "        policy_loss: 0.010182552970945835\n",
      "        total_loss: 9.808489799499512\n",
      "        vf_explained_var: -0.02240448258817196\n",
      "        vf_loss: 9.789937019348145\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 436000\n",
      "  num_agent_steps_trained: 436000\n",
      "  num_env_steps_sampled: 436000\n",
      "  num_env_steps_trained: 436000\n",
      "iterations_since_restore: 109\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 436000\n",
      "num_agent_steps_trained: 436000\n",
      "num_env_steps_sampled: 436000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 436000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.66428571428571\n",
      "  ram_util_percent: 60.00714285714286\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1935020999521622\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1494786570617027\n",
      "  mean_inference_ms: 0.9835055181107865\n",
      "  mean_raw_obs_processing_ms: 0.12193713925521248\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -870.862579859964\n",
      "  episode_reward_mean: -1132.3192506426403\n",
      "  episode_reward_min: -1712.7389585199792\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1190.7416692696738\n",
      "    - -1198.385431535943\n",
      "    - -1056.4020312639202\n",
      "    - -965.7024541611122\n",
      "    - -886.8940957982213\n",
      "    - -881.721003261111\n",
      "    - -1295.7981027579444\n",
      "    - -908.9576175680749\n",
      "    - -995.8006754836854\n",
      "    - -989.0437255312385\n",
      "    - -1021.2458696484814\n",
      "    - -888.6885614505264\n",
      "    - -1066.8235413872308\n",
      "    - -1060.6086375520554\n",
      "    - -981.451822437378\n",
      "    - -990.371520339672\n",
      "    - -1107.8093434580721\n",
      "    - -881.2478064456144\n",
      "    - -1591.4750093020477\n",
      "    - -887.3158221394325\n",
      "    - -883.4626623739798\n",
      "    - -1422.9207823779116\n",
      "    - -1004.565043878547\n",
      "    - -898.018712930283\n",
      "    - -1616.0992003330616\n",
      "    - -1712.7389585199792\n",
      "    - -1308.1070555880074\n",
      "    - -1019.9180277120048\n",
      "    - -1080.986543361857\n",
      "    - -955.2666581905783\n",
      "    - -887.6065550827565\n",
      "    - -1085.359776709376\n",
      "    - -894.1467875345668\n",
      "    - -1359.7449683295868\n",
      "    - -887.7843213536712\n",
      "    - -1074.6284746267015\n",
      "    - -894.8763571538504\n",
      "    - -1291.351343927059\n",
      "    - -1649.9277290043065\n",
      "    - -1615.0344178282305\n",
      "    - -998.3712884852137\n",
      "    - -897.5245018444549\n",
      "    - -1498.040512913482\n",
      "    - -1298.5067570425508\n",
      "    - -1459.022948613478\n",
      "    - -1295.2904403276727\n",
      "    - -1324.3481312860938\n",
      "    - -912.5604495397141\n",
      "    - -873.1155185933668\n",
      "    - -1455.3550309824439\n",
      "    - -1207.9991067522478\n",
      "    - -1146.6347604144505\n",
      "    - -889.3213826619135\n",
      "    - -965.289992598452\n",
      "    - -1080.8071659628383\n",
      "    - -1625.4550475682474\n",
      "    - -1654.7630076216994\n",
      "    - -1113.6820014953644\n",
      "    - -973.3541229390318\n",
      "    - -889.5807405500692\n",
      "    - -882.0209352777715\n",
      "    - -1017.8774465654021\n",
      "    - -894.4398151287013\n",
      "    - -1645.4330576107184\n",
      "    - -892.7045029667809\n",
      "    - -1175.990852392987\n",
      "    - -1295.753410873304\n",
      "    - -1420.006433524892\n",
      "    - -870.862579859964\n",
      "    - -1024.6180672112591\n",
      "    - -1354.5219372280392\n",
      "    - -1586.0347576188865\n",
      "    - -890.2344549259167\n",
      "    - -969.6598612403997\n",
      "    - -976.5861117808253\n",
      "    - -888.0891255679197\n",
      "    - -888.3034193486543\n",
      "    - -889.4690132936356\n",
      "    - -893.5705495561834\n",
      "    - -1526.2987810077857\n",
      "    - -1203.2020069089729\n",
      "    - -1043.5002539162551\n",
      "    - -1667.0014715916434\n",
      "    - -983.5321310652981\n",
      "    - -1516.7271205743282\n",
      "    - -1519.220834908267\n",
      "    - -915.4127189773558\n",
      "    - -898.7454986627862\n",
      "    - -887.3845748286544\n",
      "    - -1067.3262451661249\n",
      "    - -886.6664585597457\n",
      "    - -1008.359344648428\n",
      "    - -1420.5788900962596\n",
      "    - -1097.2462451994363\n",
      "    - -977.1273335222984\n",
      "    - -986.9498584194138\n",
      "    - -886.2550884942871\n",
      "    - -1081.3181706533826\n",
      "    - -1679.9275325556807\n",
      "    - -1574.9161467348551\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1935020999521622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1494786570617027\n",
      "    mean_inference_ms: 0.9835055181107865\n",
      "    mean_raw_obs_processing_ms: 0.12193713925521248\n",
      "time_since_restore: 1144.7164793014526\n",
      "time_this_iter_s: 9.574437856674194\n",
      "time_total_s: 1144.7164793014526\n",
      "timers:\n",
      "  learn_throughput: 958.417\n",
      "  learn_time_ms: 4173.547\n",
      "  load_throughput: 17641657.203\n",
      "  load_time_ms: 0.227\n",
      "  training_iteration_time_ms: 9826.007\n",
      "  update_time_ms: 2.277\n",
      "timestamp: 1660565067\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 436000\n",
      "training_iteration: 109\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 440000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 440000\n",
      "  num_agent_steps_trained: 440000\n",
      "  num_env_steps_sampled: 440000\n",
      "  num_env_steps_trained: 440000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-04-36\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -771.5146296371817\n",
      "episode_reward_mean: -1136.4524539632296\n",
      "episode_reward_min: -1712.7389585199792\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2200\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6842092275619507\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6700344681739807\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015613588504493237\n",
      "        model: {}\n",
      "        policy_loss: 0.015565820038318634\n",
      "        total_loss: 9.838133811950684\n",
      "        vf_explained_var: -0.023564163595438004\n",
      "        vf_loss: 9.811884880065918\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 440000\n",
      "  num_agent_steps_trained: 440000\n",
      "  num_env_steps_sampled: 440000\n",
      "  num_env_steps_trained: 440000\n",
      "iterations_since_restore: 110\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 440000\n",
      "num_agent_steps_trained: 440000\n",
      "num_env_steps_sampled: 440000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 440000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.153846153846146\n",
      "  ram_util_percent: 59.95384615384614\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1933872167732669\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14941581134373738\n",
      "  mean_inference_ms: 0.9829871367461416\n",
      "  mean_raw_obs_processing_ms: 0.12187522634784304\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -771.5146296371817\n",
      "  episode_reward_mean: -1136.4524539632296\n",
      "  episode_reward_min: -1712.7389585199792\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -883.4626623739798\n",
      "    - -1422.9207823779116\n",
      "    - -1004.565043878547\n",
      "    - -898.018712930283\n",
      "    - -1616.0992003330616\n",
      "    - -1712.7389585199792\n",
      "    - -1308.1070555880074\n",
      "    - -1019.9180277120048\n",
      "    - -1080.986543361857\n",
      "    - -955.2666581905783\n",
      "    - -887.6065550827565\n",
      "    - -1085.359776709376\n",
      "    - -894.1467875345668\n",
      "    - -1359.7449683295868\n",
      "    - -887.7843213536712\n",
      "    - -1074.6284746267015\n",
      "    - -894.8763571538504\n",
      "    - -1291.351343927059\n",
      "    - -1649.9277290043065\n",
      "    - -1615.0344178282305\n",
      "    - -998.3712884852137\n",
      "    - -897.5245018444549\n",
      "    - -1498.040512913482\n",
      "    - -1298.5067570425508\n",
      "    - -1459.022948613478\n",
      "    - -1295.2904403276727\n",
      "    - -1324.3481312860938\n",
      "    - -912.5604495397141\n",
      "    - -873.1155185933668\n",
      "    - -1455.3550309824439\n",
      "    - -1207.9991067522478\n",
      "    - -1146.6347604144505\n",
      "    - -889.3213826619135\n",
      "    - -965.289992598452\n",
      "    - -1080.8071659628383\n",
      "    - -1625.4550475682474\n",
      "    - -1654.7630076216994\n",
      "    - -1113.6820014953644\n",
      "    - -973.3541229390318\n",
      "    - -889.5807405500692\n",
      "    - -882.0209352777715\n",
      "    - -1017.8774465654021\n",
      "    - -894.4398151287013\n",
      "    - -1645.4330576107184\n",
      "    - -892.7045029667809\n",
      "    - -1175.990852392987\n",
      "    - -1295.753410873304\n",
      "    - -1420.006433524892\n",
      "    - -870.862579859964\n",
      "    - -1024.6180672112591\n",
      "    - -1354.5219372280392\n",
      "    - -1586.0347576188865\n",
      "    - -890.2344549259167\n",
      "    - -969.6598612403997\n",
      "    - -976.5861117808253\n",
      "    - -888.0891255679197\n",
      "    - -888.3034193486543\n",
      "    - -889.4690132936356\n",
      "    - -893.5705495561834\n",
      "    - -1526.2987810077857\n",
      "    - -1203.2020069089729\n",
      "    - -1043.5002539162551\n",
      "    - -1667.0014715916434\n",
      "    - -983.5321310652981\n",
      "    - -1516.7271205743282\n",
      "    - -1519.220834908267\n",
      "    - -915.4127189773558\n",
      "    - -898.7454986627862\n",
      "    - -887.3845748286544\n",
      "    - -1067.3262451661249\n",
      "    - -886.6664585597457\n",
      "    - -1008.359344648428\n",
      "    - -1420.5788900962596\n",
      "    - -1097.2462451994363\n",
      "    - -977.1273335222984\n",
      "    - -986.9498584194138\n",
      "    - -886.2550884942871\n",
      "    - -1081.3181706533826\n",
      "    - -1679.9275325556807\n",
      "    - -1574.9161467348551\n",
      "    - -1257.1258490224263\n",
      "    - -1020.2325427349944\n",
      "    - -887.1850462877995\n",
      "    - -892.5642638513211\n",
      "    - -886.0396415666222\n",
      "    - -893.6166146482201\n",
      "    - -1447.9657477770786\n",
      "    - -1438.223841409851\n",
      "    - -883.2966988973515\n",
      "    - -970.6602857488673\n",
      "    - -889.3142119149095\n",
      "    - -1416.6462208271557\n",
      "    - -795.5194428924525\n",
      "    - -771.5146296371817\n",
      "    - -988.7439857017439\n",
      "    - -1000.6628428887102\n",
      "    - -1556.2610082069384\n",
      "    - -1067.0173660134283\n",
      "    - -1303.0534187172884\n",
      "    - -894.161414106003\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1933872167732669\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14941581134373738\n",
      "    mean_inference_ms: 0.9829871367461416\n",
      "    mean_raw_obs_processing_ms: 0.12187522634784304\n",
      "time_since_restore: 1154.1743524074554\n",
      "time_this_iter_s: 9.457873106002808\n",
      "time_total_s: 1154.1743524074554\n",
      "timers:\n",
      "  learn_throughput: 958.898\n",
      "  learn_time_ms: 4171.455\n",
      "  load_throughput: 17595402.202\n",
      "  load_time_ms: 0.227\n",
      "  training_iteration_time_ms: 9815.929\n",
      "  update_time_ms: 2.293\n",
      "timestamp: 1660565076\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 440000\n",
      "training_iteration: 110\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 444000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 444000\n",
      "  num_agent_steps_trained: 444000\n",
      "  num_env_steps_sampled: 444000\n",
      "  num_env_steps_trained: 444000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-04-46\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -771.5146296371817\n",
      "episode_reward_mean: -1152.5804598647674\n",
      "episode_reward_min: -1691.1983920353687\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2220\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6842092275619507\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.996535301208496\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013948515988886356\n",
      "        model: {}\n",
      "        policy_loss: 0.009874220937490463\n",
      "        total_loss: 9.841814041137695\n",
      "        vf_explained_var: -0.0110230827704072\n",
      "        vf_loss: 9.822396278381348\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 444000\n",
      "  num_agent_steps_trained: 444000\n",
      "  num_env_steps_sampled: 444000\n",
      "  num_env_steps_trained: 444000\n",
      "iterations_since_restore: 111\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 444000\n",
      "num_agent_steps_trained: 444000\n",
      "num_env_steps_sampled: 444000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 444000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.66428571428572\n",
      "  ram_util_percent: 59.921428571428564\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1932647882731733\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14934907078716206\n",
      "  mean_inference_ms: 0.9824185099760336\n",
      "  mean_raw_obs_processing_ms: 0.12180987067730883\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -771.5146296371817\n",
      "  episode_reward_mean: -1152.5804598647674\n",
      "  episode_reward_min: -1691.1983920353687\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -998.3712884852137\n",
      "    - -897.5245018444549\n",
      "    - -1498.040512913482\n",
      "    - -1298.5067570425508\n",
      "    - -1459.022948613478\n",
      "    - -1295.2904403276727\n",
      "    - -1324.3481312860938\n",
      "    - -912.5604495397141\n",
      "    - -873.1155185933668\n",
      "    - -1455.3550309824439\n",
      "    - -1207.9991067522478\n",
      "    - -1146.6347604144505\n",
      "    - -889.3213826619135\n",
      "    - -965.289992598452\n",
      "    - -1080.8071659628383\n",
      "    - -1625.4550475682474\n",
      "    - -1654.7630076216994\n",
      "    - -1113.6820014953644\n",
      "    - -973.3541229390318\n",
      "    - -889.5807405500692\n",
      "    - -882.0209352777715\n",
      "    - -1017.8774465654021\n",
      "    - -894.4398151287013\n",
      "    - -1645.4330576107184\n",
      "    - -892.7045029667809\n",
      "    - -1175.990852392987\n",
      "    - -1295.753410873304\n",
      "    - -1420.006433524892\n",
      "    - -870.862579859964\n",
      "    - -1024.6180672112591\n",
      "    - -1354.5219372280392\n",
      "    - -1586.0347576188865\n",
      "    - -890.2344549259167\n",
      "    - -969.6598612403997\n",
      "    - -976.5861117808253\n",
      "    - -888.0891255679197\n",
      "    - -888.3034193486543\n",
      "    - -889.4690132936356\n",
      "    - -893.5705495561834\n",
      "    - -1526.2987810077857\n",
      "    - -1203.2020069089729\n",
      "    - -1043.5002539162551\n",
      "    - -1667.0014715916434\n",
      "    - -983.5321310652981\n",
      "    - -1516.7271205743282\n",
      "    - -1519.220834908267\n",
      "    - -915.4127189773558\n",
      "    - -898.7454986627862\n",
      "    - -887.3845748286544\n",
      "    - -1067.3262451661249\n",
      "    - -886.6664585597457\n",
      "    - -1008.359344648428\n",
      "    - -1420.5788900962596\n",
      "    - -1097.2462451994363\n",
      "    - -977.1273335222984\n",
      "    - -986.9498584194138\n",
      "    - -886.2550884942871\n",
      "    - -1081.3181706533826\n",
      "    - -1679.9275325556807\n",
      "    - -1574.9161467348551\n",
      "    - -1257.1258490224263\n",
      "    - -1020.2325427349944\n",
      "    - -887.1850462877995\n",
      "    - -892.5642638513211\n",
      "    - -886.0396415666222\n",
      "    - -893.6166146482201\n",
      "    - -1447.9657477770786\n",
      "    - -1438.223841409851\n",
      "    - -883.2966988973515\n",
      "    - -970.6602857488673\n",
      "    - -889.3142119149095\n",
      "    - -1416.6462208271557\n",
      "    - -795.5194428924525\n",
      "    - -771.5146296371817\n",
      "    - -988.7439857017439\n",
      "    - -1000.6628428887102\n",
      "    - -1556.2610082069384\n",
      "    - -1067.0173660134283\n",
      "    - -1303.0534187172884\n",
      "    - -894.161414106003\n",
      "    - -1691.1983920353687\n",
      "    - -896.8605549407075\n",
      "    - -1106.4476170249022\n",
      "    - -1378.3850539932316\n",
      "    - -1590.7089913697484\n",
      "    - -1067.4694741875692\n",
      "    - -891.2889888454915\n",
      "    - -1536.3603447983153\n",
      "    - -1027.2811078693876\n",
      "    - -1276.0068874269555\n",
      "    - -1544.906954927298\n",
      "    - -911.9440950283889\n",
      "    - -1367.164018749659\n",
      "    - -1621.4319987285137\n",
      "    - -1595.6517524039089\n",
      "    - -967.0835312107333\n",
      "    - -966.5593610496265\n",
      "    - -1505.4267710088648\n",
      "    - -889.9447206327737\n",
      "    - -1323.224350738679\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1932647882731733\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14934907078716206\n",
      "    mean_inference_ms: 0.9824185099760336\n",
      "    mean_raw_obs_processing_ms: 0.12180987067730883\n",
      "time_since_restore: 1163.719465970993\n",
      "time_this_iter_s: 9.545113563537598\n",
      "time_total_s: 1163.719465970993\n",
      "timers:\n",
      "  learn_throughput: 958.49\n",
      "  learn_time_ms: 4173.23\n",
      "  load_throughput: 17755546.619\n",
      "  load_time_ms: 0.225\n",
      "  training_iteration_time_ms: 9812.501\n",
      "  update_time_ms: 2.296\n",
      "timestamp: 1660565086\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 444000\n",
      "training_iteration: 111\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 448000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 448000\n",
      "  num_agent_steps_trained: 448000\n",
      "  num_env_steps_sampled: 448000\n",
      "  num_env_steps_trained: 448000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-04-55\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -771.5146296371817\n",
      "episode_reward_mean: -1124.3399943902723\n",
      "episode_reward_min: -1691.1983920353687\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2240\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6842092275619507\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.049590565264225006\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009103466756641865\n",
      "        model: {}\n",
      "        policy_loss: 0.010485338047146797\n",
      "        total_loss: 9.775304794311523\n",
      "        vf_explained_var: -0.01941865310072899\n",
      "        vf_loss: 9.758591651916504\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 448000\n",
      "  num_agent_steps_trained: 448000\n",
      "  num_env_steps_sampled: 448000\n",
      "  num_env_steps_trained: 448000\n",
      "iterations_since_restore: 112\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 448000\n",
      "num_agent_steps_trained: 448000\n",
      "num_env_steps_sampled: 448000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 448000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.38461538461539\n",
      "  ram_util_percent: 59.99230769230769\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19313632634996836\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1492704987125545\n",
      "  mean_inference_ms: 0.981766377083737\n",
      "  mean_raw_obs_processing_ms: 0.12173707333906637\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -771.5146296371817\n",
      "  episode_reward_mean: -1124.3399943902723\n",
      "  episode_reward_min: -1691.1983920353687\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -882.0209352777715\n",
      "    - -1017.8774465654021\n",
      "    - -894.4398151287013\n",
      "    - -1645.4330576107184\n",
      "    - -892.7045029667809\n",
      "    - -1175.990852392987\n",
      "    - -1295.753410873304\n",
      "    - -1420.006433524892\n",
      "    - -870.862579859964\n",
      "    - -1024.6180672112591\n",
      "    - -1354.5219372280392\n",
      "    - -1586.0347576188865\n",
      "    - -890.2344549259167\n",
      "    - -969.6598612403997\n",
      "    - -976.5861117808253\n",
      "    - -888.0891255679197\n",
      "    - -888.3034193486543\n",
      "    - -889.4690132936356\n",
      "    - -893.5705495561834\n",
      "    - -1526.2987810077857\n",
      "    - -1203.2020069089729\n",
      "    - -1043.5002539162551\n",
      "    - -1667.0014715916434\n",
      "    - -983.5321310652981\n",
      "    - -1516.7271205743282\n",
      "    - -1519.220834908267\n",
      "    - -915.4127189773558\n",
      "    - -898.7454986627862\n",
      "    - -887.3845748286544\n",
      "    - -1067.3262451661249\n",
      "    - -886.6664585597457\n",
      "    - -1008.359344648428\n",
      "    - -1420.5788900962596\n",
      "    - -1097.2462451994363\n",
      "    - -977.1273335222984\n",
      "    - -986.9498584194138\n",
      "    - -886.2550884942871\n",
      "    - -1081.3181706533826\n",
      "    - -1679.9275325556807\n",
      "    - -1574.9161467348551\n",
      "    - -1257.1258490224263\n",
      "    - -1020.2325427349944\n",
      "    - -887.1850462877995\n",
      "    - -892.5642638513211\n",
      "    - -886.0396415666222\n",
      "    - -893.6166146482201\n",
      "    - -1447.9657477770786\n",
      "    - -1438.223841409851\n",
      "    - -883.2966988973515\n",
      "    - -970.6602857488673\n",
      "    - -889.3142119149095\n",
      "    - -1416.6462208271557\n",
      "    - -795.5194428924525\n",
      "    - -771.5146296371817\n",
      "    - -988.7439857017439\n",
      "    - -1000.6628428887102\n",
      "    - -1556.2610082069384\n",
      "    - -1067.0173660134283\n",
      "    - -1303.0534187172884\n",
      "    - -894.161414106003\n",
      "    - -1691.1983920353687\n",
      "    - -896.8605549407075\n",
      "    - -1106.4476170249022\n",
      "    - -1378.3850539932316\n",
      "    - -1590.7089913697484\n",
      "    - -1067.4694741875692\n",
      "    - -891.2889888454915\n",
      "    - -1536.3603447983153\n",
      "    - -1027.2811078693876\n",
      "    - -1276.0068874269555\n",
      "    - -1544.906954927298\n",
      "    - -911.9440950283889\n",
      "    - -1367.164018749659\n",
      "    - -1621.4319987285137\n",
      "    - -1595.6517524039089\n",
      "    - -967.0835312107333\n",
      "    - -966.5593610496265\n",
      "    - -1505.4267710088648\n",
      "    - -889.9447206327737\n",
      "    - -1323.224350738679\n",
      "    - -1257.3909267135225\n",
      "    - -990.5009134828477\n",
      "    - -978.4303172692346\n",
      "    - -966.6945231138899\n",
      "    - -889.6776159022994\n",
      "    - -893.9087076527937\n",
      "    - -1358.6293752845756\n",
      "    - -975.9375859275705\n",
      "    - -890.2408000523777\n",
      "    - -983.4437335589865\n",
      "    - -977.6966286120153\n",
      "    - -987.8924047032045\n",
      "    - -993.1608175126524\n",
      "    - -895.4419306661403\n",
      "    - -1327.5152811193725\n",
      "    - -1077.2992037433519\n",
      "    - -969.9233982138206\n",
      "    - -890.6227822629132\n",
      "    - -882.9194139499177\n",
      "    - -1547.6500010017592\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19313632634996836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1492704987125545\n",
      "    mean_inference_ms: 0.981766377083737\n",
      "    mean_raw_obs_processing_ms: 0.12173707333906637\n",
      "time_since_restore: 1173.2399604320526\n",
      "time_this_iter_s: 9.52049446105957\n",
      "time_total_s: 1173.2399604320526\n",
      "timers:\n",
      "  learn_throughput: 968.146\n",
      "  learn_time_ms: 4131.608\n",
      "  load_throughput: 18414242.125\n",
      "  load_time_ms: 0.217\n",
      "  training_iteration_time_ms: 9632.401\n",
      "  update_time_ms: 2.254\n",
      "timestamp: 1660565095\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 448000\n",
      "training_iteration: 112\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 452000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 452000\n",
      "  num_agent_steps_trained: 452000\n",
      "  num_env_steps_sampled: 452000\n",
      "  num_env_steps_trained: 452000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-05-06\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -771.5146296371817\n",
      "episode_reward_mean: -1126.4174600335618\n",
      "episode_reward_min: -1691.1983920353687\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2260\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.6842092275619507\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0795173645019531\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020286638289690018\n",
      "        model: {}\n",
      "        policy_loss: 0.01450300496071577\n",
      "        total_loss: 9.83443546295166\n",
      "        vf_explained_var: -0.02194790728390217\n",
      "        vf_loss: 9.806052207946777\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 452000\n",
      "  num_agent_steps_trained: 452000\n",
      "  num_env_steps_sampled: 452000\n",
      "  num_env_steps_trained: 452000\n",
      "iterations_since_restore: 113\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 452000\n",
      "num_agent_steps_trained: 452000\n",
      "num_env_steps_sampled: 452000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 452000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 61.3875\n",
      "  ram_util_percent: 60.0125\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1930144298892526\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14920001115172965\n",
      "  mean_inference_ms: 0.98118287065764\n",
      "  mean_raw_obs_processing_ms: 0.12167208527267441\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -771.5146296371817\n",
      "  episode_reward_mean: -1126.4174600335618\n",
      "  episode_reward_min: -1691.1983920353687\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1203.2020069089729\n",
      "    - -1043.5002539162551\n",
      "    - -1667.0014715916434\n",
      "    - -983.5321310652981\n",
      "    - -1516.7271205743282\n",
      "    - -1519.220834908267\n",
      "    - -915.4127189773558\n",
      "    - -898.7454986627862\n",
      "    - -887.3845748286544\n",
      "    - -1067.3262451661249\n",
      "    - -886.6664585597457\n",
      "    - -1008.359344648428\n",
      "    - -1420.5788900962596\n",
      "    - -1097.2462451994363\n",
      "    - -977.1273335222984\n",
      "    - -986.9498584194138\n",
      "    - -886.2550884942871\n",
      "    - -1081.3181706533826\n",
      "    - -1679.9275325556807\n",
      "    - -1574.9161467348551\n",
      "    - -1257.1258490224263\n",
      "    - -1020.2325427349944\n",
      "    - -887.1850462877995\n",
      "    - -892.5642638513211\n",
      "    - -886.0396415666222\n",
      "    - -893.6166146482201\n",
      "    - -1447.9657477770786\n",
      "    - -1438.223841409851\n",
      "    - -883.2966988973515\n",
      "    - -970.6602857488673\n",
      "    - -889.3142119149095\n",
      "    - -1416.6462208271557\n",
      "    - -795.5194428924525\n",
      "    - -771.5146296371817\n",
      "    - -988.7439857017439\n",
      "    - -1000.6628428887102\n",
      "    - -1556.2610082069384\n",
      "    - -1067.0173660134283\n",
      "    - -1303.0534187172884\n",
      "    - -894.161414106003\n",
      "    - -1691.1983920353687\n",
      "    - -896.8605549407075\n",
      "    - -1106.4476170249022\n",
      "    - -1378.3850539932316\n",
      "    - -1590.7089913697484\n",
      "    - -1067.4694741875692\n",
      "    - -891.2889888454915\n",
      "    - -1536.3603447983153\n",
      "    - -1027.2811078693876\n",
      "    - -1276.0068874269555\n",
      "    - -1544.906954927298\n",
      "    - -911.9440950283889\n",
      "    - -1367.164018749659\n",
      "    - -1621.4319987285137\n",
      "    - -1595.6517524039089\n",
      "    - -967.0835312107333\n",
      "    - -966.5593610496265\n",
      "    - -1505.4267710088648\n",
      "    - -889.9447206327737\n",
      "    - -1323.224350738679\n",
      "    - -1257.3909267135225\n",
      "    - -990.5009134828477\n",
      "    - -978.4303172692346\n",
      "    - -966.6945231138899\n",
      "    - -889.6776159022994\n",
      "    - -893.9087076527937\n",
      "    - -1358.6293752845756\n",
      "    - -975.9375859275705\n",
      "    - -890.2408000523777\n",
      "    - -983.4437335589865\n",
      "    - -977.6966286120153\n",
      "    - -987.8924047032045\n",
      "    - -993.1608175126524\n",
      "    - -895.4419306661403\n",
      "    - -1327.5152811193725\n",
      "    - -1077.2992037433519\n",
      "    - -969.9233982138206\n",
      "    - -890.6227822629132\n",
      "    - -882.9194139499177\n",
      "    - -1547.6500010017592\n",
      "    - -1094.2216368332863\n",
      "    - -1568.3016216109013\n",
      "    - -887.5953997698767\n",
      "    - -1006.123914887769\n",
      "    - -879.610268970825\n",
      "    - -855.4081265253886\n",
      "    - -889.7452284185109\n",
      "    - -896.4648142978223\n",
      "    - -1608.4524190275592\n",
      "    - -1067.3892570786031\n",
      "    - -865.3417934548936\n",
      "    - -1614.6041155432945\n",
      "    - -1637.363454551549\n",
      "    - -1063.2399642422947\n",
      "    - -1094.7459711914557\n",
      "    - -1394.624699956621\n",
      "    - -889.5714066232015\n",
      "    - -890.3751952761811\n",
      "    - -986.5860373744455\n",
      "    - -1000.4563516745094\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1930144298892526\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14920001115172965\n",
      "    mean_inference_ms: 0.98118287065764\n",
      "    mean_raw_obs_processing_ms: 0.12167208527267441\n",
      "time_since_restore: 1184.3056647777557\n",
      "time_this_iter_s: 11.065704345703125\n",
      "time_total_s: 1184.3056647777557\n",
      "timers:\n",
      "  learn_throughput: 937.512\n",
      "  learn_time_ms: 4266.611\n",
      "  load_throughput: 18301751.936\n",
      "  load_time_ms: 0.219\n",
      "  training_iteration_time_ms: 9783.736\n",
      "  update_time_ms: 2.363\n",
      "timestamp: 1660565106\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 452000\n",
      "training_iteration: 113\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 456000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 456000\n",
      "  num_agent_steps_trained: 456000\n",
      "  num_env_steps_sampled: 456000\n",
      "  num_env_steps_trained: 456000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-05-18\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -771.5146296371817\n",
      "episode_reward_mean: -1109.9124123695908\n",
      "episode_reward_min: -1691.1983920353687\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2280\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0263137817382812\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5874055027961731\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005615829024463892\n",
      "        model: {}\n",
      "        policy_loss: 0.008990480564534664\n",
      "        total_loss: 9.831293106079102\n",
      "        vf_explained_var: -0.024900231510400772\n",
      "        vf_loss: 9.81653881072998\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 456000\n",
      "  num_agent_steps_trained: 456000\n",
      "  num_env_steps_sampled: 456000\n",
      "  num_env_steps_trained: 456000\n",
      "iterations_since_restore: 114\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 456000\n",
      "num_agent_steps_trained: 456000\n",
      "num_env_steps_sampled: 456000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 456000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 65.4235294117647\n",
      "  ram_util_percent: 60.44117647058823\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1929728187193519\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14917691680209677\n",
      "  mean_inference_ms: 0.9809748666483764\n",
      "  mean_raw_obs_processing_ms: 0.12164608342039367\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -771.5146296371817\n",
      "  episode_reward_mean: -1109.9124123695908\n",
      "  episode_reward_min: -1691.1983920353687\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1257.1258490224263\n",
      "    - -1020.2325427349944\n",
      "    - -887.1850462877995\n",
      "    - -892.5642638513211\n",
      "    - -886.0396415666222\n",
      "    - -893.6166146482201\n",
      "    - -1447.9657477770786\n",
      "    - -1438.223841409851\n",
      "    - -883.2966988973515\n",
      "    - -970.6602857488673\n",
      "    - -889.3142119149095\n",
      "    - -1416.6462208271557\n",
      "    - -795.5194428924525\n",
      "    - -771.5146296371817\n",
      "    - -988.7439857017439\n",
      "    - -1000.6628428887102\n",
      "    - -1556.2610082069384\n",
      "    - -1067.0173660134283\n",
      "    - -1303.0534187172884\n",
      "    - -894.161414106003\n",
      "    - -1691.1983920353687\n",
      "    - -896.8605549407075\n",
      "    - -1106.4476170249022\n",
      "    - -1378.3850539932316\n",
      "    - -1590.7089913697484\n",
      "    - -1067.4694741875692\n",
      "    - -891.2889888454915\n",
      "    - -1536.3603447983153\n",
      "    - -1027.2811078693876\n",
      "    - -1276.0068874269555\n",
      "    - -1544.906954927298\n",
      "    - -911.9440950283889\n",
      "    - -1367.164018749659\n",
      "    - -1621.4319987285137\n",
      "    - -1595.6517524039089\n",
      "    - -967.0835312107333\n",
      "    - -966.5593610496265\n",
      "    - -1505.4267710088648\n",
      "    - -889.9447206327737\n",
      "    - -1323.224350738679\n",
      "    - -1257.3909267135225\n",
      "    - -990.5009134828477\n",
      "    - -978.4303172692346\n",
      "    - -966.6945231138899\n",
      "    - -889.6776159022994\n",
      "    - -893.9087076527937\n",
      "    - -1358.6293752845756\n",
      "    - -975.9375859275705\n",
      "    - -890.2408000523777\n",
      "    - -983.4437335589865\n",
      "    - -977.6966286120153\n",
      "    - -987.8924047032045\n",
      "    - -993.1608175126524\n",
      "    - -895.4419306661403\n",
      "    - -1327.5152811193725\n",
      "    - -1077.2992037433519\n",
      "    - -969.9233982138206\n",
      "    - -890.6227822629132\n",
      "    - -882.9194139499177\n",
      "    - -1547.6500010017592\n",
      "    - -1094.2216368332863\n",
      "    - -1568.3016216109013\n",
      "    - -887.5953997698767\n",
      "    - -1006.123914887769\n",
      "    - -879.610268970825\n",
      "    - -855.4081265253886\n",
      "    - -889.7452284185109\n",
      "    - -896.4648142978223\n",
      "    - -1608.4524190275592\n",
      "    - -1067.3892570786031\n",
      "    - -865.3417934548936\n",
      "    - -1614.6041155432945\n",
      "    - -1637.363454551549\n",
      "    - -1063.2399642422947\n",
      "    - -1094.7459711914557\n",
      "    - -1394.624699956621\n",
      "    - -889.5714066232015\n",
      "    - -890.3751952761811\n",
      "    - -986.5860373744455\n",
      "    - -1000.4563516745094\n",
      "    - -1581.759570173398\n",
      "    - -985.7821133115136\n",
      "    - -1067.3228692171938\n",
      "    - -1690.6326019031153\n",
      "    - -928.3269067041224\n",
      "    - -1173.0459137335895\n",
      "    - -1326.0319540567145\n",
      "    - -890.3860817151776\n",
      "    - -892.0181025705448\n",
      "    - -1196.29169472981\n",
      "    - -910.7095798421278\n",
      "    - -886.3048181527125\n",
      "    - -975.015153544144\n",
      "    - -1166.6615447888935\n",
      "    - -1008.4574349036296\n",
      "    - -981.4239699678562\n",
      "    - -890.6757110743216\n",
      "    - -1223.7419882720465\n",
      "    - -894.854758039217\n",
      "    - -981.4503923862503\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1929728187193519\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14917691680209677\n",
      "    mean_inference_ms: 0.9809748666483764\n",
      "    mean_raw_obs_processing_ms: 0.12164608342039367\n",
      "time_since_restore: 1195.6463561058044\n",
      "time_this_iter_s: 11.340691328048706\n",
      "time_total_s: 1195.6463561058044\n",
      "timers:\n",
      "  learn_throughput: 924.634\n",
      "  learn_time_ms: 4326.035\n",
      "  load_throughput: 17806427.51\n",
      "  load_time_ms: 0.225\n",
      "  training_iteration_time_ms: 9971.158\n",
      "  update_time_ms: 2.362\n",
      "timestamp: 1660565118\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 456000\n",
      "training_iteration: 114\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 460000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 460000\n",
      "  num_agent_steps_trained: 460000\n",
      "  num_env_steps_sampled: 460000\n",
      "  num_env_steps_trained: 460000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-05-28\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -855.4081265253886\n",
      "episode_reward_mean: -1127.084020033562\n",
      "episode_reward_min: -1696.6135507702809\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2300\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0263137817382812\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4521455764770508\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008057884871959686\n",
      "        model: {}\n",
      "        policy_loss: 0.009006098844110966\n",
      "        total_loss: 9.784958839416504\n",
      "        vf_explained_var: -0.028733758255839348\n",
      "        vf_loss: 9.767683029174805\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 460000\n",
      "  num_agent_steps_trained: 460000\n",
      "  num_env_steps_sampled: 460000\n",
      "  num_env_steps_trained: 460000\n",
      "iterations_since_restore: 115\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 460000\n",
      "num_agent_steps_trained: 460000\n",
      "num_env_steps_sampled: 460000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 460000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.885714285714286\n",
      "  ram_util_percent: 60.421428571428564\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1929450963109829\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1491670050568574\n",
      "  mean_inference_ms: 0.980881776644425\n",
      "  mean_raw_obs_processing_ms: 0.12162887683414408\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -855.4081265253886\n",
      "  episode_reward_mean: -1127.084020033562\n",
      "  episode_reward_min: -1696.6135507702809\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1691.1983920353687\n",
      "    - -896.8605549407075\n",
      "    - -1106.4476170249022\n",
      "    - -1378.3850539932316\n",
      "    - -1590.7089913697484\n",
      "    - -1067.4694741875692\n",
      "    - -891.2889888454915\n",
      "    - -1536.3603447983153\n",
      "    - -1027.2811078693876\n",
      "    - -1276.0068874269555\n",
      "    - -1544.906954927298\n",
      "    - -911.9440950283889\n",
      "    - -1367.164018749659\n",
      "    - -1621.4319987285137\n",
      "    - -1595.6517524039089\n",
      "    - -967.0835312107333\n",
      "    - -966.5593610496265\n",
      "    - -1505.4267710088648\n",
      "    - -889.9447206327737\n",
      "    - -1323.224350738679\n",
      "    - -1257.3909267135225\n",
      "    - -990.5009134828477\n",
      "    - -978.4303172692346\n",
      "    - -966.6945231138899\n",
      "    - -889.6776159022994\n",
      "    - -893.9087076527937\n",
      "    - -1358.6293752845756\n",
      "    - -975.9375859275705\n",
      "    - -890.2408000523777\n",
      "    - -983.4437335589865\n",
      "    - -977.6966286120153\n",
      "    - -987.8924047032045\n",
      "    - -993.1608175126524\n",
      "    - -895.4419306661403\n",
      "    - -1327.5152811193725\n",
      "    - -1077.2992037433519\n",
      "    - -969.9233982138206\n",
      "    - -890.6227822629132\n",
      "    - -882.9194139499177\n",
      "    - -1547.6500010017592\n",
      "    - -1094.2216368332863\n",
      "    - -1568.3016216109013\n",
      "    - -887.5953997698767\n",
      "    - -1006.123914887769\n",
      "    - -879.610268970825\n",
      "    - -855.4081265253886\n",
      "    - -889.7452284185109\n",
      "    - -896.4648142978223\n",
      "    - -1608.4524190275592\n",
      "    - -1067.3892570786031\n",
      "    - -865.3417934548936\n",
      "    - -1614.6041155432945\n",
      "    - -1637.363454551549\n",
      "    - -1063.2399642422947\n",
      "    - -1094.7459711914557\n",
      "    - -1394.624699956621\n",
      "    - -889.5714066232015\n",
      "    - -890.3751952761811\n",
      "    - -986.5860373744455\n",
      "    - -1000.4563516745094\n",
      "    - -1581.759570173398\n",
      "    - -985.7821133115136\n",
      "    - -1067.3228692171938\n",
      "    - -1690.6326019031153\n",
      "    - -928.3269067041224\n",
      "    - -1173.0459137335895\n",
      "    - -1326.0319540567145\n",
      "    - -890.3860817151776\n",
      "    - -892.0181025705448\n",
      "    - -1196.29169472981\n",
      "    - -910.7095798421278\n",
      "    - -886.3048181527125\n",
      "    - -975.015153544144\n",
      "    - -1166.6615447888935\n",
      "    - -1008.4574349036296\n",
      "    - -981.4239699678562\n",
      "    - -890.6757110743216\n",
      "    - -1223.7419882720465\n",
      "    - -894.854758039217\n",
      "    - -981.4503923862503\n",
      "    - -1320.0581896419449\n",
      "    - -887.8053824856585\n",
      "    - -960.1374475239072\n",
      "    - -873.8550145570591\n",
      "    - -1488.1169375150803\n",
      "    - -1398.807624641844\n",
      "    - -1067.3478589548783\n",
      "    - -889.1268080877695\n",
      "    - -987.5114221560391\n",
      "    - -915.6957253981811\n",
      "    - -1566.302335350854\n",
      "    - -1599.3250119606575\n",
      "    - -1227.1566594463059\n",
      "    - -913.9363669384437\n",
      "    - -898.9133577541713\n",
      "    - -918.8444003144317\n",
      "    - -1259.8430957981138\n",
      "    - -1069.8010016450767\n",
      "    - -1037.7676483067685\n",
      "    - -1696.6135507702809\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1929450963109829\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1491670050568574\n",
      "    mean_inference_ms: 0.980881776644425\n",
      "    mean_raw_obs_processing_ms: 0.12162887683414408\n",
      "time_since_restore: 1205.788666009903\n",
      "time_this_iter_s: 10.14230990409851\n",
      "time_total_s: 1205.788666009903\n",
      "timers:\n",
      "  learn_throughput: 918.118\n",
      "  learn_time_ms: 4356.74\n",
      "  load_throughput: 17636093.766\n",
      "  load_time_ms: 0.227\n",
      "  training_iteration_time_ms: 10012.724\n",
      "  update_time_ms: 2.388\n",
      "timestamp: 1660565128\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 460000\n",
      "training_iteration: 115\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 464000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 464000\n",
      "  num_agent_steps_trained: 464000\n",
      "  num_env_steps_sampled: 464000\n",
      "  num_env_steps_trained: 464000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-05-38\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -855.4081265253886\n",
      "episode_reward_mean: -1098.4984651867721\n",
      "episode_reward_min: -1725.6575870534139\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2320\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0263137817382812\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1324045658111572\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006460573524236679\n",
      "        model: {}\n",
      "        policy_loss: 0.008683957159519196\n",
      "        total_loss: 9.8485107421875\n",
      "        vf_explained_var: -0.02340589463710785\n",
      "        vf_loss: 9.833196640014648\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 464000\n",
      "  num_agent_steps_trained: 464000\n",
      "  num_env_steps_sampled: 464000\n",
      "  num_env_steps_trained: 464000\n",
      "iterations_since_restore: 116\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 464000\n",
      "num_agent_steps_trained: 464000\n",
      "num_env_steps_sampled: 464000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 464000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 58.36\n",
      "  ram_util_percent: 60.31333333333331\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19293913395359508\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14916598279948068\n",
      "  mean_inference_ms: 0.9808825585711974\n",
      "  mean_raw_obs_processing_ms: 0.12162014620534095\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -855.4081265253886\n",
      "  episode_reward_mean: -1098.4984651867721\n",
      "  episode_reward_min: -1725.6575870534139\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1257.3909267135225\n",
      "    - -990.5009134828477\n",
      "    - -978.4303172692346\n",
      "    - -966.6945231138899\n",
      "    - -889.6776159022994\n",
      "    - -893.9087076527937\n",
      "    - -1358.6293752845756\n",
      "    - -975.9375859275705\n",
      "    - -890.2408000523777\n",
      "    - -983.4437335589865\n",
      "    - -977.6966286120153\n",
      "    - -987.8924047032045\n",
      "    - -993.1608175126524\n",
      "    - -895.4419306661403\n",
      "    - -1327.5152811193725\n",
      "    - -1077.2992037433519\n",
      "    - -969.9233982138206\n",
      "    - -890.6227822629132\n",
      "    - -882.9194139499177\n",
      "    - -1547.6500010017592\n",
      "    - -1094.2216368332863\n",
      "    - -1568.3016216109013\n",
      "    - -887.5953997698767\n",
      "    - -1006.123914887769\n",
      "    - -879.610268970825\n",
      "    - -855.4081265253886\n",
      "    - -889.7452284185109\n",
      "    - -896.4648142978223\n",
      "    - -1608.4524190275592\n",
      "    - -1067.3892570786031\n",
      "    - -865.3417934548936\n",
      "    - -1614.6041155432945\n",
      "    - -1637.363454551549\n",
      "    - -1063.2399642422947\n",
      "    - -1094.7459711914557\n",
      "    - -1394.624699956621\n",
      "    - -889.5714066232015\n",
      "    - -890.3751952761811\n",
      "    - -986.5860373744455\n",
      "    - -1000.4563516745094\n",
      "    - -1581.759570173398\n",
      "    - -985.7821133115136\n",
      "    - -1067.3228692171938\n",
      "    - -1690.6326019031153\n",
      "    - -928.3269067041224\n",
      "    - -1173.0459137335895\n",
      "    - -1326.0319540567145\n",
      "    - -890.3860817151776\n",
      "    - -892.0181025705448\n",
      "    - -1196.29169472981\n",
      "    - -910.7095798421278\n",
      "    - -886.3048181527125\n",
      "    - -975.015153544144\n",
      "    - -1166.6615447888935\n",
      "    - -1008.4574349036296\n",
      "    - -981.4239699678562\n",
      "    - -890.6757110743216\n",
      "    - -1223.7419882720465\n",
      "    - -894.854758039217\n",
      "    - -981.4503923862503\n",
      "    - -1320.0581896419449\n",
      "    - -887.8053824856585\n",
      "    - -960.1374475239072\n",
      "    - -873.8550145570591\n",
      "    - -1488.1169375150803\n",
      "    - -1398.807624641844\n",
      "    - -1067.3478589548783\n",
      "    - -889.1268080877695\n",
      "    - -987.5114221560391\n",
      "    - -915.6957253981811\n",
      "    - -1566.302335350854\n",
      "    - -1599.3250119606575\n",
      "    - -1227.1566594463059\n",
      "    - -913.9363669384437\n",
      "    - -898.9133577541713\n",
      "    - -918.8444003144317\n",
      "    - -1259.8430957981138\n",
      "    - -1069.8010016450767\n",
      "    - -1037.7676483067685\n",
      "    - -1696.6135507702809\n",
      "    - -1320.1406810946658\n",
      "    - -978.3799500210379\n",
      "    - -995.2464701306573\n",
      "    - -889.7207369461842\n",
      "    - -1629.3158893312059\n",
      "    - -893.4335581633463\n",
      "    - -881.9656210399907\n",
      "    - -884.3185851871451\n",
      "    - -1329.1884409824477\n",
      "    - -896.2592316915209\n",
      "    - -1069.306957713362\n",
      "    - -1339.8846914479407\n",
      "    - -966.3204232818781\n",
      "    - -1056.2602362793868\n",
      "    - -1725.6575870534139\n",
      "    - -981.9750732721889\n",
      "    - -1536.5614590855241\n",
      "    - -1012.6747720973622\n",
      "    - -1010.9796640303036\n",
      "    - -899.199453441558\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19293913395359508\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14916598279948068\n",
      "    mean_inference_ms: 0.9808825585711974\n",
      "    mean_raw_obs_processing_ms: 0.12162014620534095\n",
      "time_since_restore: 1216.1307258605957\n",
      "time_this_iter_s: 10.342059850692749\n",
      "time_total_s: 1216.1307258605957\n",
      "timers:\n",
      "  learn_throughput: 914.919\n",
      "  learn_time_ms: 4371.974\n",
      "  load_throughput: 16401618.927\n",
      "  load_time_ms: 0.244\n",
      "  training_iteration_time_ms: 10040.189\n",
      "  update_time_ms: 2.344\n",
      "timestamp: 1660565138\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 464000\n",
      "training_iteration: 116\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 468000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 468000\n",
      "  num_agent_steps_trained: 468000\n",
      "  num_env_steps_sampled: 468000\n",
      "  num_env_steps_trained: 468000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-05-50\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -756.7262199447661\n",
      "episode_reward_mean: -1100.3736223045364\n",
      "episode_reward_min: -1725.6575870534139\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2340\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0263137817382812\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.537156879901886\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013678456656634808\n",
      "        model: {}\n",
      "        policy_loss: 0.011700686998665333\n",
      "        total_loss: 9.822754859924316\n",
      "        vf_explained_var: -0.01812019757926464\n",
      "        vf_loss: 9.797016143798828\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 468000\n",
      "  num_agent_steps_trained: 468000\n",
      "  num_env_steps_sampled: 468000\n",
      "  num_env_steps_trained: 468000\n",
      "iterations_since_restore: 117\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 468000\n",
      "num_agent_steps_trained: 468000\n",
      "num_env_steps_sampled: 468000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 468000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 63.28125\n",
      "  ram_util_percent: 60.568749999999994\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19298033233306927\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14919998206140042\n",
      "  mean_inference_ms: 0.9811380652073594\n",
      "  mean_raw_obs_processing_ms: 0.12164268459457628\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -756.7262199447661\n",
      "  episode_reward_mean: -1100.3736223045364\n",
      "  episode_reward_min: -1725.6575870534139\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1094.2216368332863\n",
      "    - -1568.3016216109013\n",
      "    - -887.5953997698767\n",
      "    - -1006.123914887769\n",
      "    - -879.610268970825\n",
      "    - -855.4081265253886\n",
      "    - -889.7452284185109\n",
      "    - -896.4648142978223\n",
      "    - -1608.4524190275592\n",
      "    - -1067.3892570786031\n",
      "    - -865.3417934548936\n",
      "    - -1614.6041155432945\n",
      "    - -1637.363454551549\n",
      "    - -1063.2399642422947\n",
      "    - -1094.7459711914557\n",
      "    - -1394.624699956621\n",
      "    - -889.5714066232015\n",
      "    - -890.3751952761811\n",
      "    - -986.5860373744455\n",
      "    - -1000.4563516745094\n",
      "    - -1581.759570173398\n",
      "    - -985.7821133115136\n",
      "    - -1067.3228692171938\n",
      "    - -1690.6326019031153\n",
      "    - -928.3269067041224\n",
      "    - -1173.0459137335895\n",
      "    - -1326.0319540567145\n",
      "    - -890.3860817151776\n",
      "    - -892.0181025705448\n",
      "    - -1196.29169472981\n",
      "    - -910.7095798421278\n",
      "    - -886.3048181527125\n",
      "    - -975.015153544144\n",
      "    - -1166.6615447888935\n",
      "    - -1008.4574349036296\n",
      "    - -981.4239699678562\n",
      "    - -890.6757110743216\n",
      "    - -1223.7419882720465\n",
      "    - -894.854758039217\n",
      "    - -981.4503923862503\n",
      "    - -1320.0581896419449\n",
      "    - -887.8053824856585\n",
      "    - -960.1374475239072\n",
      "    - -873.8550145570591\n",
      "    - -1488.1169375150803\n",
      "    - -1398.807624641844\n",
      "    - -1067.3478589548783\n",
      "    - -889.1268080877695\n",
      "    - -987.5114221560391\n",
      "    - -915.6957253981811\n",
      "    - -1566.302335350854\n",
      "    - -1599.3250119606575\n",
      "    - -1227.1566594463059\n",
      "    - -913.9363669384437\n",
      "    - -898.9133577541713\n",
      "    - -918.8444003144317\n",
      "    - -1259.8430957981138\n",
      "    - -1069.8010016450767\n",
      "    - -1037.7676483067685\n",
      "    - -1696.6135507702809\n",
      "    - -1320.1406810946658\n",
      "    - -978.3799500210379\n",
      "    - -995.2464701306573\n",
      "    - -889.7207369461842\n",
      "    - -1629.3158893312059\n",
      "    - -893.4335581633463\n",
      "    - -881.9656210399907\n",
      "    - -884.3185851871451\n",
      "    - -1329.1884409824477\n",
      "    - -896.2592316915209\n",
      "    - -1069.306957713362\n",
      "    - -1339.8846914479407\n",
      "    - -966.3204232818781\n",
      "    - -1056.2602362793868\n",
      "    - -1725.6575870534139\n",
      "    - -981.9750732721889\n",
      "    - -1536.5614590855241\n",
      "    - -1012.6747720973622\n",
      "    - -1010.9796640303036\n",
      "    - -899.199453441558\n",
      "    - -1314.094142392093\n",
      "    - -884.5439742299077\n",
      "    - -1105.8638076622997\n",
      "    - -892.8809787669213\n",
      "    - -886.8354593074685\n",
      "    - -889.2539114978938\n",
      "    - -1369.3584943260944\n",
      "    - -1357.6356186951773\n",
      "    - -900.2919654251918\n",
      "    - -892.9258643236634\n",
      "    - -884.837613972807\n",
      "    - -985.1596128933817\n",
      "    - -1005.4486191385646\n",
      "    - -1503.2662658244183\n",
      "    - -987.5015670486046\n",
      "    - -1056.504843793356\n",
      "    - -1188.6905171155006\n",
      "    - -756.7262199447661\n",
      "    - -1202.7543894793914\n",
      "    - -857.9182066821912\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19298033233306927\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14919998206140042\n",
      "    mean_inference_ms: 0.9811380652073594\n",
      "    mean_raw_obs_processing_ms: 0.12164268459457628\n",
      "time_since_restore: 1227.6979990005493\n",
      "time_this_iter_s: 11.567273139953613\n",
      "time_total_s: 1227.6979990005493\n",
      "timers:\n",
      "  learn_throughput: 892.553\n",
      "  learn_time_ms: 4481.525\n",
      "  load_throughput: 15167901.636\n",
      "  load_time_ms: 0.264\n",
      "  training_iteration_time_ms: 10208.039\n",
      "  update_time_ms: 2.386\n",
      "timestamp: 1660565150\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 468000\n",
      "training_iteration: 117\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 472000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 472000\n",
      "  num_agent_steps_trained: 472000\n",
      "  num_env_steps_sampled: 472000\n",
      "  num_env_steps_trained: 472000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-06-00\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -756.7262199447661\n",
      "episode_reward_mean: -1106.458230724085\n",
      "episode_reward_min: -1725.6575870534139\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2360\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0263137817382812\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3900352716445923\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01085728220641613\n",
      "        model: {}\n",
      "        policy_loss: 0.011109794490039349\n",
      "        total_loss: 9.882990837097168\n",
      "        vf_explained_var: -0.010683461092412472\n",
      "        vf_loss: 9.860737800598145\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 472000\n",
      "  num_agent_steps_trained: 472000\n",
      "  num_env_steps_sampled: 472000\n",
      "  num_env_steps_trained: 472000\n",
      "iterations_since_restore: 118\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 472000\n",
      "num_agent_steps_trained: 472000\n",
      "num_env_steps_sampled: 472000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 472000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.900000000000006\n",
      "  ram_util_percent: 60.887499999999996\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19303576963580704\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14923773128035556\n",
      "  mean_inference_ms: 0.9814529453270562\n",
      "  mean_raw_obs_processing_ms: 0.12166740445435793\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -756.7262199447661\n",
      "  episode_reward_mean: -1106.458230724085\n",
      "  episode_reward_min: -1725.6575870534139\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1581.759570173398\n",
      "    - -985.7821133115136\n",
      "    - -1067.3228692171938\n",
      "    - -1690.6326019031153\n",
      "    - -928.3269067041224\n",
      "    - -1173.0459137335895\n",
      "    - -1326.0319540567145\n",
      "    - -890.3860817151776\n",
      "    - -892.0181025705448\n",
      "    - -1196.29169472981\n",
      "    - -910.7095798421278\n",
      "    - -886.3048181527125\n",
      "    - -975.015153544144\n",
      "    - -1166.6615447888935\n",
      "    - -1008.4574349036296\n",
      "    - -981.4239699678562\n",
      "    - -890.6757110743216\n",
      "    - -1223.7419882720465\n",
      "    - -894.854758039217\n",
      "    - -981.4503923862503\n",
      "    - -1320.0581896419449\n",
      "    - -887.8053824856585\n",
      "    - -960.1374475239072\n",
      "    - -873.8550145570591\n",
      "    - -1488.1169375150803\n",
      "    - -1398.807624641844\n",
      "    - -1067.3478589548783\n",
      "    - -889.1268080877695\n",
      "    - -987.5114221560391\n",
      "    - -915.6957253981811\n",
      "    - -1566.302335350854\n",
      "    - -1599.3250119606575\n",
      "    - -1227.1566594463059\n",
      "    - -913.9363669384437\n",
      "    - -898.9133577541713\n",
      "    - -918.8444003144317\n",
      "    - -1259.8430957981138\n",
      "    - -1069.8010016450767\n",
      "    - -1037.7676483067685\n",
      "    - -1696.6135507702809\n",
      "    - -1320.1406810946658\n",
      "    - -978.3799500210379\n",
      "    - -995.2464701306573\n",
      "    - -889.7207369461842\n",
      "    - -1629.3158893312059\n",
      "    - -893.4335581633463\n",
      "    - -881.9656210399907\n",
      "    - -884.3185851871451\n",
      "    - -1329.1884409824477\n",
      "    - -896.2592316915209\n",
      "    - -1069.306957713362\n",
      "    - -1339.8846914479407\n",
      "    - -966.3204232818781\n",
      "    - -1056.2602362793868\n",
      "    - -1725.6575870534139\n",
      "    - -981.9750732721889\n",
      "    - -1536.5614590855241\n",
      "    - -1012.6747720973622\n",
      "    - -1010.9796640303036\n",
      "    - -899.199453441558\n",
      "    - -1314.094142392093\n",
      "    - -884.5439742299077\n",
      "    - -1105.8638076622997\n",
      "    - -892.8809787669213\n",
      "    - -886.8354593074685\n",
      "    - -889.2539114978938\n",
      "    - -1369.3584943260944\n",
      "    - -1357.6356186951773\n",
      "    - -900.2919654251918\n",
      "    - -892.9258643236634\n",
      "    - -884.837613972807\n",
      "    - -985.1596128933817\n",
      "    - -1005.4486191385646\n",
      "    - -1503.2662658244183\n",
      "    - -987.5015670486046\n",
      "    - -1056.504843793356\n",
      "    - -1188.6905171155006\n",
      "    - -756.7262199447661\n",
      "    - -1202.7543894793914\n",
      "    - -857.9182066821912\n",
      "    - -1048.6197373800187\n",
      "    - -1068.8198530166203\n",
      "    - -1298.7762347722755\n",
      "    - -893.3841986946359\n",
      "    - -1188.5541102179136\n",
      "    - -1619.8890464395697\n",
      "    - -983.9548480396792\n",
      "    - -1217.1721283060137\n",
      "    - -951.7144595017807\n",
      "    - -982.1914354909018\n",
      "    - -986.8618914641605\n",
      "    - -984.2556315444643\n",
      "    - -1541.5254536425232\n",
      "    - -980.1878675174929\n",
      "    - -1086.441731963236\n",
      "    - -1532.4625641311022\n",
      "    - -1179.608144563868\n",
      "    - -1381.8172240844226\n",
      "    - -894.9561848159502\n",
      "    - -977.489773677215\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19303576963580704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14923773128035556\n",
      "    mean_inference_ms: 0.9814529453270562\n",
      "    mean_raw_obs_processing_ms: 0.12166740445435793\n",
      "time_since_restore: 1238.2469589710236\n",
      "time_this_iter_s: 10.548959970474243\n",
      "time_total_s: 1238.2469589710236\n",
      "timers:\n",
      "  learn_throughput: 880.823\n",
      "  learn_time_ms: 4541.209\n",
      "  load_throughput: 15181627.002\n",
      "  load_time_ms: 0.263\n",
      "  training_iteration_time_ms: 10303.941\n",
      "  update_time_ms: 2.476\n",
      "timestamp: 1660565160\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 472000\n",
      "training_iteration: 118\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 476000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 476000\n",
      "  num_agent_steps_trained: 476000\n",
      "  num_env_steps_sampled: 476000\n",
      "  num_env_steps_trained: 476000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-06-11\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -756.7262199447661\n",
      "episode_reward_mean: -1109.2089386743557\n",
      "episode_reward_min: -1725.6575870534139\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2380\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0263137817382812\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9078611731529236\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00487714447081089\n",
      "        model: {}\n",
      "        policy_loss: 0.009112141095101833\n",
      "        total_loss: 9.73024845123291\n",
      "        vf_explained_var: -0.019459599629044533\n",
      "        vf_loss: 9.716130256652832\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 476000\n",
      "  num_agent_steps_trained: 476000\n",
      "  num_env_steps_sampled: 476000\n",
      "  num_env_steps_trained: 476000\n",
      "iterations_since_restore: 119\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 476000\n",
      "num_agent_steps_trained: 476000\n",
      "num_env_steps_sampled: 476000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 476000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.67857142857143\n",
      "  ram_util_percent: 60.64285714285715\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1930303988131251\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14923914610326394\n",
      "  mean_inference_ms: 0.9814927281655906\n",
      "  mean_raw_obs_processing_ms: 0.12166336851388096\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -756.7262199447661\n",
      "  episode_reward_mean: -1109.2089386743557\n",
      "  episode_reward_min: -1725.6575870534139\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1320.0581896419449\n",
      "    - -887.8053824856585\n",
      "    - -960.1374475239072\n",
      "    - -873.8550145570591\n",
      "    - -1488.1169375150803\n",
      "    - -1398.807624641844\n",
      "    - -1067.3478589548783\n",
      "    - -889.1268080877695\n",
      "    - -987.5114221560391\n",
      "    - -915.6957253981811\n",
      "    - -1566.302335350854\n",
      "    - -1599.3250119606575\n",
      "    - -1227.1566594463059\n",
      "    - -913.9363669384437\n",
      "    - -898.9133577541713\n",
      "    - -918.8444003144317\n",
      "    - -1259.8430957981138\n",
      "    - -1069.8010016450767\n",
      "    - -1037.7676483067685\n",
      "    - -1696.6135507702809\n",
      "    - -1320.1406810946658\n",
      "    - -978.3799500210379\n",
      "    - -995.2464701306573\n",
      "    - -889.7207369461842\n",
      "    - -1629.3158893312059\n",
      "    - -893.4335581633463\n",
      "    - -881.9656210399907\n",
      "    - -884.3185851871451\n",
      "    - -1329.1884409824477\n",
      "    - -896.2592316915209\n",
      "    - -1069.306957713362\n",
      "    - -1339.8846914479407\n",
      "    - -966.3204232818781\n",
      "    - -1056.2602362793868\n",
      "    - -1725.6575870534139\n",
      "    - -981.9750732721889\n",
      "    - -1536.5614590855241\n",
      "    - -1012.6747720973622\n",
      "    - -1010.9796640303036\n",
      "    - -899.199453441558\n",
      "    - -1314.094142392093\n",
      "    - -884.5439742299077\n",
      "    - -1105.8638076622997\n",
      "    - -892.8809787669213\n",
      "    - -886.8354593074685\n",
      "    - -889.2539114978938\n",
      "    - -1369.3584943260944\n",
      "    - -1357.6356186951773\n",
      "    - -900.2919654251918\n",
      "    - -892.9258643236634\n",
      "    - -884.837613972807\n",
      "    - -985.1596128933817\n",
      "    - -1005.4486191385646\n",
      "    - -1503.2662658244183\n",
      "    - -987.5015670486046\n",
      "    - -1056.504843793356\n",
      "    - -1188.6905171155006\n",
      "    - -756.7262199447661\n",
      "    - -1202.7543894793914\n",
      "    - -857.9182066821912\n",
      "    - -1048.6197373800187\n",
      "    - -1068.8198530166203\n",
      "    - -1298.7762347722755\n",
      "    - -893.3841986946359\n",
      "    - -1188.5541102179136\n",
      "    - -1619.8890464395697\n",
      "    - -983.9548480396792\n",
      "    - -1217.1721283060137\n",
      "    - -951.7144595017807\n",
      "    - -982.1914354909018\n",
      "    - -986.8618914641605\n",
      "    - -984.2556315444643\n",
      "    - -1541.5254536425232\n",
      "    - -980.1878675174929\n",
      "    - -1086.441731963236\n",
      "    - -1532.4625641311022\n",
      "    - -1179.608144563868\n",
      "    - -1381.8172240844226\n",
      "    - -894.9561848159502\n",
      "    - -977.489773677215\n",
      "    - -956.9657207945403\n",
      "    - -1712.613680820893\n",
      "    - -1498.5435184809298\n",
      "    - -883.7191263842913\n",
      "    - -892.3701687468206\n",
      "    - -1198.7935101211233\n",
      "    - -890.3978655811239\n",
      "    - -972.0434602642744\n",
      "    - -1063.441565978225\n",
      "    - -1005.419726310572\n",
      "    - -1572.8460139217739\n",
      "    - -1000.4615934491493\n",
      "    - -1186.9921375775416\n",
      "    - -1326.367393619063\n",
      "    - -886.4163082243513\n",
      "    - -1227.679110389234\n",
      "    - -889.8977677918155\n",
      "    - -981.576565125628\n",
      "    - -892.6688705748495\n",
      "    - -886.7498499572528\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1930303988131251\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14923914610326394\n",
      "    mean_inference_ms: 0.9814927281655906\n",
      "    mean_raw_obs_processing_ms: 0.12166336851388096\n",
      "time_since_restore: 1248.5318012237549\n",
      "time_this_iter_s: 10.284842252731323\n",
      "time_total_s: 1248.5318012237549\n",
      "timers:\n",
      "  learn_throughput: 872.823\n",
      "  learn_time_ms: 4582.828\n",
      "  load_throughput: 14549662.649\n",
      "  load_time_ms: 0.275\n",
      "  training_iteration_time_ms: 10375.063\n",
      "  update_time_ms: 2.493\n",
      "timestamp: 1660565171\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 476000\n",
      "training_iteration: 119\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 480000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 480000\n",
      "  num_agent_steps_trained: 480000\n",
      "  num_env_steps_sampled: 480000\n",
      "  num_env_steps_trained: 480000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-06-23\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -756.7262199447661\n",
      "episode_reward_mean: -1101.3998593189035\n",
      "episode_reward_min: -1725.6575870534139\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2400\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5131568908691406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0721828937530518\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009811530821025372\n",
      "        model: {}\n",
      "        policy_loss: 0.01229886058717966\n",
      "        total_loss: 9.73871898651123\n",
      "        vf_explained_var: -0.025513945147395134\n",
      "        vf_loss: 9.721385955810547\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 480000\n",
      "  num_agent_steps_trained: 480000\n",
      "  num_env_steps_sampled: 480000\n",
      "  num_env_steps_trained: 480000\n",
      "iterations_since_restore: 120\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 480000\n",
      "num_agent_steps_trained: 480000\n",
      "num_env_steps_sampled: 480000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 480000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 63.19411764705882\n",
      "  ram_util_percent: 60.629411764705885\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1930300899385587\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14923872891852705\n",
      "  mean_inference_ms: 0.9815421454730026\n",
      "  mean_raw_obs_processing_ms: 0.12165876648871887\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -756.7262199447661\n",
      "  episode_reward_mean: -1101.3998593189035\n",
      "  episode_reward_min: -1725.6575870534139\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1320.1406810946658\n",
      "    - -978.3799500210379\n",
      "    - -995.2464701306573\n",
      "    - -889.7207369461842\n",
      "    - -1629.3158893312059\n",
      "    - -893.4335581633463\n",
      "    - -881.9656210399907\n",
      "    - -884.3185851871451\n",
      "    - -1329.1884409824477\n",
      "    - -896.2592316915209\n",
      "    - -1069.306957713362\n",
      "    - -1339.8846914479407\n",
      "    - -966.3204232818781\n",
      "    - -1056.2602362793868\n",
      "    - -1725.6575870534139\n",
      "    - -981.9750732721889\n",
      "    - -1536.5614590855241\n",
      "    - -1012.6747720973622\n",
      "    - -1010.9796640303036\n",
      "    - -899.199453441558\n",
      "    - -1314.094142392093\n",
      "    - -884.5439742299077\n",
      "    - -1105.8638076622997\n",
      "    - -892.8809787669213\n",
      "    - -886.8354593074685\n",
      "    - -889.2539114978938\n",
      "    - -1369.3584943260944\n",
      "    - -1357.6356186951773\n",
      "    - -900.2919654251918\n",
      "    - -892.9258643236634\n",
      "    - -884.837613972807\n",
      "    - -985.1596128933817\n",
      "    - -1005.4486191385646\n",
      "    - -1503.2662658244183\n",
      "    - -987.5015670486046\n",
      "    - -1056.504843793356\n",
      "    - -1188.6905171155006\n",
      "    - -756.7262199447661\n",
      "    - -1202.7543894793914\n",
      "    - -857.9182066821912\n",
      "    - -1048.6197373800187\n",
      "    - -1068.8198530166203\n",
      "    - -1298.7762347722755\n",
      "    - -893.3841986946359\n",
      "    - -1188.5541102179136\n",
      "    - -1619.8890464395697\n",
      "    - -983.9548480396792\n",
      "    - -1217.1721283060137\n",
      "    - -951.7144595017807\n",
      "    - -982.1914354909018\n",
      "    - -986.8618914641605\n",
      "    - -984.2556315444643\n",
      "    - -1541.5254536425232\n",
      "    - -980.1878675174929\n",
      "    - -1086.441731963236\n",
      "    - -1532.4625641311022\n",
      "    - -1179.608144563868\n",
      "    - -1381.8172240844226\n",
      "    - -894.9561848159502\n",
      "    - -977.489773677215\n",
      "    - -956.9657207945403\n",
      "    - -1712.613680820893\n",
      "    - -1498.5435184809298\n",
      "    - -883.7191263842913\n",
      "    - -892.3701687468206\n",
      "    - -1198.7935101211233\n",
      "    - -890.3978655811239\n",
      "    - -972.0434602642744\n",
      "    - -1063.441565978225\n",
      "    - -1005.419726310572\n",
      "    - -1572.8460139217739\n",
      "    - -1000.4615934491493\n",
      "    - -1186.9921375775416\n",
      "    - -1326.367393619063\n",
      "    - -886.4163082243513\n",
      "    - -1227.679110389234\n",
      "    - -889.8977677918155\n",
      "    - -981.576565125628\n",
      "    - -892.6688705748495\n",
      "    - -886.7498499572528\n",
      "    - -1073.956527628528\n",
      "    - -985.3189187014624\n",
      "    - -911.6113816165024\n",
      "    - -1296.1297938763585\n",
      "    - -990.772562653854\n",
      "    - -1363.2677558114299\n",
      "    - -1158.0661118122282\n",
      "    - -889.0827270915705\n",
      "    - -1671.5023784618409\n",
      "    - -1241.6732598739447\n",
      "    - -985.2179564338481\n",
      "    - -1000.600773756546\n",
      "    - -880.7895548318893\n",
      "    - -891.5450747331381\n",
      "    - -1421.210917740847\n",
      "    - -1160.9294547069442\n",
      "    - -967.4867572078933\n",
      "    - -883.6085581980348\n",
      "    - -1532.2114942665123\n",
      "    - -891.0759442988898\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1930300899385587\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14923872891852705\n",
      "    mean_inference_ms: 0.9815421454730026\n",
      "    mean_raw_obs_processing_ms: 0.12165876648871887\n",
      "time_since_restore: 1260.2455179691315\n",
      "time_this_iter_s: 11.713716745376587\n",
      "time_total_s: 1260.2455179691315\n",
      "timers:\n",
      "  learn_throughput: 837.934\n",
      "  learn_time_ms: 4773.647\n",
      "  load_throughput: 14044212.289\n",
      "  load_time_ms: 0.285\n",
      "  training_iteration_time_ms: 10600.329\n",
      "  update_time_ms: 2.451\n",
      "timestamp: 1660565183\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 480000\n",
      "training_iteration: 120\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 484000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 484000\n",
      "  num_agent_steps_trained: 484000\n",
      "  num_env_steps_sampled: 484000\n",
      "  num_env_steps_trained: 484000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-06-33\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -756.7262199447661\n",
      "episode_reward_mean: -1107.7770987157353\n",
      "episode_reward_min: -1769.3932603277517\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2420\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5131568908691406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5283321142196655\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01056042779237032\n",
      "        model: {}\n",
      "        policy_loss: 0.01029331237077713\n",
      "        total_loss: 9.724150657653809\n",
      "        vf_explained_var: -0.02076992765069008\n",
      "        vf_loss: 9.7084379196167\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 484000\n",
      "  num_agent_steps_trained: 484000\n",
      "  num_env_steps_sampled: 484000\n",
      "  num_env_steps_trained: 484000\n",
      "iterations_since_restore: 121\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 484000\n",
      "num_agent_steps_trained: 484000\n",
      "num_env_steps_sampled: 484000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 484000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.70666666666667\n",
      "  ram_util_percent: 60.45333333333333\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19304971058941028\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14925713453642178\n",
      "  mean_inference_ms: 0.98170945377057\n",
      "  mean_raw_obs_processing_ms: 0.121669178052254\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -756.7262199447661\n",
      "  episode_reward_mean: -1107.7770987157353\n",
      "  episode_reward_min: -1769.3932603277517\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1314.094142392093\n",
      "    - -884.5439742299077\n",
      "    - -1105.8638076622997\n",
      "    - -892.8809787669213\n",
      "    - -886.8354593074685\n",
      "    - -889.2539114978938\n",
      "    - -1369.3584943260944\n",
      "    - -1357.6356186951773\n",
      "    - -900.2919654251918\n",
      "    - -892.9258643236634\n",
      "    - -884.837613972807\n",
      "    - -985.1596128933817\n",
      "    - -1005.4486191385646\n",
      "    - -1503.2662658244183\n",
      "    - -987.5015670486046\n",
      "    - -1056.504843793356\n",
      "    - -1188.6905171155006\n",
      "    - -756.7262199447661\n",
      "    - -1202.7543894793914\n",
      "    - -857.9182066821912\n",
      "    - -1048.6197373800187\n",
      "    - -1068.8198530166203\n",
      "    - -1298.7762347722755\n",
      "    - -893.3841986946359\n",
      "    - -1188.5541102179136\n",
      "    - -1619.8890464395697\n",
      "    - -983.9548480396792\n",
      "    - -1217.1721283060137\n",
      "    - -951.7144595017807\n",
      "    - -982.1914354909018\n",
      "    - -986.8618914641605\n",
      "    - -984.2556315444643\n",
      "    - -1541.5254536425232\n",
      "    - -980.1878675174929\n",
      "    - -1086.441731963236\n",
      "    - -1532.4625641311022\n",
      "    - -1179.608144563868\n",
      "    - -1381.8172240844226\n",
      "    - -894.9561848159502\n",
      "    - -977.489773677215\n",
      "    - -956.9657207945403\n",
      "    - -1712.613680820893\n",
      "    - -1498.5435184809298\n",
      "    - -883.7191263842913\n",
      "    - -892.3701687468206\n",
      "    - -1198.7935101211233\n",
      "    - -890.3978655811239\n",
      "    - -972.0434602642744\n",
      "    - -1063.441565978225\n",
      "    - -1005.419726310572\n",
      "    - -1572.8460139217739\n",
      "    - -1000.4615934491493\n",
      "    - -1186.9921375775416\n",
      "    - -1326.367393619063\n",
      "    - -886.4163082243513\n",
      "    - -1227.679110389234\n",
      "    - -889.8977677918155\n",
      "    - -981.576565125628\n",
      "    - -892.6688705748495\n",
      "    - -886.7498499572528\n",
      "    - -1073.956527628528\n",
      "    - -985.3189187014624\n",
      "    - -911.6113816165024\n",
      "    - -1296.1297938763585\n",
      "    - -990.772562653854\n",
      "    - -1363.2677558114299\n",
      "    - -1158.0661118122282\n",
      "    - -889.0827270915705\n",
      "    - -1671.5023784618409\n",
      "    - -1241.6732598739447\n",
      "    - -985.2179564338481\n",
      "    - -1000.600773756546\n",
      "    - -880.7895548318893\n",
      "    - -891.5450747331381\n",
      "    - -1421.210917740847\n",
      "    - -1160.9294547069442\n",
      "    - -967.4867572078933\n",
      "    - -883.6085581980348\n",
      "    - -1532.2114942665123\n",
      "    - -891.0759442988898\n",
      "    - -969.1124874502533\n",
      "    - -872.6596411589766\n",
      "    - -889.0583392467507\n",
      "    - -1440.169895750199\n",
      "    - -1376.811494153103\n",
      "    - -1264.9352309335036\n",
      "    - -1084.0617939075803\n",
      "    - -886.3302073267314\n",
      "    - -987.3842398946126\n",
      "    - -890.859454959301\n",
      "    - -1769.3932603277517\n",
      "    - -1392.5031008041074\n",
      "    - -1420.1010809274214\n",
      "    - -975.3938601008648\n",
      "    - -885.998016058487\n",
      "    - -1183.2875932351922\n",
      "    - -1418.888583842415\n",
      "    - -967.3324481671131\n",
      "    - -1170.517516860451\n",
      "    - -1089.7151768694732\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19304971058941028\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14925713453642178\n",
      "    mean_inference_ms: 0.98170945377057\n",
      "    mean_raw_obs_processing_ms: 0.121669178052254\n",
      "time_since_restore: 1270.84614610672\n",
      "time_this_iter_s: 10.600628137588501\n",
      "time_total_s: 1270.84614610672\n",
      "timers:\n",
      "  learn_throughput: 831.896\n",
      "  learn_time_ms: 4808.295\n",
      "  load_throughput: 13607929.272\n",
      "  load_time_ms: 0.294\n",
      "  training_iteration_time_ms: 10705.946\n",
      "  update_time_ms: 2.452\n",
      "timestamp: 1660565193\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 484000\n",
      "training_iteration: 121\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 488000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 488000\n",
      "  num_agent_steps_trained: 488000\n",
      "  num_env_steps_sampled: 488000\n",
      "  num_env_steps_trained: 488000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-06-43\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -872.6596411589766\n",
      "episode_reward_mean: -1125.006925863036\n",
      "episode_reward_min: -1769.3932603277517\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2440\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5131568908691406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3958884477615356\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01758168265223503\n",
      "        model: {}\n",
      "        policy_loss: 0.0149497389793396\n",
      "        total_loss: 9.841302871704102\n",
      "        vf_explained_var: -0.014726800844073296\n",
      "        vf_loss: 9.81733226776123\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 488000\n",
      "  num_agent_steps_trained: 488000\n",
      "  num_env_steps_sampled: 488000\n",
      "  num_env_steps_trained: 488000\n",
      "iterations_since_restore: 122\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 488000\n",
      "num_agent_steps_trained: 488000\n",
      "num_env_steps_sampled: 488000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 488000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.4357142857143\n",
      "  ram_util_percent: 60.48571428571427\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19303726717466682\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1492507373072206\n",
      "  mean_inference_ms: 0.9816891924122237\n",
      "  mean_raw_obs_processing_ms: 0.12165656398331948\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -872.6596411589766\n",
      "  episode_reward_mean: -1125.006925863036\n",
      "  episode_reward_min: -1769.3932603277517\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1048.6197373800187\n",
      "    - -1068.8198530166203\n",
      "    - -1298.7762347722755\n",
      "    - -893.3841986946359\n",
      "    - -1188.5541102179136\n",
      "    - -1619.8890464395697\n",
      "    - -983.9548480396792\n",
      "    - -1217.1721283060137\n",
      "    - -951.7144595017807\n",
      "    - -982.1914354909018\n",
      "    - -986.8618914641605\n",
      "    - -984.2556315444643\n",
      "    - -1541.5254536425232\n",
      "    - -980.1878675174929\n",
      "    - -1086.441731963236\n",
      "    - -1532.4625641311022\n",
      "    - -1179.608144563868\n",
      "    - -1381.8172240844226\n",
      "    - -894.9561848159502\n",
      "    - -977.489773677215\n",
      "    - -956.9657207945403\n",
      "    - -1712.613680820893\n",
      "    - -1498.5435184809298\n",
      "    - -883.7191263842913\n",
      "    - -892.3701687468206\n",
      "    - -1198.7935101211233\n",
      "    - -890.3978655811239\n",
      "    - -972.0434602642744\n",
      "    - -1063.441565978225\n",
      "    - -1005.419726310572\n",
      "    - -1572.8460139217739\n",
      "    - -1000.4615934491493\n",
      "    - -1186.9921375775416\n",
      "    - -1326.367393619063\n",
      "    - -886.4163082243513\n",
      "    - -1227.679110389234\n",
      "    - -889.8977677918155\n",
      "    - -981.576565125628\n",
      "    - -892.6688705748495\n",
      "    - -886.7498499572528\n",
      "    - -1073.956527628528\n",
      "    - -985.3189187014624\n",
      "    - -911.6113816165024\n",
      "    - -1296.1297938763585\n",
      "    - -990.772562653854\n",
      "    - -1363.2677558114299\n",
      "    - -1158.0661118122282\n",
      "    - -889.0827270915705\n",
      "    - -1671.5023784618409\n",
      "    - -1241.6732598739447\n",
      "    - -985.2179564338481\n",
      "    - -1000.600773756546\n",
      "    - -880.7895548318893\n",
      "    - -891.5450747331381\n",
      "    - -1421.210917740847\n",
      "    - -1160.9294547069442\n",
      "    - -967.4867572078933\n",
      "    - -883.6085581980348\n",
      "    - -1532.2114942665123\n",
      "    - -891.0759442988898\n",
      "    - -969.1124874502533\n",
      "    - -872.6596411589766\n",
      "    - -889.0583392467507\n",
      "    - -1440.169895750199\n",
      "    - -1376.811494153103\n",
      "    - -1264.9352309335036\n",
      "    - -1084.0617939075803\n",
      "    - -886.3302073267314\n",
      "    - -987.3842398946126\n",
      "    - -890.859454959301\n",
      "    - -1769.3932603277517\n",
      "    - -1392.5031008041074\n",
      "    - -1420.1010809274214\n",
      "    - -975.3938601008648\n",
      "    - -885.998016058487\n",
      "    - -1183.2875932351922\n",
      "    - -1418.888583842415\n",
      "    - -967.3324481671131\n",
      "    - -1170.517516860451\n",
      "    - -1089.7151768694732\n",
      "    - -970.1988977247198\n",
      "    - -1526.5039199041237\n",
      "    - -1310.446383308183\n",
      "    - -1181.4598488660909\n",
      "    - -1170.709989216025\n",
      "    - -977.5677755379759\n",
      "    - -1405.6959908644185\n",
      "    - -1196.0707609680135\n",
      "    - -905.597541940327\n",
      "    - -977.0403795139301\n",
      "    - -894.7833043379131\n",
      "    - -989.887285321031\n",
      "    - -1120.569718598258\n",
      "    - -998.1165447856271\n",
      "    - -1457.0650232429907\n",
      "    - -886.1229592871138\n",
      "    - -1532.4930063582976\n",
      "    - -1197.729603746647\n",
      "    - -1064.8046994086963\n",
      "    - -882.6111543193832\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19303726717466682\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1492507373072206\n",
      "    mean_inference_ms: 0.9816891924122237\n",
      "    mean_raw_obs_processing_ms: 0.12165656398331948\n",
      "time_since_restore: 1280.6013340950012\n",
      "time_this_iter_s: 9.75518798828125\n",
      "time_total_s: 1280.6013340950012\n",
      "timers:\n",
      "  learn_throughput: 831.568\n",
      "  learn_time_ms: 4810.191\n",
      "  load_throughput: 13607929.272\n",
      "  load_time_ms: 0.294\n",
      "  training_iteration_time_ms: 10729.39\n",
      "  update_time_ms: 2.454\n",
      "timestamp: 1660565203\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 488000\n",
      "training_iteration: 122\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 492000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 492000\n",
      "  num_agent_steps_trained: 492000\n",
      "  num_env_steps_sampled: 492000\n",
      "  num_env_steps_trained: 492000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-07-03\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -872.6596411589766\n",
      "episode_reward_mean: -1119.028341630373\n",
      "episode_reward_min: -1769.3932603277517\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2460\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5131568908691406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1170436143875122\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016564004123210907\n",
      "        model: {}\n",
      "        policy_loss: 0.011516505852341652\n",
      "        total_loss: 9.77904987335205\n",
      "        vf_explained_var: -0.017414290457963943\n",
      "        vf_loss: 9.759032249450684\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 492000\n",
      "  num_agent_steps_trained: 492000\n",
      "  num_env_steps_sampled: 492000\n",
      "  num_env_steps_trained: 492000\n",
      "iterations_since_restore: 123\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 492000\n",
      "num_agent_steps_trained: 492000\n",
      "num_env_steps_sampled: 492000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 492000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 87.22142857142858\n",
      "  ram_util_percent: 61.27499999999999\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19318104330677804\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14936902800226567\n",
      "  mean_inference_ms: 0.9825561472117296\n",
      "  mean_raw_obs_processing_ms: 0.12174018362597101\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -872.6596411589766\n",
      "  episode_reward_mean: -1119.028341630373\n",
      "  episode_reward_min: -1769.3932603277517\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -956.9657207945403\n",
      "    - -1712.613680820893\n",
      "    - -1498.5435184809298\n",
      "    - -883.7191263842913\n",
      "    - -892.3701687468206\n",
      "    - -1198.7935101211233\n",
      "    - -890.3978655811239\n",
      "    - -972.0434602642744\n",
      "    - -1063.441565978225\n",
      "    - -1005.419726310572\n",
      "    - -1572.8460139217739\n",
      "    - -1000.4615934491493\n",
      "    - -1186.9921375775416\n",
      "    - -1326.367393619063\n",
      "    - -886.4163082243513\n",
      "    - -1227.679110389234\n",
      "    - -889.8977677918155\n",
      "    - -981.576565125628\n",
      "    - -892.6688705748495\n",
      "    - -886.7498499572528\n",
      "    - -1073.956527628528\n",
      "    - -985.3189187014624\n",
      "    - -911.6113816165024\n",
      "    - -1296.1297938763585\n",
      "    - -990.772562653854\n",
      "    - -1363.2677558114299\n",
      "    - -1158.0661118122282\n",
      "    - -889.0827270915705\n",
      "    - -1671.5023784618409\n",
      "    - -1241.6732598739447\n",
      "    - -985.2179564338481\n",
      "    - -1000.600773756546\n",
      "    - -880.7895548318893\n",
      "    - -891.5450747331381\n",
      "    - -1421.210917740847\n",
      "    - -1160.9294547069442\n",
      "    - -967.4867572078933\n",
      "    - -883.6085581980348\n",
      "    - -1532.2114942665123\n",
      "    - -891.0759442988898\n",
      "    - -969.1124874502533\n",
      "    - -872.6596411589766\n",
      "    - -889.0583392467507\n",
      "    - -1440.169895750199\n",
      "    - -1376.811494153103\n",
      "    - -1264.9352309335036\n",
      "    - -1084.0617939075803\n",
      "    - -886.3302073267314\n",
      "    - -987.3842398946126\n",
      "    - -890.859454959301\n",
      "    - -1769.3932603277517\n",
      "    - -1392.5031008041074\n",
      "    - -1420.1010809274214\n",
      "    - -975.3938601008648\n",
      "    - -885.998016058487\n",
      "    - -1183.2875932351922\n",
      "    - -1418.888583842415\n",
      "    - -967.3324481671131\n",
      "    - -1170.517516860451\n",
      "    - -1089.7151768694732\n",
      "    - -970.1988977247198\n",
      "    - -1526.5039199041237\n",
      "    - -1310.446383308183\n",
      "    - -1181.4598488660909\n",
      "    - -1170.709989216025\n",
      "    - -977.5677755379759\n",
      "    - -1405.6959908644185\n",
      "    - -1196.0707609680135\n",
      "    - -905.597541940327\n",
      "    - -977.0403795139301\n",
      "    - -894.7833043379131\n",
      "    - -989.887285321031\n",
      "    - -1120.569718598258\n",
      "    - -998.1165447856271\n",
      "    - -1457.0650232429907\n",
      "    - -886.1229592871138\n",
      "    - -1532.4930063582976\n",
      "    - -1197.729603746647\n",
      "    - -1064.8046994086963\n",
      "    - -882.6111543193832\n",
      "    - -1333.130318711086\n",
      "    - -1533.1299587689825\n",
      "    - -888.861312971135\n",
      "    - -1073.9772357016068\n",
      "    - -884.681911919317\n",
      "    - -895.2794637839354\n",
      "    - -1072.9027756803275\n",
      "    - -886.7593937998914\n",
      "    - -1504.7911610641474\n",
      "    - -971.4868662665507\n",
      "    - -1302.3870864664495\n",
      "    - -1254.8665845235469\n",
      "    - -886.6063708669357\n",
      "    - -979.2133847553499\n",
      "    - -1625.7128798930767\n",
      "    - -889.9373525538446\n",
      "    - -888.4557344708096\n",
      "    - -905.7219061507558\n",
      "    - -889.6619656042202\n",
      "    - -1533.260432045574\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19318104330677804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14936902800226567\n",
      "    mean_inference_ms: 0.9825561472117296\n",
      "    mean_raw_obs_processing_ms: 0.12174018362597101\n",
      "time_since_restore: 1300.4545731544495\n",
      "time_this_iter_s: 19.853239059448242\n",
      "time_total_s: 1300.4545731544495\n",
      "timers:\n",
      "  learn_throughput: 746.741\n",
      "  learn_time_ms: 5356.607\n",
      "  load_throughput: 13592494.531\n",
      "  load_time_ms: 0.294\n",
      "  training_iteration_time_ms: 11607.162\n",
      "  update_time_ms: 2.523\n",
      "timestamp: 1660565223\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 492000\n",
      "training_iteration: 123\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 496000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 496000\n",
      "  num_agent_steps_trained: 496000\n",
      "  num_env_steps_sampled: 496000\n",
      "  num_env_steps_trained: 496000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-07-17\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -872.6596411589766\n",
      "episode_reward_mean: -1120.5503979563018\n",
      "episode_reward_min: -1769.3932603277517\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2480\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5131568908691406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0284029245376587\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017414219677448273\n",
      "        model: {}\n",
      "        policy_loss: 0.012073859572410583\n",
      "        total_loss: 9.862984657287598\n",
      "        vf_explained_var: -0.02316289022564888\n",
      "        vf_loss: 9.841975212097168\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 496000\n",
      "  num_agent_steps_trained: 496000\n",
      "  num_env_steps_sampled: 496000\n",
      "  num_env_steps_trained: 496000\n",
      "iterations_since_restore: 124\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 496000\n",
      "num_agent_steps_trained: 496000\n",
      "num_env_steps_sampled: 496000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 496000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 75.49047619047617\n",
      "  ram_util_percent: 63.209523809523816\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19345711239510677\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.14957423638227407\n",
      "  mean_inference_ms: 0.9841747907201248\n",
      "  mean_raw_obs_processing_ms: 0.1218962937559581\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -872.6596411589766\n",
      "  episode_reward_mean: -1120.5503979563018\n",
      "  episode_reward_min: -1769.3932603277517\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1073.956527628528\n",
      "    - -985.3189187014624\n",
      "    - -911.6113816165024\n",
      "    - -1296.1297938763585\n",
      "    - -990.772562653854\n",
      "    - -1363.2677558114299\n",
      "    - -1158.0661118122282\n",
      "    - -889.0827270915705\n",
      "    - -1671.5023784618409\n",
      "    - -1241.6732598739447\n",
      "    - -985.2179564338481\n",
      "    - -1000.600773756546\n",
      "    - -880.7895548318893\n",
      "    - -891.5450747331381\n",
      "    - -1421.210917740847\n",
      "    - -1160.9294547069442\n",
      "    - -967.4867572078933\n",
      "    - -883.6085581980348\n",
      "    - -1532.2114942665123\n",
      "    - -891.0759442988898\n",
      "    - -969.1124874502533\n",
      "    - -872.6596411589766\n",
      "    - -889.0583392467507\n",
      "    - -1440.169895750199\n",
      "    - -1376.811494153103\n",
      "    - -1264.9352309335036\n",
      "    - -1084.0617939075803\n",
      "    - -886.3302073267314\n",
      "    - -987.3842398946126\n",
      "    - -890.859454959301\n",
      "    - -1769.3932603277517\n",
      "    - -1392.5031008041074\n",
      "    - -1420.1010809274214\n",
      "    - -975.3938601008648\n",
      "    - -885.998016058487\n",
      "    - -1183.2875932351922\n",
      "    - -1418.888583842415\n",
      "    - -967.3324481671131\n",
      "    - -1170.517516860451\n",
      "    - -1089.7151768694732\n",
      "    - -970.1988977247198\n",
      "    - -1526.5039199041237\n",
      "    - -1310.446383308183\n",
      "    - -1181.4598488660909\n",
      "    - -1170.709989216025\n",
      "    - -977.5677755379759\n",
      "    - -1405.6959908644185\n",
      "    - -1196.0707609680135\n",
      "    - -905.597541940327\n",
      "    - -977.0403795139301\n",
      "    - -894.7833043379131\n",
      "    - -989.887285321031\n",
      "    - -1120.569718598258\n",
      "    - -998.1165447856271\n",
      "    - -1457.0650232429907\n",
      "    - -886.1229592871138\n",
      "    - -1532.4930063582976\n",
      "    - -1197.729603746647\n",
      "    - -1064.8046994086963\n",
      "    - -882.6111543193832\n",
      "    - -1333.130318711086\n",
      "    - -1533.1299587689825\n",
      "    - -888.861312971135\n",
      "    - -1073.9772357016068\n",
      "    - -884.681911919317\n",
      "    - -895.2794637839354\n",
      "    - -1072.9027756803275\n",
      "    - -886.7593937998914\n",
      "    - -1504.7911610641474\n",
      "    - -971.4868662665507\n",
      "    - -1302.3870864664495\n",
      "    - -1254.8665845235469\n",
      "    - -886.6063708669357\n",
      "    - -979.2133847553499\n",
      "    - -1625.7128798930767\n",
      "    - -889.9373525538446\n",
      "    - -888.4557344708096\n",
      "    - -905.7219061507558\n",
      "    - -889.6619656042202\n",
      "    - -1533.260432045574\n",
      "    - -1532.2210338277\n",
      "    - -996.5772103570973\n",
      "    - -1438.590165311124\n",
      "    - -1336.9995814201943\n",
      "    - -888.0376163632428\n",
      "    - -1585.5253465737842\n",
      "    - -961.0686516971214\n",
      "    - -992.0616660848516\n",
      "    - -966.1056953553092\n",
      "    - -881.9387333570522\n",
      "    - -884.0760849782793\n",
      "    - -989.1681518483382\n",
      "    - -880.9744022623399\n",
      "    - -882.11535637332\n",
      "    - -906.9503112363952\n",
      "    - -1057.8044695356887\n",
      "    - -982.5242175531939\n",
      "    - -1361.8000971339475\n",
      "    - -1095.7834774086757\n",
      "    - -1457.8473180286721\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19345711239510677\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14957423638227407\n",
      "    mean_inference_ms: 0.9841747907201248\n",
      "    mean_raw_obs_processing_ms: 0.1218962937559581\n",
      "time_since_restore: 1314.7421634197235\n",
      "time_this_iter_s: 14.287590265274048\n",
      "time_total_s: 1314.7421634197235\n",
      "timers:\n",
      "  learn_throughput: 729.976\n",
      "  learn_time_ms: 5479.636\n",
      "  load_throughput: 13549681.796\n",
      "  load_time_ms: 0.295\n",
      "  training_iteration_time_ms: 11901.299\n",
      "  update_time_ms: 2.546\n",
      "timestamp: 1660565237\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 496000\n",
      "training_iteration: 124\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 500000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 500000\n",
      "  num_agent_steps_trained: 500000\n",
      "  num_env_steps_sampled: 500000\n",
      "  num_env_steps_trained: 500000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-07-29\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -872.6596411589766\n",
      "episode_reward_mean: -1140.779923260405\n",
      "episode_reward_min: -1769.3932603277517\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2500\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5131568908691406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.5271799564361572\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00868880096822977\n",
      "        model: {}\n",
      "        policy_loss: 0.009778455831110477\n",
      "        total_loss: 9.940359115600586\n",
      "        vf_explained_var: -0.018608253449201584\n",
      "        vf_loss: 9.926122665405273\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 500000\n",
      "  num_agent_steps_trained: 500000\n",
      "  num_env_steps_sampled: 500000\n",
      "  num_env_steps_trained: 500000\n",
      "iterations_since_restore: 125\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 500000\n",
      "num_agent_steps_trained: 500000\n",
      "num_env_steps_sampled: 500000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 500000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 63.64705882352941\n",
      "  ram_util_percent: 63.1\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.193789130708399\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1498215391305567\n",
      "  mean_inference_ms: 0.9860900457568492\n",
      "  mean_raw_obs_processing_ms: 0.12208499630811026\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -872.6596411589766\n",
      "  episode_reward_mean: -1140.779923260405\n",
      "  episode_reward_min: -1769.3932603277517\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -969.1124874502533\n",
      "    - -872.6596411589766\n",
      "    - -889.0583392467507\n",
      "    - -1440.169895750199\n",
      "    - -1376.811494153103\n",
      "    - -1264.9352309335036\n",
      "    - -1084.0617939075803\n",
      "    - -886.3302073267314\n",
      "    - -987.3842398946126\n",
      "    - -890.859454959301\n",
      "    - -1769.3932603277517\n",
      "    - -1392.5031008041074\n",
      "    - -1420.1010809274214\n",
      "    - -975.3938601008648\n",
      "    - -885.998016058487\n",
      "    - -1183.2875932351922\n",
      "    - -1418.888583842415\n",
      "    - -967.3324481671131\n",
      "    - -1170.517516860451\n",
      "    - -1089.7151768694732\n",
      "    - -970.1988977247198\n",
      "    - -1526.5039199041237\n",
      "    - -1310.446383308183\n",
      "    - -1181.4598488660909\n",
      "    - -1170.709989216025\n",
      "    - -977.5677755379759\n",
      "    - -1405.6959908644185\n",
      "    - -1196.0707609680135\n",
      "    - -905.597541940327\n",
      "    - -977.0403795139301\n",
      "    - -894.7833043379131\n",
      "    - -989.887285321031\n",
      "    - -1120.569718598258\n",
      "    - -998.1165447856271\n",
      "    - -1457.0650232429907\n",
      "    - -886.1229592871138\n",
      "    - -1532.4930063582976\n",
      "    - -1197.729603746647\n",
      "    - -1064.8046994086963\n",
      "    - -882.6111543193832\n",
      "    - -1333.130318711086\n",
      "    - -1533.1299587689825\n",
      "    - -888.861312971135\n",
      "    - -1073.9772357016068\n",
      "    - -884.681911919317\n",
      "    - -895.2794637839354\n",
      "    - -1072.9027756803275\n",
      "    - -886.7593937998914\n",
      "    - -1504.7911610641474\n",
      "    - -971.4868662665507\n",
      "    - -1302.3870864664495\n",
      "    - -1254.8665845235469\n",
      "    - -886.6063708669357\n",
      "    - -979.2133847553499\n",
      "    - -1625.7128798930767\n",
      "    - -889.9373525538446\n",
      "    - -888.4557344708096\n",
      "    - -905.7219061507558\n",
      "    - -889.6619656042202\n",
      "    - -1533.260432045574\n",
      "    - -1532.2210338277\n",
      "    - -996.5772103570973\n",
      "    - -1438.590165311124\n",
      "    - -1336.9995814201943\n",
      "    - -888.0376163632428\n",
      "    - -1585.5253465737842\n",
      "    - -961.0686516971214\n",
      "    - -992.0616660848516\n",
      "    - -966.1056953553092\n",
      "    - -881.9387333570522\n",
      "    - -884.0760849782793\n",
      "    - -989.1681518483382\n",
      "    - -880.9744022623399\n",
      "    - -882.11535637332\n",
      "    - -906.9503112363952\n",
      "    - -1057.8044695356887\n",
      "    - -982.5242175531939\n",
      "    - -1361.8000971339475\n",
      "    - -1095.7834774086757\n",
      "    - -1457.8473180286721\n",
      "    - -884.9080668002618\n",
      "    - -1012.4595079042411\n",
      "    - -1086.1239601346194\n",
      "    - -1075.9602882860431\n",
      "    - -1382.3931197042716\n",
      "    - -1370.3313187843744\n",
      "    - -1506.03634850755\n",
      "    - -1534.3960402667688\n",
      "    - -942.940133832068\n",
      "    - -1570.0820696945502\n",
      "    - -888.4056537643868\n",
      "    - -1554.9024074567253\n",
      "    - -886.4177681155383\n",
      "    - -1593.6928544365005\n",
      "    - -999.9061727532414\n",
      "    - -1651.6885062628037\n",
      "    - -885.0996935308693\n",
      "    - -1416.7617593906864\n",
      "    - -976.3917091186404\n",
      "    - -1000.1130553684137\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.193789130708399\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1498215391305567\n",
      "    mean_inference_ms: 0.9860900457568492\n",
      "    mean_raw_obs_processing_ms: 0.12208499630811026\n",
      "time_since_restore: 1326.6456851959229\n",
      "time_this_iter_s: 11.90352177619934\n",
      "time_total_s: 1326.6456851959229\n",
      "timers:\n",
      "  learn_throughput: 721.867\n",
      "  learn_time_ms: 5541.184\n",
      "  load_throughput: 13662228.013\n",
      "  load_time_ms: 0.293\n",
      "  training_iteration_time_ms: 12077.354\n",
      "  update_time_ms: 2.593\n",
      "timestamp: 1660565249\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 500000\n",
      "training_iteration: 125\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 504000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 504000\n",
      "  num_agent_steps_trained: 504000\n",
      "  num_env_steps_sampled: 504000\n",
      "  num_env_steps_trained: 504000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-07-41\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -874.1461144781831\n",
      "episode_reward_mean: -1116.2774658789747\n",
      "episode_reward_min: -1651.6885062628037\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2520\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5131568908691406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.005913558881729841\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014934748411178589\n",
      "        model: {}\n",
      "        policy_loss: 0.012160086072981358\n",
      "        total_loss: 9.816332817077637\n",
      "        vf_explained_var: -0.014384686946868896\n",
      "        vf_loss: 9.796507835388184\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 504000\n",
      "  num_agent_steps_trained: 504000\n",
      "  num_env_steps_sampled: 504000\n",
      "  num_env_steps_trained: 504000\n",
      "iterations_since_restore: 126\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 504000\n",
      "num_agent_steps_trained: 504000\n",
      "num_env_steps_sampled: 504000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 504000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 63.405882352941184\n",
      "  ram_util_percent: 63.22941176470587\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19412454389924824\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15006654036749725\n",
      "  mean_inference_ms: 0.9880176104122532\n",
      "  mean_raw_obs_processing_ms: 0.12227126564828332\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -874.1461144781831\n",
      "  episode_reward_mean: -1116.2774658789747\n",
      "  episode_reward_min: -1651.6885062628037\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -970.1988977247198\n",
      "    - -1526.5039199041237\n",
      "    - -1310.446383308183\n",
      "    - -1181.4598488660909\n",
      "    - -1170.709989216025\n",
      "    - -977.5677755379759\n",
      "    - -1405.6959908644185\n",
      "    - -1196.0707609680135\n",
      "    - -905.597541940327\n",
      "    - -977.0403795139301\n",
      "    - -894.7833043379131\n",
      "    - -989.887285321031\n",
      "    - -1120.569718598258\n",
      "    - -998.1165447856271\n",
      "    - -1457.0650232429907\n",
      "    - -886.1229592871138\n",
      "    - -1532.4930063582976\n",
      "    - -1197.729603746647\n",
      "    - -1064.8046994086963\n",
      "    - -882.6111543193832\n",
      "    - -1333.130318711086\n",
      "    - -1533.1299587689825\n",
      "    - -888.861312971135\n",
      "    - -1073.9772357016068\n",
      "    - -884.681911919317\n",
      "    - -895.2794637839354\n",
      "    - -1072.9027756803275\n",
      "    - -886.7593937998914\n",
      "    - -1504.7911610641474\n",
      "    - -971.4868662665507\n",
      "    - -1302.3870864664495\n",
      "    - -1254.8665845235469\n",
      "    - -886.6063708669357\n",
      "    - -979.2133847553499\n",
      "    - -1625.7128798930767\n",
      "    - -889.9373525538446\n",
      "    - -888.4557344708096\n",
      "    - -905.7219061507558\n",
      "    - -889.6619656042202\n",
      "    - -1533.260432045574\n",
      "    - -1532.2210338277\n",
      "    - -996.5772103570973\n",
      "    - -1438.590165311124\n",
      "    - -1336.9995814201943\n",
      "    - -888.0376163632428\n",
      "    - -1585.5253465737842\n",
      "    - -961.0686516971214\n",
      "    - -992.0616660848516\n",
      "    - -966.1056953553092\n",
      "    - -881.9387333570522\n",
      "    - -884.0760849782793\n",
      "    - -989.1681518483382\n",
      "    - -880.9744022623399\n",
      "    - -882.11535637332\n",
      "    - -906.9503112363952\n",
      "    - -1057.8044695356887\n",
      "    - -982.5242175531939\n",
      "    - -1361.8000971339475\n",
      "    - -1095.7834774086757\n",
      "    - -1457.8473180286721\n",
      "    - -884.9080668002618\n",
      "    - -1012.4595079042411\n",
      "    - -1086.1239601346194\n",
      "    - -1075.9602882860431\n",
      "    - -1382.3931197042716\n",
      "    - -1370.3313187843744\n",
      "    - -1506.03634850755\n",
      "    - -1534.3960402667688\n",
      "    - -942.940133832068\n",
      "    - -1570.0820696945502\n",
      "    - -888.4056537643868\n",
      "    - -1554.9024074567253\n",
      "    - -886.4177681155383\n",
      "    - -1593.6928544365005\n",
      "    - -999.9061727532414\n",
      "    - -1651.6885062628037\n",
      "    - -885.0996935308693\n",
      "    - -1416.7617593906864\n",
      "    - -976.3917091186404\n",
      "    - -1000.1130553684137\n",
      "    - -1356.7360847616326\n",
      "    - -887.2207239595191\n",
      "    - -961.3712689253897\n",
      "    - -1010.5076756714774\n",
      "    - -882.3601353910647\n",
      "    - -1326.5242198293895\n",
      "    - -1195.86857443263\n",
      "    - -1304.61484056051\n",
      "    - -887.355750405848\n",
      "    - -888.1754464332628\n",
      "    - -1070.8329914250883\n",
      "    - -895.6296677305105\n",
      "    - -888.2719538595254\n",
      "    - -1076.078203443007\n",
      "    - -1009.2511825203228\n",
      "    - -898.4064545548464\n",
      "    - -874.1461144781831\n",
      "    - -886.8000190606749\n",
      "    - -1221.1043860321447\n",
      "    - -963.0119903562573\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19412454389924824\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15006654036749725\n",
      "    mean_inference_ms: 0.9880176104122532\n",
      "    mean_raw_obs_processing_ms: 0.12227126564828332\n",
      "time_since_restore: 1338.7061359882355\n",
      "time_this_iter_s: 12.060450792312622\n",
      "time_total_s: 1338.7061359882355\n",
      "timers:\n",
      "  learn_throughput: 706.493\n",
      "  learn_time_ms: 5661.769\n",
      "  load_throughput: 14515673.992\n",
      "  load_time_ms: 0.276\n",
      "  training_iteration_time_ms: 12249.048\n",
      "  update_time_ms: 2.62\n",
      "timestamp: 1660565261\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 504000\n",
      "training_iteration: 126\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 508000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 508000\n",
      "  num_agent_steps_trained: 508000\n",
      "  num_env_steps_sampled: 508000\n",
      "  num_env_steps_trained: 508000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-07-54\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -868.0626873537883\n",
      "episode_reward_mean: -1123.3728741352895\n",
      "episode_reward_min: -1698.2595367609165\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2540\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5131568908691406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.938252329826355\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008084391243755817\n",
      "        model: {}\n",
      "        policy_loss: 0.011940411292016506\n",
      "        total_loss: 9.808099746704102\n",
      "        vf_explained_var: -0.025725726038217545\n",
      "        vf_loss: 9.792010307312012\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 508000\n",
      "  num_agent_steps_trained: 508000\n",
      "  num_env_steps_sampled: 508000\n",
      "  num_env_steps_trained: 508000\n",
      "iterations_since_restore: 127\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 508000\n",
      "num_agent_steps_trained: 508000\n",
      "num_env_steps_sampled: 508000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 508000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 67.48421052631579\n",
      "  ram_util_percent: 63.47894736842106\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19454296634082316\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15037315470764184\n",
      "  mean_inference_ms: 0.990442026068021\n",
      "  mean_raw_obs_processing_ms: 0.12251033291184474\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -868.0626873537883\n",
      "  episode_reward_mean: -1123.3728741352895\n",
      "  episode_reward_min: -1698.2595367609165\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1333.130318711086\n",
      "    - -1533.1299587689825\n",
      "    - -888.861312971135\n",
      "    - -1073.9772357016068\n",
      "    - -884.681911919317\n",
      "    - -895.2794637839354\n",
      "    - -1072.9027756803275\n",
      "    - -886.7593937998914\n",
      "    - -1504.7911610641474\n",
      "    - -971.4868662665507\n",
      "    - -1302.3870864664495\n",
      "    - -1254.8665845235469\n",
      "    - -886.6063708669357\n",
      "    - -979.2133847553499\n",
      "    - -1625.7128798930767\n",
      "    - -889.9373525538446\n",
      "    - -888.4557344708096\n",
      "    - -905.7219061507558\n",
      "    - -889.6619656042202\n",
      "    - -1533.260432045574\n",
      "    - -1532.2210338277\n",
      "    - -996.5772103570973\n",
      "    - -1438.590165311124\n",
      "    - -1336.9995814201943\n",
      "    - -888.0376163632428\n",
      "    - -1585.5253465737842\n",
      "    - -961.0686516971214\n",
      "    - -992.0616660848516\n",
      "    - -966.1056953553092\n",
      "    - -881.9387333570522\n",
      "    - -884.0760849782793\n",
      "    - -989.1681518483382\n",
      "    - -880.9744022623399\n",
      "    - -882.11535637332\n",
      "    - -906.9503112363952\n",
      "    - -1057.8044695356887\n",
      "    - -982.5242175531939\n",
      "    - -1361.8000971339475\n",
      "    - -1095.7834774086757\n",
      "    - -1457.8473180286721\n",
      "    - -884.9080668002618\n",
      "    - -1012.4595079042411\n",
      "    - -1086.1239601346194\n",
      "    - -1075.9602882860431\n",
      "    - -1382.3931197042716\n",
      "    - -1370.3313187843744\n",
      "    - -1506.03634850755\n",
      "    - -1534.3960402667688\n",
      "    - -942.940133832068\n",
      "    - -1570.0820696945502\n",
      "    - -888.4056537643868\n",
      "    - -1554.9024074567253\n",
      "    - -886.4177681155383\n",
      "    - -1593.6928544365005\n",
      "    - -999.9061727532414\n",
      "    - -1651.6885062628037\n",
      "    - -885.0996935308693\n",
      "    - -1416.7617593906864\n",
      "    - -976.3917091186404\n",
      "    - -1000.1130553684137\n",
      "    - -1356.7360847616326\n",
      "    - -887.2207239595191\n",
      "    - -961.3712689253897\n",
      "    - -1010.5076756714774\n",
      "    - -882.3601353910647\n",
      "    - -1326.5242198293895\n",
      "    - -1195.86857443263\n",
      "    - -1304.61484056051\n",
      "    - -887.355750405848\n",
      "    - -888.1754464332628\n",
      "    - -1070.8329914250883\n",
      "    - -895.6296677305105\n",
      "    - -888.2719538595254\n",
      "    - -1076.078203443007\n",
      "    - -1009.2511825203228\n",
      "    - -898.4064545548464\n",
      "    - -874.1461144781831\n",
      "    - -886.8000190606749\n",
      "    - -1221.1043860321447\n",
      "    - -963.0119903562573\n",
      "    - -868.0626873537883\n",
      "    - -1613.68946693013\n",
      "    - -885.5235321712189\n",
      "    - -1070.992189411859\n",
      "    - -1535.504084275916\n",
      "    - -1192.1325644965411\n",
      "    - -1684.6274698202774\n",
      "    - -961.7965132329848\n",
      "    - -904.0147753600123\n",
      "    - -943.5877029512922\n",
      "    - -1600.3555053885268\n",
      "    - -883.4253674806351\n",
      "    - -1307.538313366646\n",
      "    - -1698.2595367609165\n",
      "    - -1221.2195212168833\n",
      "    - -883.9686375672566\n",
      "    - -1279.123238120803\n",
      "    - -987.975491173103\n",
      "    - -891.39857741677\n",
      "    - -941.8204383856912\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19454296634082316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15037315470764184\n",
      "    mean_inference_ms: 0.990442026068021\n",
      "    mean_raw_obs_processing_ms: 0.12251033291184474\n",
      "time_since_restore: 1351.7164361476898\n",
      "time_this_iter_s: 13.010300159454346\n",
      "time_total_s: 1351.7164361476898\n",
      "timers:\n",
      "  learn_throughput: 703.279\n",
      "  learn_time_ms: 5687.643\n",
      "  load_throughput: 15430162.789\n",
      "  load_time_ms: 0.259\n",
      "  training_iteration_time_ms: 12393.075\n",
      "  update_time_ms: 2.629\n",
      "timestamp: 1660565274\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 508000\n",
      "training_iteration: 127\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 512000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 512000\n",
      "  num_agent_steps_trained: 512000\n",
      "  num_env_steps_sampled: 512000\n",
      "  num_env_steps_trained: 512000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-08-08\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -868.0626873537883\n",
      "episode_reward_mean: -1127.6078734076114\n",
      "episode_reward_min: -1698.2595367609165\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2560\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5131568908691406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.23463773727417\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0118241086602211\n",
      "        model: {}\n",
      "        policy_loss: 0.007544009014964104\n",
      "        total_loss: 9.77481746673584\n",
      "        vf_explained_var: -0.007934651337563992\n",
      "        vf_loss: 9.761205673217773\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 512000\n",
      "  num_agent_steps_trained: 512000\n",
      "  num_env_steps_sampled: 512000\n",
      "  num_env_steps_trained: 512000\n",
      "iterations_since_restore: 128\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 512000\n",
      "num_agent_steps_trained: 512000\n",
      "num_env_steps_sampled: 512000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 512000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 68.77894736842104\n",
      "  ram_util_percent: 63.531578947368416\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1949065168336837\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15062454702447664\n",
      "  mean_inference_ms: 0.9924893597598917\n",
      "  mean_raw_obs_processing_ms: 0.12270772835667809\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -868.0626873537883\n",
      "  episode_reward_mean: -1127.6078734076114\n",
      "  episode_reward_min: -1698.2595367609165\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1532.2210338277\n",
      "    - -996.5772103570973\n",
      "    - -1438.590165311124\n",
      "    - -1336.9995814201943\n",
      "    - -888.0376163632428\n",
      "    - -1585.5253465737842\n",
      "    - -961.0686516971214\n",
      "    - -992.0616660848516\n",
      "    - -966.1056953553092\n",
      "    - -881.9387333570522\n",
      "    - -884.0760849782793\n",
      "    - -989.1681518483382\n",
      "    - -880.9744022623399\n",
      "    - -882.11535637332\n",
      "    - -906.9503112363952\n",
      "    - -1057.8044695356887\n",
      "    - -982.5242175531939\n",
      "    - -1361.8000971339475\n",
      "    - -1095.7834774086757\n",
      "    - -1457.8473180286721\n",
      "    - -884.9080668002618\n",
      "    - -1012.4595079042411\n",
      "    - -1086.1239601346194\n",
      "    - -1075.9602882860431\n",
      "    - -1382.3931197042716\n",
      "    - -1370.3313187843744\n",
      "    - -1506.03634850755\n",
      "    - -1534.3960402667688\n",
      "    - -942.940133832068\n",
      "    - -1570.0820696945502\n",
      "    - -888.4056537643868\n",
      "    - -1554.9024074567253\n",
      "    - -886.4177681155383\n",
      "    - -1593.6928544365005\n",
      "    - -999.9061727532414\n",
      "    - -1651.6885062628037\n",
      "    - -885.0996935308693\n",
      "    - -1416.7617593906864\n",
      "    - -976.3917091186404\n",
      "    - -1000.1130553684137\n",
      "    - -1356.7360847616326\n",
      "    - -887.2207239595191\n",
      "    - -961.3712689253897\n",
      "    - -1010.5076756714774\n",
      "    - -882.3601353910647\n",
      "    - -1326.5242198293895\n",
      "    - -1195.86857443263\n",
      "    - -1304.61484056051\n",
      "    - -887.355750405848\n",
      "    - -888.1754464332628\n",
      "    - -1070.8329914250883\n",
      "    - -895.6296677305105\n",
      "    - -888.2719538595254\n",
      "    - -1076.078203443007\n",
      "    - -1009.2511825203228\n",
      "    - -898.4064545548464\n",
      "    - -874.1461144781831\n",
      "    - -886.8000190606749\n",
      "    - -1221.1043860321447\n",
      "    - -963.0119903562573\n",
      "    - -868.0626873537883\n",
      "    - -1613.68946693013\n",
      "    - -885.5235321712189\n",
      "    - -1070.992189411859\n",
      "    - -1535.504084275916\n",
      "    - -1192.1325644965411\n",
      "    - -1684.6274698202774\n",
      "    - -961.7965132329848\n",
      "    - -904.0147753600123\n",
      "    - -943.5877029512922\n",
      "    - -1600.3555053885268\n",
      "    - -883.4253674806351\n",
      "    - -1307.538313366646\n",
      "    - -1698.2595367609165\n",
      "    - -1221.2195212168833\n",
      "    - -883.9686375672566\n",
      "    - -1279.123238120803\n",
      "    - -987.975491173103\n",
      "    - -891.39857741677\n",
      "    - -941.8204383856912\n",
      "    - -891.7323598221418\n",
      "    - -1359.81443203163\n",
      "    - -1291.183293017665\n",
      "    - -886.3235925018823\n",
      "    - -1096.3956657807964\n",
      "    - -1269.9073599685544\n",
      "    - -1255.7278733670505\n",
      "    - -1089.727286231475\n",
      "    - -890.7230253068784\n",
      "    - -885.6928338809512\n",
      "    - -1097.144975602754\n",
      "    - -885.5783087282715\n",
      "    - -1415.0087554462634\n",
      "    - -888.8790170877897\n",
      "    - -1590.848629528092\n",
      "    - -1650.6299894081837\n",
      "    - -984.1904724979952\n",
      "    - -1062.8151440452186\n",
      "    - -1021.1473965960909\n",
      "    - -1110.8536123800238\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1949065168336837\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15062454702447664\n",
      "    mean_inference_ms: 0.9924893597598917\n",
      "    mean_raw_obs_processing_ms: 0.12270772835667809\n",
      "time_since_restore: 1365.4094731807709\n",
      "time_this_iter_s: 13.693037033081055\n",
      "time_total_s: 1365.4094731807709\n",
      "timers:\n",
      "  learn_throughput: 689.039\n",
      "  learn_time_ms: 5805.185\n",
      "  load_throughput: 15200884.298\n",
      "  load_time_ms: 0.263\n",
      "  training_iteration_time_ms: 12707.078\n",
      "  update_time_ms: 2.58\n",
      "timestamp: 1660565288\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 512000\n",
      "training_iteration: 128\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 516000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 516000\n",
      "  num_agent_steps_trained: 516000\n",
      "  num_env_steps_sampled: 516000\n",
      "  num_env_steps_trained: 516000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-08-21\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -868.0626873537883\n",
      "episode_reward_mean: -1151.5656256229363\n",
      "episode_reward_min: -1730.081348527989\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2580\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5131568908691406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6797094345092773\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008552760817110538\n",
      "        model: {}\n",
      "        policy_loss: 0.011058087460696697\n",
      "        total_loss: 9.828786849975586\n",
      "        vf_explained_var: -0.0126132657751441\n",
      "        vf_loss: 9.813340187072754\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 516000\n",
      "  num_agent_steps_trained: 516000\n",
      "  num_env_steps_sampled: 516000\n",
      "  num_env_steps_trained: 516000\n",
      "iterations_since_restore: 129\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 516000\n",
      "num_agent_steps_trained: 516000\n",
      "num_env_steps_sampled: 516000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 516000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 64.37222222222222\n",
      "  ram_util_percent: 63.55555555555556\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19519990805757892\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1508280082741372\n",
      "  mean_inference_ms: 0.9940955306196293\n",
      "  mean_raw_obs_processing_ms: 0.1228647963482203\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -868.0626873537883\n",
      "  episode_reward_mean: -1151.5656256229363\n",
      "  episode_reward_min: -1730.081348527989\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -884.9080668002618\n",
      "    - -1012.4595079042411\n",
      "    - -1086.1239601346194\n",
      "    - -1075.9602882860431\n",
      "    - -1382.3931197042716\n",
      "    - -1370.3313187843744\n",
      "    - -1506.03634850755\n",
      "    - -1534.3960402667688\n",
      "    - -942.940133832068\n",
      "    - -1570.0820696945502\n",
      "    - -888.4056537643868\n",
      "    - -1554.9024074567253\n",
      "    - -886.4177681155383\n",
      "    - -1593.6928544365005\n",
      "    - -999.9061727532414\n",
      "    - -1651.6885062628037\n",
      "    - -885.0996935308693\n",
      "    - -1416.7617593906864\n",
      "    - -976.3917091186404\n",
      "    - -1000.1130553684137\n",
      "    - -1356.7360847616326\n",
      "    - -887.2207239595191\n",
      "    - -961.3712689253897\n",
      "    - -1010.5076756714774\n",
      "    - -882.3601353910647\n",
      "    - -1326.5242198293895\n",
      "    - -1195.86857443263\n",
      "    - -1304.61484056051\n",
      "    - -887.355750405848\n",
      "    - -888.1754464332628\n",
      "    - -1070.8329914250883\n",
      "    - -895.6296677305105\n",
      "    - -888.2719538595254\n",
      "    - -1076.078203443007\n",
      "    - -1009.2511825203228\n",
      "    - -898.4064545548464\n",
      "    - -874.1461144781831\n",
      "    - -886.8000190606749\n",
      "    - -1221.1043860321447\n",
      "    - -963.0119903562573\n",
      "    - -868.0626873537883\n",
      "    - -1613.68946693013\n",
      "    - -885.5235321712189\n",
      "    - -1070.992189411859\n",
      "    - -1535.504084275916\n",
      "    - -1192.1325644965411\n",
      "    - -1684.6274698202774\n",
      "    - -961.7965132329848\n",
      "    - -904.0147753600123\n",
      "    - -943.5877029512922\n",
      "    - -1600.3555053885268\n",
      "    - -883.4253674806351\n",
      "    - -1307.538313366646\n",
      "    - -1698.2595367609165\n",
      "    - -1221.2195212168833\n",
      "    - -883.9686375672566\n",
      "    - -1279.123238120803\n",
      "    - -987.975491173103\n",
      "    - -891.39857741677\n",
      "    - -941.8204383856912\n",
      "    - -891.7323598221418\n",
      "    - -1359.81443203163\n",
      "    - -1291.183293017665\n",
      "    - -886.3235925018823\n",
      "    - -1096.3956657807964\n",
      "    - -1269.9073599685544\n",
      "    - -1255.7278733670505\n",
      "    - -1089.727286231475\n",
      "    - -890.7230253068784\n",
      "    - -885.6928338809512\n",
      "    - -1097.144975602754\n",
      "    - -885.5783087282715\n",
      "    - -1415.0087554462634\n",
      "    - -888.8790170877897\n",
      "    - -1590.848629528092\n",
      "    - -1650.6299894081837\n",
      "    - -984.1904724979952\n",
      "    - -1062.8151440452186\n",
      "    - -1021.1473965960909\n",
      "    - -1110.8536123800238\n",
      "    - -1082.6054465711197\n",
      "    - -1286.1148289649645\n",
      "    - -888.2262578557066\n",
      "    - -890.506107247664\n",
      "    - -1486.622788045806\n",
      "    - -1628.2765975746372\n",
      "    - -884.4948046526705\n",
      "    - -1273.2369886499825\n",
      "    - -1500.6342397863411\n",
      "    - -1197.9814132513493\n",
      "    - -1066.5194165494818\n",
      "    - -987.5198199788204\n",
      "    - -1623.6124089677166\n",
      "    - -884.825295762026\n",
      "    - -875.4335640558404\n",
      "    - -985.2919489899616\n",
      "    - -1468.846074989645\n",
      "    - -1730.081348527989\n",
      "    - -1536.4299090876862\n",
      "    - -1196.685548729424\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19519990805757892\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1508280082741372\n",
      "    mean_inference_ms: 0.9940955306196293\n",
      "    mean_raw_obs_processing_ms: 0.1228647963482203\n",
      "time_since_restore: 1377.9543945789337\n",
      "time_this_iter_s: 12.544921398162842\n",
      "time_total_s: 1377.9543945789337\n",
      "timers:\n",
      "  learn_throughput: 676.998\n",
      "  learn_time_ms: 5908.436\n",
      "  load_throughput: 16540684.216\n",
      "  load_time_ms: 0.242\n",
      "  training_iteration_time_ms: 12933.04\n",
      "  update_time_ms: 2.562\n",
      "timestamp: 1660565301\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 516000\n",
      "training_iteration: 129\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 520000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 520000\n",
      "  num_agent_steps_trained: 520000\n",
      "  num_env_steps_sampled: 520000\n",
      "  num_env_steps_trained: 520000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-08-35\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -868.0626873537883\n",
      "episode_reward_mean: -1141.4778547376354\n",
      "episode_reward_min: -1792.4575483116955\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2600\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5131568908691406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.595286250114441\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014727375470101833\n",
      "        model: {}\n",
      "        policy_loss: 0.011110546067357063\n",
      "        total_loss: 9.797178268432617\n",
      "        vf_explained_var: -0.02102486416697502\n",
      "        vf_loss: 9.778511047363281\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 520000\n",
      "  num_agent_steps_trained: 520000\n",
      "  num_env_steps_sampled: 520000\n",
      "  num_env_steps_trained: 520000\n",
      "iterations_since_restore: 130\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 520000\n",
      "num_agent_steps_trained: 520000\n",
      "num_env_steps_sampled: 520000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 520000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 66.74285714285713\n",
      "  ram_util_percent: 63.07619047619048\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1955248789927613\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15104553016167332\n",
      "  mean_inference_ms: 0.9959555812818763\n",
      "  mean_raw_obs_processing_ms: 0.12303733431925107\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -868.0626873537883\n",
      "  episode_reward_mean: -1141.4778547376354\n",
      "  episode_reward_min: -1792.4575483116955\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1356.7360847616326\n",
      "    - -887.2207239595191\n",
      "    - -961.3712689253897\n",
      "    - -1010.5076756714774\n",
      "    - -882.3601353910647\n",
      "    - -1326.5242198293895\n",
      "    - -1195.86857443263\n",
      "    - -1304.61484056051\n",
      "    - -887.355750405848\n",
      "    - -888.1754464332628\n",
      "    - -1070.8329914250883\n",
      "    - -895.6296677305105\n",
      "    - -888.2719538595254\n",
      "    - -1076.078203443007\n",
      "    - -1009.2511825203228\n",
      "    - -898.4064545548464\n",
      "    - -874.1461144781831\n",
      "    - -886.8000190606749\n",
      "    - -1221.1043860321447\n",
      "    - -963.0119903562573\n",
      "    - -868.0626873537883\n",
      "    - -1613.68946693013\n",
      "    - -885.5235321712189\n",
      "    - -1070.992189411859\n",
      "    - -1535.504084275916\n",
      "    - -1192.1325644965411\n",
      "    - -1684.6274698202774\n",
      "    - -961.7965132329848\n",
      "    - -904.0147753600123\n",
      "    - -943.5877029512922\n",
      "    - -1600.3555053885268\n",
      "    - -883.4253674806351\n",
      "    - -1307.538313366646\n",
      "    - -1698.2595367609165\n",
      "    - -1221.2195212168833\n",
      "    - -883.9686375672566\n",
      "    - -1279.123238120803\n",
      "    - -987.975491173103\n",
      "    - -891.39857741677\n",
      "    - -941.8204383856912\n",
      "    - -891.7323598221418\n",
      "    - -1359.81443203163\n",
      "    - -1291.183293017665\n",
      "    - -886.3235925018823\n",
      "    - -1096.3956657807964\n",
      "    - -1269.9073599685544\n",
      "    - -1255.7278733670505\n",
      "    - -1089.727286231475\n",
      "    - -890.7230253068784\n",
      "    - -885.6928338809512\n",
      "    - -1097.144975602754\n",
      "    - -885.5783087282715\n",
      "    - -1415.0087554462634\n",
      "    - -888.8790170877897\n",
      "    - -1590.848629528092\n",
      "    - -1650.6299894081837\n",
      "    - -984.1904724979952\n",
      "    - -1062.8151440452186\n",
      "    - -1021.1473965960909\n",
      "    - -1110.8536123800238\n",
      "    - -1082.6054465711197\n",
      "    - -1286.1148289649645\n",
      "    - -888.2262578557066\n",
      "    - -890.506107247664\n",
      "    - -1486.622788045806\n",
      "    - -1628.2765975746372\n",
      "    - -884.4948046526705\n",
      "    - -1273.2369886499825\n",
      "    - -1500.6342397863411\n",
      "    - -1197.9814132513493\n",
      "    - -1066.5194165494818\n",
      "    - -987.5198199788204\n",
      "    - -1623.6124089677166\n",
      "    - -884.825295762026\n",
      "    - -875.4335640558404\n",
      "    - -985.2919489899616\n",
      "    - -1468.846074989645\n",
      "    - -1730.081348527989\n",
      "    - -1536.4299090876862\n",
      "    - -1196.685548729424\n",
      "    - -1741.4701578038564\n",
      "    - -1170.0693384558544\n",
      "    - -878.6225552854395\n",
      "    - -889.9730102670363\n",
      "    - -1098.0317044483556\n",
      "    - -989.6567496377435\n",
      "    - -1633.820414428138\n",
      "    - -1062.575291725945\n",
      "    - -1213.1528025365346\n",
      "    - -979.4852440263608\n",
      "    - -890.2855010566324\n",
      "    - -1047.9800195886532\n",
      "    - -1792.4575483116955\n",
      "    - -973.5847019452632\n",
      "    - -879.5286806002767\n",
      "    - -1309.1641412249849\n",
      "    - -891.0552022936024\n",
      "    - -1185.2022562500974\n",
      "    - -1533.8544362904722\n",
      "    - -1050.2635894055095\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1955248789927613\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15104553016167332\n",
      "    mean_inference_ms: 0.9959555812818763\n",
      "    mean_raw_obs_processing_ms: 0.12303733431925107\n",
      "time_since_restore: 1392.5646369457245\n",
      "time_this_iter_s: 14.610242366790771\n",
      "time_total_s: 1392.5646369457245\n",
      "timers:\n",
      "  learn_throughput: 667.066\n",
      "  learn_time_ms: 5996.405\n",
      "  load_throughput: 17294316.05\n",
      "  load_time_ms: 0.231\n",
      "  training_iteration_time_ms: 13223.021\n",
      "  update_time_ms: 2.553\n",
      "timestamp: 1660565315\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 520000\n",
      "training_iteration: 130\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 524000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 524000\n",
      "  num_agent_steps_trained: 524000\n",
      "  num_env_steps_sampled: 524000\n",
      "  num_env_steps_trained: 524000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-08-46\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -868.0626873537883\n",
      "episode_reward_mean: -1150.105079171894\n",
      "episode_reward_min: -1792.4575483116955\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2620\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5131568908691406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5741158723831177\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011242023669183254\n",
      "        model: {}\n",
      "        policy_loss: 0.010623407550156116\n",
      "        total_loss: 9.776352882385254\n",
      "        vf_explained_var: -0.020273204892873764\n",
      "        vf_loss: 9.759960174560547\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 524000\n",
      "  num_agent_steps_trained: 524000\n",
      "  num_env_steps_sampled: 524000\n",
      "  num_env_steps_trained: 524000\n",
      "iterations_since_restore: 131\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 524000\n",
      "num_agent_steps_trained: 524000\n",
      "num_env_steps_sampled: 524000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 524000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.586666666666666\n",
      "  ram_util_percent: 60.58\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19582317512468886\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15124725005219583\n",
      "  mean_inference_ms: 0.9976806602064272\n",
      "  mean_raw_obs_processing_ms: 0.12319850277657814\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -868.0626873537883\n",
      "  episode_reward_mean: -1150.105079171894\n",
      "  episode_reward_min: -1792.4575483116955\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -868.0626873537883\n",
      "    - -1613.68946693013\n",
      "    - -885.5235321712189\n",
      "    - -1070.992189411859\n",
      "    - -1535.504084275916\n",
      "    - -1192.1325644965411\n",
      "    - -1684.6274698202774\n",
      "    - -961.7965132329848\n",
      "    - -904.0147753600123\n",
      "    - -943.5877029512922\n",
      "    - -1600.3555053885268\n",
      "    - -883.4253674806351\n",
      "    - -1307.538313366646\n",
      "    - -1698.2595367609165\n",
      "    - -1221.2195212168833\n",
      "    - -883.9686375672566\n",
      "    - -1279.123238120803\n",
      "    - -987.975491173103\n",
      "    - -891.39857741677\n",
      "    - -941.8204383856912\n",
      "    - -891.7323598221418\n",
      "    - -1359.81443203163\n",
      "    - -1291.183293017665\n",
      "    - -886.3235925018823\n",
      "    - -1096.3956657807964\n",
      "    - -1269.9073599685544\n",
      "    - -1255.7278733670505\n",
      "    - -1089.727286231475\n",
      "    - -890.7230253068784\n",
      "    - -885.6928338809512\n",
      "    - -1097.144975602754\n",
      "    - -885.5783087282715\n",
      "    - -1415.0087554462634\n",
      "    - -888.8790170877897\n",
      "    - -1590.848629528092\n",
      "    - -1650.6299894081837\n",
      "    - -984.1904724979952\n",
      "    - -1062.8151440452186\n",
      "    - -1021.1473965960909\n",
      "    - -1110.8536123800238\n",
      "    - -1082.6054465711197\n",
      "    - -1286.1148289649645\n",
      "    - -888.2262578557066\n",
      "    - -890.506107247664\n",
      "    - -1486.622788045806\n",
      "    - -1628.2765975746372\n",
      "    - -884.4948046526705\n",
      "    - -1273.2369886499825\n",
      "    - -1500.6342397863411\n",
      "    - -1197.9814132513493\n",
      "    - -1066.5194165494818\n",
      "    - -987.5198199788204\n",
      "    - -1623.6124089677166\n",
      "    - -884.825295762026\n",
      "    - -875.4335640558404\n",
      "    - -985.2919489899616\n",
      "    - -1468.846074989645\n",
      "    - -1730.081348527989\n",
      "    - -1536.4299090876862\n",
      "    - -1196.685548729424\n",
      "    - -1741.4701578038564\n",
      "    - -1170.0693384558544\n",
      "    - -878.6225552854395\n",
      "    - -889.9730102670363\n",
      "    - -1098.0317044483556\n",
      "    - -989.6567496377435\n",
      "    - -1633.820414428138\n",
      "    - -1062.575291725945\n",
      "    - -1213.1528025365346\n",
      "    - -979.4852440263608\n",
      "    - -890.2855010566324\n",
      "    - -1047.9800195886532\n",
      "    - -1792.4575483116955\n",
      "    - -973.5847019452632\n",
      "    - -879.5286806002767\n",
      "    - -1309.1641412249849\n",
      "    - -891.0552022936024\n",
      "    - -1185.2022562500974\n",
      "    - -1533.8544362904722\n",
      "    - -1050.2635894055095\n",
      "    - -1623.125677061647\n",
      "    - -1082.2735389588506\n",
      "    - -1335.3390979155013\n",
      "    - -888.0440380270235\n",
      "    - -1023.7849111313776\n",
      "    - -907.9809441451173\n",
      "    - -1073.8721293362005\n",
      "    - -884.9551962839465\n",
      "    - -895.4948882229335\n",
      "    - -1246.4728713295217\n",
      "    - -1199.7710582712132\n",
      "    - -892.6964615460967\n",
      "    - -1169.5051102710001\n",
      "    - -1290.3094661014616\n",
      "    - -915.092334423397\n",
      "    - -878.1105216906641\n",
      "    - -1086.6091349039239\n",
      "    - -899.1052184194798\n",
      "    - -983.2530321256476\n",
      "    - -1071.1944970921613\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19582317512468886\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15124725005219583\n",
      "    mean_inference_ms: 0.9976806602064272\n",
      "    mean_raw_obs_processing_ms: 0.12319850277657814\n",
      "time_since_restore: 1402.7382884025574\n",
      "time_this_iter_s: 10.173651456832886\n",
      "time_total_s: 1402.7382884025574\n",
      "timers:\n",
      "  learn_throughput: 668.007\n",
      "  learn_time_ms: 5987.959\n",
      "  load_throughput: 17322886.939\n",
      "  load_time_ms: 0.231\n",
      "  training_iteration_time_ms: 13180.284\n",
      "  update_time_ms: 2.538\n",
      "timestamp: 1660565326\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 524000\n",
      "training_iteration: 131\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 528000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 528000\n",
      "  num_agent_steps_trained: 528000\n",
      "  num_env_steps_sampled: 528000\n",
      "  num_env_steps_trained: 528000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-08-55\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -875.4335640558404\n",
      "episode_reward_mean: -1151.1099737564734\n",
      "episode_reward_min: -1792.4575483116955\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2640\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5131568908691406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8809961080551147\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011362932622432709\n",
      "        model: {}\n",
      "        policy_loss: 0.009139683097600937\n",
      "        total_loss: 9.864213943481445\n",
      "        vf_explained_var: -0.01231660321354866\n",
      "        vf_loss: 9.849244117736816\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 528000\n",
      "  num_agent_steps_trained: 528000\n",
      "  num_env_steps_sampled: 528000\n",
      "  num_env_steps_trained: 528000\n",
      "iterations_since_restore: 132\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 528000\n",
      "num_agent_steps_trained: 528000\n",
      "num_env_steps_sampled: 528000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 528000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.128571428571426\n",
      "  ram_util_percent: 60.200000000000024\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1960366359876958\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15138194579919467\n",
      "  mean_inference_ms: 0.9988899222210622\n",
      "  mean_raw_obs_processing_ms: 0.1233053135302481\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -875.4335640558404\n",
      "  episode_reward_mean: -1151.1099737564734\n",
      "  episode_reward_min: -1792.4575483116955\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -891.7323598221418\n",
      "    - -1359.81443203163\n",
      "    - -1291.183293017665\n",
      "    - -886.3235925018823\n",
      "    - -1096.3956657807964\n",
      "    - -1269.9073599685544\n",
      "    - -1255.7278733670505\n",
      "    - -1089.727286231475\n",
      "    - -890.7230253068784\n",
      "    - -885.6928338809512\n",
      "    - -1097.144975602754\n",
      "    - -885.5783087282715\n",
      "    - -1415.0087554462634\n",
      "    - -888.8790170877897\n",
      "    - -1590.848629528092\n",
      "    - -1650.6299894081837\n",
      "    - -984.1904724979952\n",
      "    - -1062.8151440452186\n",
      "    - -1021.1473965960909\n",
      "    - -1110.8536123800238\n",
      "    - -1082.6054465711197\n",
      "    - -1286.1148289649645\n",
      "    - -888.2262578557066\n",
      "    - -890.506107247664\n",
      "    - -1486.622788045806\n",
      "    - -1628.2765975746372\n",
      "    - -884.4948046526705\n",
      "    - -1273.2369886499825\n",
      "    - -1500.6342397863411\n",
      "    - -1197.9814132513493\n",
      "    - -1066.5194165494818\n",
      "    - -987.5198199788204\n",
      "    - -1623.6124089677166\n",
      "    - -884.825295762026\n",
      "    - -875.4335640558404\n",
      "    - -985.2919489899616\n",
      "    - -1468.846074989645\n",
      "    - -1730.081348527989\n",
      "    - -1536.4299090876862\n",
      "    - -1196.685548729424\n",
      "    - -1741.4701578038564\n",
      "    - -1170.0693384558544\n",
      "    - -878.6225552854395\n",
      "    - -889.9730102670363\n",
      "    - -1098.0317044483556\n",
      "    - -989.6567496377435\n",
      "    - -1633.820414428138\n",
      "    - -1062.575291725945\n",
      "    - -1213.1528025365346\n",
      "    - -979.4852440263608\n",
      "    - -890.2855010566324\n",
      "    - -1047.9800195886532\n",
      "    - -1792.4575483116955\n",
      "    - -973.5847019452632\n",
      "    - -879.5286806002767\n",
      "    - -1309.1641412249849\n",
      "    - -891.0552022936024\n",
      "    - -1185.2022562500974\n",
      "    - -1533.8544362904722\n",
      "    - -1050.2635894055095\n",
      "    - -1623.125677061647\n",
      "    - -1082.2735389588506\n",
      "    - -1335.3390979155013\n",
      "    - -888.0440380270235\n",
      "    - -1023.7849111313776\n",
      "    - -907.9809441451173\n",
      "    - -1073.8721293362005\n",
      "    - -884.9551962839465\n",
      "    - -895.4948882229335\n",
      "    - -1246.4728713295217\n",
      "    - -1199.7710582712132\n",
      "    - -892.6964615460967\n",
      "    - -1169.5051102710001\n",
      "    - -1290.3094661014616\n",
      "    - -915.092334423397\n",
      "    - -878.1105216906641\n",
      "    - -1086.6091349039239\n",
      "    - -899.1052184194798\n",
      "    - -983.2530321256476\n",
      "    - -1071.1944970921613\n",
      "    - -1657.807320680647\n",
      "    - -951.5796334961925\n",
      "    - -881.2614680041104\n",
      "    - -892.7524438833788\n",
      "    - -897.6900433445263\n",
      "    - -1784.469707562838\n",
      "    - -976.0816489088897\n",
      "    - -1620.004572538884\n",
      "    - -1188.0506717018409\n",
      "    - -986.239045319927\n",
      "    - -1298.859268368624\n",
      "    - -1058.4018521813055\n",
      "    - -1436.1071018669086\n",
      "    - -1193.573823025394\n",
      "    - -1052.407475992999\n",
      "    - -1017.9522388490253\n",
      "    - -1210.4856735144965\n",
      "    - -1575.4818872254998\n",
      "    - -886.8652038432269\n",
      "    - -889.4339910304664\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1960366359876958\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15138194579919467\n",
      "    mean_inference_ms: 0.9988899222210622\n",
      "    mean_raw_obs_processing_ms: 0.1233053135302481\n",
      "time_since_restore: 1412.5137884616852\n",
      "time_this_iter_s: 9.775500059127808\n",
      "time_total_s: 1412.5137884616852\n",
      "timers:\n",
      "  learn_throughput: 667.941\n",
      "  learn_time_ms: 5988.55\n",
      "  load_throughput: 17374913.007\n",
      "  load_time_ms: 0.23\n",
      "  training_iteration_time_ms: 13182.278\n",
      "  update_time_ms: 2.558\n",
      "timestamp: 1660565335\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 528000\n",
      "training_iteration: 132\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 532000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 532000\n",
      "  num_agent_steps_trained: 532000\n",
      "  num_env_steps_sampled: 532000\n",
      "  num_env_steps_trained: 532000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-09-06\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -867.7653273259976\n",
      "episode_reward_mean: -1152.7369952918625\n",
      "episode_reward_min: -1792.4575483116955\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2660\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5131568908691406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5829243659973145\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009203490801155567\n",
      "        model: {}\n",
      "        policy_loss: 0.010651393793523312\n",
      "        total_loss: 9.884194374084473\n",
      "        vf_explained_var: -0.03523558750748634\n",
      "        vf_loss: 9.868821144104004\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 532000\n",
      "  num_agent_steps_trained: 532000\n",
      "  num_env_steps_sampled: 532000\n",
      "  num_env_steps_trained: 532000\n",
      "iterations_since_restore: 133\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 532000\n",
      "num_agent_steps_trained: 532000\n",
      "num_env_steps_sampled: 532000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 532000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 61.54666666666666\n",
      "  ram_util_percent: 60.27999999999999\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19614835318366267\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15144931757698024\n",
      "  mean_inference_ms: 0.9995739157011835\n",
      "  mean_raw_obs_processing_ms: 0.12335996738570905\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -867.7653273259976\n",
      "  episode_reward_mean: -1152.7369952918625\n",
      "  episode_reward_min: -1792.4575483116955\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1082.6054465711197\n",
      "    - -1286.1148289649645\n",
      "    - -888.2262578557066\n",
      "    - -890.506107247664\n",
      "    - -1486.622788045806\n",
      "    - -1628.2765975746372\n",
      "    - -884.4948046526705\n",
      "    - -1273.2369886499825\n",
      "    - -1500.6342397863411\n",
      "    - -1197.9814132513493\n",
      "    - -1066.5194165494818\n",
      "    - -987.5198199788204\n",
      "    - -1623.6124089677166\n",
      "    - -884.825295762026\n",
      "    - -875.4335640558404\n",
      "    - -985.2919489899616\n",
      "    - -1468.846074989645\n",
      "    - -1730.081348527989\n",
      "    - -1536.4299090876862\n",
      "    - -1196.685548729424\n",
      "    - -1741.4701578038564\n",
      "    - -1170.0693384558544\n",
      "    - -878.6225552854395\n",
      "    - -889.9730102670363\n",
      "    - -1098.0317044483556\n",
      "    - -989.6567496377435\n",
      "    - -1633.820414428138\n",
      "    - -1062.575291725945\n",
      "    - -1213.1528025365346\n",
      "    - -979.4852440263608\n",
      "    - -890.2855010566324\n",
      "    - -1047.9800195886532\n",
      "    - -1792.4575483116955\n",
      "    - -973.5847019452632\n",
      "    - -879.5286806002767\n",
      "    - -1309.1641412249849\n",
      "    - -891.0552022936024\n",
      "    - -1185.2022562500974\n",
      "    - -1533.8544362904722\n",
      "    - -1050.2635894055095\n",
      "    - -1623.125677061647\n",
      "    - -1082.2735389588506\n",
      "    - -1335.3390979155013\n",
      "    - -888.0440380270235\n",
      "    - -1023.7849111313776\n",
      "    - -907.9809441451173\n",
      "    - -1073.8721293362005\n",
      "    - -884.9551962839465\n",
      "    - -895.4948882229335\n",
      "    - -1246.4728713295217\n",
      "    - -1199.7710582712132\n",
      "    - -892.6964615460967\n",
      "    - -1169.5051102710001\n",
      "    - -1290.3094661014616\n",
      "    - -915.092334423397\n",
      "    - -878.1105216906641\n",
      "    - -1086.6091349039239\n",
      "    - -899.1052184194798\n",
      "    - -983.2530321256476\n",
      "    - -1071.1944970921613\n",
      "    - -1657.807320680647\n",
      "    - -951.5796334961925\n",
      "    - -881.2614680041104\n",
      "    - -892.7524438833788\n",
      "    - -897.6900433445263\n",
      "    - -1784.469707562838\n",
      "    - -976.0816489088897\n",
      "    - -1620.004572538884\n",
      "    - -1188.0506717018409\n",
      "    - -986.239045319927\n",
      "    - -1298.859268368624\n",
      "    - -1058.4018521813055\n",
      "    - -1436.1071018669086\n",
      "    - -1193.573823025394\n",
      "    - -1052.407475992999\n",
      "    - -1017.9522388490253\n",
      "    - -1210.4856735144965\n",
      "    - -1575.4818872254998\n",
      "    - -886.8652038432269\n",
      "    - -889.4339910304664\n",
      "    - -976.8296377197211\n",
      "    - -889.943780446886\n",
      "    - -908.2395181580499\n",
      "    - -867.7653273259976\n",
      "    - -1055.013960518101\n",
      "    - -913.4997220363521\n",
      "    - -1576.1928264922533\n",
      "    - -891.136059613374\n",
      "    - -1010.4511273932728\n",
      "    - -1192.3696493104887\n",
      "    - -901.3429702068134\n",
      "    - -1399.1691881316683\n",
      "    - -1629.363699849448\n",
      "    - -1498.027699949538\n",
      "    - -933.2234803025364\n",
      "    - -1669.870002601219\n",
      "    - -1180.3866278258167\n",
      "    - -890.3185138854212\n",
      "    - -1522.9806423574041\n",
      "    - -880.9017426442423\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19614835318366267\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15144931757698024\n",
      "    mean_inference_ms: 0.9995739157011835\n",
      "    mean_raw_obs_processing_ms: 0.12335996738570905\n",
      "time_since_restore: 1423.3982644081116\n",
      "time_this_iter_s: 10.884475946426392\n",
      "time_total_s: 1423.3982644081116\n",
      "timers:\n",
      "  learn_throughput: 740.897\n",
      "  learn_time_ms: 5398.858\n",
      "  load_throughput: 17571445.329\n",
      "  load_time_ms: 0.228\n",
      "  training_iteration_time_ms: 12286.601\n",
      "  update_time_ms: 2.411\n",
      "timestamp: 1660565346\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 532000\n",
      "training_iteration: 133\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 536000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 536000\n",
      "  num_agent_steps_trained: 536000\n",
      "  num_env_steps_sampled: 536000\n",
      "  num_env_steps_trained: 536000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-09-17\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -867.7653273259976\n",
      "episode_reward_mean: -1120.6527092648003\n",
      "episode_reward_min: -1792.4575483116955\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2680\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5131568908691406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5446374416351318\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017544396221637726\n",
      "        model: {}\n",
      "        policy_loss: 0.009222310967743397\n",
      "        total_loss: 9.84807014465332\n",
      "        vf_explained_var: -0.024985147640109062\n",
      "        vf_loss: 9.829845428466797\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 536000\n",
      "  num_agent_steps_trained: 536000\n",
      "  num_env_steps_sampled: 536000\n",
      "  num_env_steps_trained: 536000\n",
      "iterations_since_restore: 134\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 536000\n",
      "num_agent_steps_trained: 536000\n",
      "num_env_steps_sampled: 536000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 536000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 35.0625\n",
      "  ram_util_percent: 60.318749999999994\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19623296588223507\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1514988273740738\n",
      "  mean_inference_ms: 1.0001485922298872\n",
      "  mean_raw_obs_processing_ms: 0.12339641829257764\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -867.7653273259976\n",
      "  episode_reward_mean: -1120.6527092648003\n",
      "  episode_reward_min: -1792.4575483116955\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1741.4701578038564\n",
      "    - -1170.0693384558544\n",
      "    - -878.6225552854395\n",
      "    - -889.9730102670363\n",
      "    - -1098.0317044483556\n",
      "    - -989.6567496377435\n",
      "    - -1633.820414428138\n",
      "    - -1062.575291725945\n",
      "    - -1213.1528025365346\n",
      "    - -979.4852440263608\n",
      "    - -890.2855010566324\n",
      "    - -1047.9800195886532\n",
      "    - -1792.4575483116955\n",
      "    - -973.5847019452632\n",
      "    - -879.5286806002767\n",
      "    - -1309.1641412249849\n",
      "    - -891.0552022936024\n",
      "    - -1185.2022562500974\n",
      "    - -1533.8544362904722\n",
      "    - -1050.2635894055095\n",
      "    - -1623.125677061647\n",
      "    - -1082.2735389588506\n",
      "    - -1335.3390979155013\n",
      "    - -888.0440380270235\n",
      "    - -1023.7849111313776\n",
      "    - -907.9809441451173\n",
      "    - -1073.8721293362005\n",
      "    - -884.9551962839465\n",
      "    - -895.4948882229335\n",
      "    - -1246.4728713295217\n",
      "    - -1199.7710582712132\n",
      "    - -892.6964615460967\n",
      "    - -1169.5051102710001\n",
      "    - -1290.3094661014616\n",
      "    - -915.092334423397\n",
      "    - -878.1105216906641\n",
      "    - -1086.6091349039239\n",
      "    - -899.1052184194798\n",
      "    - -983.2530321256476\n",
      "    - -1071.1944970921613\n",
      "    - -1657.807320680647\n",
      "    - -951.5796334961925\n",
      "    - -881.2614680041104\n",
      "    - -892.7524438833788\n",
      "    - -897.6900433445263\n",
      "    - -1784.469707562838\n",
      "    - -976.0816489088897\n",
      "    - -1620.004572538884\n",
      "    - -1188.0506717018409\n",
      "    - -986.239045319927\n",
      "    - -1298.859268368624\n",
      "    - -1058.4018521813055\n",
      "    - -1436.1071018669086\n",
      "    - -1193.573823025394\n",
      "    - -1052.407475992999\n",
      "    - -1017.9522388490253\n",
      "    - -1210.4856735144965\n",
      "    - -1575.4818872254998\n",
      "    - -886.8652038432269\n",
      "    - -889.4339910304664\n",
      "    - -976.8296377197211\n",
      "    - -889.943780446886\n",
      "    - -908.2395181580499\n",
      "    - -867.7653273259976\n",
      "    - -1055.013960518101\n",
      "    - -913.4997220363521\n",
      "    - -1576.1928264922533\n",
      "    - -891.136059613374\n",
      "    - -1010.4511273932728\n",
      "    - -1192.3696493104887\n",
      "    - -901.3429702068134\n",
      "    - -1399.1691881316683\n",
      "    - -1629.363699849448\n",
      "    - -1498.027699949538\n",
      "    - -933.2234803025364\n",
      "    - -1669.870002601219\n",
      "    - -1180.3866278258167\n",
      "    - -890.3185138854212\n",
      "    - -1522.9806423574041\n",
      "    - -880.9017426442423\n",
      "    - -1275.7310654041548\n",
      "    - -1675.7153299241716\n",
      "    - -908.9117541938484\n",
      "    - -914.9868964447576\n",
      "    - -1284.6042946454404\n",
      "    - -880.2227428955422\n",
      "    - -892.5915245824457\n",
      "    - -1058.8129501165558\n",
      "    - -889.1952436455696\n",
      "    - -1103.1694226712355\n",
      "    - -924.7269363453631\n",
      "    - -885.7005762249055\n",
      "    - -1198.156240096691\n",
      "    - -1306.9438477433225\n",
      "    - -885.6662486571068\n",
      "    - -886.6627907840765\n",
      "    - -893.3639432720369\n",
      "    - -1538.241759966262\n",
      "    - -890.5830377696443\n",
      "    - -971.5296001494947\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19623296588223507\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1514988273740738\n",
      "    mean_inference_ms: 1.0001485922298872\n",
      "    mean_raw_obs_processing_ms: 0.12339641829257764\n",
      "time_since_restore: 1434.4790494441986\n",
      "time_this_iter_s: 11.080785036087036\n",
      "time_total_s: 1434.4790494441986\n",
      "timers:\n",
      "  learn_throughput: 760.59\n",
      "  learn_time_ms: 5259.074\n",
      "  load_throughput: 18084742.913\n",
      "  load_time_ms: 0.221\n",
      "  training_iteration_time_ms: 11966.499\n",
      "  update_time_ms: 2.37\n",
      "timestamp: 1660565357\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 536000\n",
      "training_iteration: 134\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 540000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 540000\n",
      "  num_agent_steps_trained: 540000\n",
      "  num_env_steps_sampled: 540000\n",
      "  num_env_steps_trained: 540000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-09-28\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -867.7653273259976\n",
      "episode_reward_mean: -1104.810787645939\n",
      "episode_reward_min: -1784.469707562838\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2700\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5131568908691406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.667747437953949\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010554214008152485\n",
      "        model: {}\n",
      "        policy_loss: 0.011788499541580677\n",
      "        total_loss: 9.84758186340332\n",
      "        vf_explained_var: -0.015016641467809677\n",
      "        vf_loss: 9.830378532409668\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 540000\n",
      "  num_agent_steps_trained: 540000\n",
      "  num_env_steps_sampled: 540000\n",
      "  num_env_steps_trained: 540000\n",
      "iterations_since_restore: 135\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 540000\n",
      "num_agent_steps_trained: 540000\n",
      "num_env_steps_sampled: 540000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 540000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.47333333333333\n",
      "  ram_util_percent: 60.22666666666668\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1962238596382011\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15149142015250544\n",
      "  mean_inference_ms: 1.0001591172286166\n",
      "  mean_raw_obs_processing_ms: 0.12338713405837204\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -867.7653273259976\n",
      "  episode_reward_mean: -1104.810787645939\n",
      "  episode_reward_min: -1784.469707562838\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1623.125677061647\n",
      "    - -1082.2735389588506\n",
      "    - -1335.3390979155013\n",
      "    - -888.0440380270235\n",
      "    - -1023.7849111313776\n",
      "    - -907.9809441451173\n",
      "    - -1073.8721293362005\n",
      "    - -884.9551962839465\n",
      "    - -895.4948882229335\n",
      "    - -1246.4728713295217\n",
      "    - -1199.7710582712132\n",
      "    - -892.6964615460967\n",
      "    - -1169.5051102710001\n",
      "    - -1290.3094661014616\n",
      "    - -915.092334423397\n",
      "    - -878.1105216906641\n",
      "    - -1086.6091349039239\n",
      "    - -899.1052184194798\n",
      "    - -983.2530321256476\n",
      "    - -1071.1944970921613\n",
      "    - -1657.807320680647\n",
      "    - -951.5796334961925\n",
      "    - -881.2614680041104\n",
      "    - -892.7524438833788\n",
      "    - -897.6900433445263\n",
      "    - -1784.469707562838\n",
      "    - -976.0816489088897\n",
      "    - -1620.004572538884\n",
      "    - -1188.0506717018409\n",
      "    - -986.239045319927\n",
      "    - -1298.859268368624\n",
      "    - -1058.4018521813055\n",
      "    - -1436.1071018669086\n",
      "    - -1193.573823025394\n",
      "    - -1052.407475992999\n",
      "    - -1017.9522388490253\n",
      "    - -1210.4856735144965\n",
      "    - -1575.4818872254998\n",
      "    - -886.8652038432269\n",
      "    - -889.4339910304664\n",
      "    - -976.8296377197211\n",
      "    - -889.943780446886\n",
      "    - -908.2395181580499\n",
      "    - -867.7653273259976\n",
      "    - -1055.013960518101\n",
      "    - -913.4997220363521\n",
      "    - -1576.1928264922533\n",
      "    - -891.136059613374\n",
      "    - -1010.4511273932728\n",
      "    - -1192.3696493104887\n",
      "    - -901.3429702068134\n",
      "    - -1399.1691881316683\n",
      "    - -1629.363699849448\n",
      "    - -1498.027699949538\n",
      "    - -933.2234803025364\n",
      "    - -1669.870002601219\n",
      "    - -1180.3866278258167\n",
      "    - -890.3185138854212\n",
      "    - -1522.9806423574041\n",
      "    - -880.9017426442423\n",
      "    - -1275.7310654041548\n",
      "    - -1675.7153299241716\n",
      "    - -908.9117541938484\n",
      "    - -914.9868964447576\n",
      "    - -1284.6042946454404\n",
      "    - -880.2227428955422\n",
      "    - -892.5915245824457\n",
      "    - -1058.8129501165558\n",
      "    - -889.1952436455696\n",
      "    - -1103.1694226712355\n",
      "    - -924.7269363453631\n",
      "    - -885.7005762249055\n",
      "    - -1198.156240096691\n",
      "    - -1306.9438477433225\n",
      "    - -885.6662486571068\n",
      "    - -886.6627907840765\n",
      "    - -893.3639432720369\n",
      "    - -1538.241759966262\n",
      "    - -890.5830377696443\n",
      "    - -971.5296001494947\n",
      "    - -891.1018175496581\n",
      "    - -988.8438762165939\n",
      "    - -1096.9964555514327\n",
      "    - -1705.6996593608442\n",
      "    - -914.2798755453567\n",
      "    - -985.161711138878\n",
      "    - -1183.5244532243232\n",
      "    - -1191.25708061539\n",
      "    - -890.678396644931\n",
      "    - -989.9619998607812\n",
      "    - -1291.699110734095\n",
      "    - -993.989027258439\n",
      "    - -995.7196771616143\n",
      "    - -1722.1880120353592\n",
      "    - -892.6501435783612\n",
      "    - -964.7474255056819\n",
      "    - -988.3825203254642\n",
      "    - -1038.5311297259577\n",
      "    - -893.371245492103\n",
      "    - -1007.2575661710343\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1962238596382011\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15149142015250544\n",
      "    mean_inference_ms: 1.0001591172286166\n",
      "    mean_raw_obs_processing_ms: 0.12338713405837204\n",
      "time_since_restore: 1445.1369338035583\n",
      "time_this_iter_s: 10.657884359359741\n",
      "time_total_s: 1445.1369338035583\n",
      "timers:\n",
      "  learn_throughput: 763.052\n",
      "  learn_time_ms: 5242.104\n",
      "  load_throughput: 17855700.298\n",
      "  load_time_ms: 0.224\n",
      "  training_iteration_time_ms: 11841.581\n",
      "  update_time_ms: 2.403\n",
      "timestamp: 1660565368\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 540000\n",
      "training_iteration: 135\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 544000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 544000\n",
      "  num_agent_steps_trained: 544000\n",
      "  num_env_steps_sampled: 544000\n",
      "  num_env_steps_trained: 544000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-09-38\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -867.7653273259976\n",
      "episode_reward_mean: -1109.2730029887473\n",
      "episode_reward_min: -1784.469707562838\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2720\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5131568908691406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7570973634719849\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016972851008176804\n",
      "        model: {}\n",
      "        policy_loss: 0.00680286530405283\n",
      "        total_loss: 9.838752746582031\n",
      "        vf_explained_var: -0.01814497634768486\n",
      "        vf_loss: 9.823240280151367\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 544000\n",
      "  num_agent_steps_trained: 544000\n",
      "  num_env_steps_sampled: 544000\n",
      "  num_env_steps_trained: 544000\n",
      "iterations_since_restore: 136\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 544000\n",
      "num_agent_steps_trained: 544000\n",
      "num_env_steps_sampled: 544000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 544000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.413333333333334\n",
      "  ram_util_percent: 60.446666666666665\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19622527286148816\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15148812192507444\n",
      "  mean_inference_ms: 1.0002080549072871\n",
      "  mean_raw_obs_processing_ms: 0.12337989490640427\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -867.7653273259976\n",
      "  episode_reward_mean: -1109.2730029887473\n",
      "  episode_reward_min: -1784.469707562838\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1657.807320680647\n",
      "    - -951.5796334961925\n",
      "    - -881.2614680041104\n",
      "    - -892.7524438833788\n",
      "    - -897.6900433445263\n",
      "    - -1784.469707562838\n",
      "    - -976.0816489088897\n",
      "    - -1620.004572538884\n",
      "    - -1188.0506717018409\n",
      "    - -986.239045319927\n",
      "    - -1298.859268368624\n",
      "    - -1058.4018521813055\n",
      "    - -1436.1071018669086\n",
      "    - -1193.573823025394\n",
      "    - -1052.407475992999\n",
      "    - -1017.9522388490253\n",
      "    - -1210.4856735144965\n",
      "    - -1575.4818872254998\n",
      "    - -886.8652038432269\n",
      "    - -889.4339910304664\n",
      "    - -976.8296377197211\n",
      "    - -889.943780446886\n",
      "    - -908.2395181580499\n",
      "    - -867.7653273259976\n",
      "    - -1055.013960518101\n",
      "    - -913.4997220363521\n",
      "    - -1576.1928264922533\n",
      "    - -891.136059613374\n",
      "    - -1010.4511273932728\n",
      "    - -1192.3696493104887\n",
      "    - -901.3429702068134\n",
      "    - -1399.1691881316683\n",
      "    - -1629.363699849448\n",
      "    - -1498.027699949538\n",
      "    - -933.2234803025364\n",
      "    - -1669.870002601219\n",
      "    - -1180.3866278258167\n",
      "    - -890.3185138854212\n",
      "    - -1522.9806423574041\n",
      "    - -880.9017426442423\n",
      "    - -1275.7310654041548\n",
      "    - -1675.7153299241716\n",
      "    - -908.9117541938484\n",
      "    - -914.9868964447576\n",
      "    - -1284.6042946454404\n",
      "    - -880.2227428955422\n",
      "    - -892.5915245824457\n",
      "    - -1058.8129501165558\n",
      "    - -889.1952436455696\n",
      "    - -1103.1694226712355\n",
      "    - -924.7269363453631\n",
      "    - -885.7005762249055\n",
      "    - -1198.156240096691\n",
      "    - -1306.9438477433225\n",
      "    - -885.6662486571068\n",
      "    - -886.6627907840765\n",
      "    - -893.3639432720369\n",
      "    - -1538.241759966262\n",
      "    - -890.5830377696443\n",
      "    - -971.5296001494947\n",
      "    - -891.1018175496581\n",
      "    - -988.8438762165939\n",
      "    - -1096.9964555514327\n",
      "    - -1705.6996593608442\n",
      "    - -914.2798755453567\n",
      "    - -985.161711138878\n",
      "    - -1183.5244532243232\n",
      "    - -1191.25708061539\n",
      "    - -890.678396644931\n",
      "    - -989.9619998607812\n",
      "    - -1291.699110734095\n",
      "    - -993.989027258439\n",
      "    - -995.7196771616143\n",
      "    - -1722.1880120353592\n",
      "    - -892.6501435783612\n",
      "    - -964.7474255056819\n",
      "    - -988.3825203254642\n",
      "    - -1038.5311297259577\n",
      "    - -893.371245492103\n",
      "    - -1007.2575661710343\n",
      "    - -1693.3753506384862\n",
      "    - -885.871458318439\n",
      "    - -883.5795202087377\n",
      "    - -986.7077977492555\n",
      "    - -1533.991986458041\n",
      "    - -897.5431877770819\n",
      "    - -967.5587821124977\n",
      "    - -942.6694848773506\n",
      "    - -891.9080765822731\n",
      "    - -1177.882595346043\n",
      "    - -1068.0526397295919\n",
      "    - -938.5046963358227\n",
      "    - -1068.90059121492\n",
      "    - -1197.1938305258268\n",
      "    - -894.6364866031613\n",
      "    - -1418.7862318899113\n",
      "    - -1386.8104189579815\n",
      "    - -888.6308611535331\n",
      "    - -1167.0096040658336\n",
      "    - -903.5980609932186\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19622527286148816\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15148812192507444\n",
      "    mean_inference_ms: 1.0002080549072871\n",
      "    mean_raw_obs_processing_ms: 0.12337989490640427\n",
      "time_since_restore: 1455.2820887565613\n",
      "time_this_iter_s: 10.14515495300293\n",
      "time_total_s: 1455.2820887565613\n",
      "timers:\n",
      "  learn_throughput: 787.451\n",
      "  learn_time_ms: 5079.679\n",
      "  load_throughput: 17834820.878\n",
      "  load_time_ms: 0.224\n",
      "  training_iteration_time_ms: 11650.122\n",
      "  update_time_ms: 2.388\n",
      "timestamp: 1660565378\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 544000\n",
      "training_iteration: 136\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 548000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 548000\n",
      "  num_agent_steps_trained: 548000\n",
      "  num_env_steps_sampled: 548000\n",
      "  num_env_steps_trained: 548000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-09-48\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -867.7653273259976\n",
      "episode_reward_mean: -1111.966701813654\n",
      "episode_reward_min: -1731.9956532874005\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2740\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5131568908691406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.2473065853118896\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01633284240961075\n",
      "        model: {}\n",
      "        policy_loss: 0.008033656515181065\n",
      "        total_loss: 9.85188102722168\n",
      "        vf_explained_var: -0.010200570337474346\n",
      "        vf_loss: 9.835465431213379\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 548000\n",
      "  num_agent_steps_trained: 548000\n",
      "  num_env_steps_sampled: 548000\n",
      "  num_env_steps_trained: 548000\n",
      "iterations_since_restore: 137\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 548000\n",
      "num_agent_steps_trained: 548000\n",
      "num_env_steps_sampled: 548000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 548000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.07857142857143\n",
      "  ram_util_percent: 60.35714285714284\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19621512779056124\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1514826298062616\n",
      "  mean_inference_ms: 1.0002334992677657\n",
      "  mean_raw_obs_processing_ms: 0.12337247105609161\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -867.7653273259976\n",
      "  episode_reward_mean: -1111.966701813654\n",
      "  episode_reward_min: -1731.9956532874005\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -976.8296377197211\n",
      "    - -889.943780446886\n",
      "    - -908.2395181580499\n",
      "    - -867.7653273259976\n",
      "    - -1055.013960518101\n",
      "    - -913.4997220363521\n",
      "    - -1576.1928264922533\n",
      "    - -891.136059613374\n",
      "    - -1010.4511273932728\n",
      "    - -1192.3696493104887\n",
      "    - -901.3429702068134\n",
      "    - -1399.1691881316683\n",
      "    - -1629.363699849448\n",
      "    - -1498.027699949538\n",
      "    - -933.2234803025364\n",
      "    - -1669.870002601219\n",
      "    - -1180.3866278258167\n",
      "    - -890.3185138854212\n",
      "    - -1522.9806423574041\n",
      "    - -880.9017426442423\n",
      "    - -1275.7310654041548\n",
      "    - -1675.7153299241716\n",
      "    - -908.9117541938484\n",
      "    - -914.9868964447576\n",
      "    - -1284.6042946454404\n",
      "    - -880.2227428955422\n",
      "    - -892.5915245824457\n",
      "    - -1058.8129501165558\n",
      "    - -889.1952436455696\n",
      "    - -1103.1694226712355\n",
      "    - -924.7269363453631\n",
      "    - -885.7005762249055\n",
      "    - -1198.156240096691\n",
      "    - -1306.9438477433225\n",
      "    - -885.6662486571068\n",
      "    - -886.6627907840765\n",
      "    - -893.3639432720369\n",
      "    - -1538.241759966262\n",
      "    - -890.5830377696443\n",
      "    - -971.5296001494947\n",
      "    - -891.1018175496581\n",
      "    - -988.8438762165939\n",
      "    - -1096.9964555514327\n",
      "    - -1705.6996593608442\n",
      "    - -914.2798755453567\n",
      "    - -985.161711138878\n",
      "    - -1183.5244532243232\n",
      "    - -1191.25708061539\n",
      "    - -890.678396644931\n",
      "    - -989.9619998607812\n",
      "    - -1291.699110734095\n",
      "    - -993.989027258439\n",
      "    - -995.7196771616143\n",
      "    - -1722.1880120353592\n",
      "    - -892.6501435783612\n",
      "    - -964.7474255056819\n",
      "    - -988.3825203254642\n",
      "    - -1038.5311297259577\n",
      "    - -893.371245492103\n",
      "    - -1007.2575661710343\n",
      "    - -1693.3753506384862\n",
      "    - -885.871458318439\n",
      "    - -883.5795202087377\n",
      "    - -986.7077977492555\n",
      "    - -1533.991986458041\n",
      "    - -897.5431877770819\n",
      "    - -967.5587821124977\n",
      "    - -942.6694848773506\n",
      "    - -891.9080765822731\n",
      "    - -1177.882595346043\n",
      "    - -1068.0526397295919\n",
      "    - -938.5046963358227\n",
      "    - -1068.90059121492\n",
      "    - -1197.1938305258268\n",
      "    - -894.6364866031613\n",
      "    - -1418.7862318899113\n",
      "    - -1386.8104189579815\n",
      "    - -888.6308611535331\n",
      "    - -1167.0096040658336\n",
      "    - -903.5980609932186\n",
      "    - -1731.9956532874005\n",
      "    - -1285.7103934440597\n",
      "    - -1316.196680194159\n",
      "    - -980.8085991233505\n",
      "    - -1253.552890520307\n",
      "    - -905.2172096565674\n",
      "    - -1067.0333923062776\n",
      "    - -1612.6764077992907\n",
      "    - -1235.7011280348495\n",
      "    - -1497.310233671297\n",
      "    - -1573.0313457931309\n",
      "    - -1030.1981165603927\n",
      "    - -1503.2482678266092\n",
      "    - -889.2287204658791\n",
      "    - -1052.3105068488617\n",
      "    - -890.6588968847192\n",
      "    - -1026.1627444904773\n",
      "    - -874.7506394950451\n",
      "    - -1097.8739383391955\n",
      "    - -901.2091890880025\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19621512779056124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1514826298062616\n",
      "    mean_inference_ms: 1.0002334992677657\n",
      "    mean_raw_obs_processing_ms: 0.12337247105609161\n",
      "time_since_restore: 1464.8740236759186\n",
      "time_this_iter_s: 9.5919349193573\n",
      "time_total_s: 1464.8740236759186\n",
      "timers:\n",
      "  learn_throughput: 811.272\n",
      "  learn_time_ms: 4930.526\n",
      "  load_throughput: 18090593.056\n",
      "  load_time_ms: 0.221\n",
      "  training_iteration_time_ms: 11308.567\n",
      "  update_time_ms: 2.337\n",
      "timestamp: 1660565388\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 548000\n",
      "training_iteration: 137\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 552000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 552000\n",
      "  num_agent_steps_trained: 552000\n",
      "  num_env_steps_sampled: 552000\n",
      "  num_env_steps_trained: 552000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-09-58\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -874.7506394950451\n",
      "episode_reward_mean: -1114.5959893129302\n",
      "episode_reward_min: -1731.9956532874005\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2760\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5131568908691406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7319647073745728\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013525859452784061\n",
      "        model: {}\n",
      "        policy_loss: 0.013574597425758839\n",
      "        total_loss: 9.830316543579102\n",
      "        vf_explained_var: -0.021155361086130142\n",
      "        vf_loss: 9.809800148010254\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 552000\n",
      "  num_agent_steps_trained: 552000\n",
      "  num_env_steps_sampled: 552000\n",
      "  num_env_steps_trained: 552000\n",
      "iterations_since_restore: 138\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 552000\n",
      "num_agent_steps_trained: 552000\n",
      "num_env_steps_sampled: 552000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 552000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 48.807692307692314\n",
      "  ram_util_percent: 60.23846153846154\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19618677698770562\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15146223636179137\n",
      "  mean_inference_ms: 1.0001434995627574\n",
      "  mean_raw_obs_processing_ms: 0.1233537912059583\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -874.7506394950451\n",
      "  episode_reward_mean: -1114.5959893129302\n",
      "  episode_reward_min: -1731.9956532874005\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1275.7310654041548\n",
      "    - -1675.7153299241716\n",
      "    - -908.9117541938484\n",
      "    - -914.9868964447576\n",
      "    - -1284.6042946454404\n",
      "    - -880.2227428955422\n",
      "    - -892.5915245824457\n",
      "    - -1058.8129501165558\n",
      "    - -889.1952436455696\n",
      "    - -1103.1694226712355\n",
      "    - -924.7269363453631\n",
      "    - -885.7005762249055\n",
      "    - -1198.156240096691\n",
      "    - -1306.9438477433225\n",
      "    - -885.6662486571068\n",
      "    - -886.6627907840765\n",
      "    - -893.3639432720369\n",
      "    - -1538.241759966262\n",
      "    - -890.5830377696443\n",
      "    - -971.5296001494947\n",
      "    - -891.1018175496581\n",
      "    - -988.8438762165939\n",
      "    - -1096.9964555514327\n",
      "    - -1705.6996593608442\n",
      "    - -914.2798755453567\n",
      "    - -985.161711138878\n",
      "    - -1183.5244532243232\n",
      "    - -1191.25708061539\n",
      "    - -890.678396644931\n",
      "    - -989.9619998607812\n",
      "    - -1291.699110734095\n",
      "    - -993.989027258439\n",
      "    - -995.7196771616143\n",
      "    - -1722.1880120353592\n",
      "    - -892.6501435783612\n",
      "    - -964.7474255056819\n",
      "    - -988.3825203254642\n",
      "    - -1038.5311297259577\n",
      "    - -893.371245492103\n",
      "    - -1007.2575661710343\n",
      "    - -1693.3753506384862\n",
      "    - -885.871458318439\n",
      "    - -883.5795202087377\n",
      "    - -986.7077977492555\n",
      "    - -1533.991986458041\n",
      "    - -897.5431877770819\n",
      "    - -967.5587821124977\n",
      "    - -942.6694848773506\n",
      "    - -891.9080765822731\n",
      "    - -1177.882595346043\n",
      "    - -1068.0526397295919\n",
      "    - -938.5046963358227\n",
      "    - -1068.90059121492\n",
      "    - -1197.1938305258268\n",
      "    - -894.6364866031613\n",
      "    - -1418.7862318899113\n",
      "    - -1386.8104189579815\n",
      "    - -888.6308611535331\n",
      "    - -1167.0096040658336\n",
      "    - -903.5980609932186\n",
      "    - -1731.9956532874005\n",
      "    - -1285.7103934440597\n",
      "    - -1316.196680194159\n",
      "    - -980.8085991233505\n",
      "    - -1253.552890520307\n",
      "    - -905.2172096565674\n",
      "    - -1067.0333923062776\n",
      "    - -1612.6764077992907\n",
      "    - -1235.7011280348495\n",
      "    - -1497.310233671297\n",
      "    - -1573.0313457931309\n",
      "    - -1030.1981165603927\n",
      "    - -1503.2482678266092\n",
      "    - -889.2287204658791\n",
      "    - -1052.3105068488617\n",
      "    - -890.6588968847192\n",
      "    - -1026.1627444904773\n",
      "    - -874.7506394950451\n",
      "    - -1097.8739383391955\n",
      "    - -901.2091890880025\n",
      "    - -1338.319809570784\n",
      "    - -924.3744054821954\n",
      "    - -1547.126641955675\n",
      "    - -1076.8695307460805\n",
      "    - -965.4945414431063\n",
      "    - -1611.9804443984976\n",
      "    - -1012.9187842652138\n",
      "    - -895.264985831948\n",
      "    - -1567.06631999244\n",
      "    - -891.768923212791\n",
      "    - -892.4800459113643\n",
      "    - -1571.5139749483615\n",
      "    - -994.1251585539895\n",
      "    - -884.727031976493\n",
      "    - -984.5795025040751\n",
      "    - -1304.6563689120585\n",
      "    - -1100.3187711589499\n",
      "    - -1016.2725258419459\n",
      "    - -1556.2058627954543\n",
      "    - -913.8912971947939\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19618677698770562\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15146223636179137\n",
      "    mean_inference_ms: 1.0001434995627574\n",
      "    mean_raw_obs_processing_ms: 0.1233537912059583\n",
      "time_since_restore: 1474.5652809143066\n",
      "time_this_iter_s: 9.691257238388062\n",
      "time_total_s: 1474.5652809143066\n",
      "timers:\n",
      "  learn_throughput: 839.412\n",
      "  learn_time_ms: 4765.24\n",
      "  load_throughput: 18418285.212\n",
      "  load_time_ms: 0.217\n",
      "  training_iteration_time_ms: 10908.537\n",
      "  update_time_ms: 2.357\n",
      "timestamp: 1660565398\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 552000\n",
      "training_iteration: 138\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 556000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 556000\n",
      "  num_agent_steps_trained: 556000\n",
      "  num_env_steps_sampled: 556000\n",
      "  num_env_steps_trained: 556000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-10-07\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -874.7506394950451\n",
      "episode_reward_mean: -1151.9593266580582\n",
      "episode_reward_min: -1731.9956532874005\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2780\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5131568908691406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.132903575897217\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012800469994544983\n",
      "        model: {}\n",
      "        policy_loss: 0.007126687094569206\n",
      "        total_loss: 9.780449867248535\n",
      "        vf_explained_var: -0.014779831282794476\n",
      "        vf_loss: 9.766754150390625\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 556000\n",
      "  num_agent_steps_trained: 556000\n",
      "  num_env_steps_sampled: 556000\n",
      "  num_env_steps_trained: 556000\n",
      "iterations_since_restore: 139\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 556000\n",
      "num_agent_steps_trained: 556000\n",
      "num_env_steps_sampled: 556000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 556000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 10.12142857142857\n",
      "  ram_util_percent: 60.27857142857141\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19610506993282023\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15140632907391308\n",
      "  mean_inference_ms: 0.9997463423210857\n",
      "  mean_raw_obs_processing_ms: 0.1233095448383999\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -874.7506394950451\n",
      "  episode_reward_mean: -1151.9593266580582\n",
      "  episode_reward_min: -1731.9956532874005\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -891.1018175496581\n",
      "    - -988.8438762165939\n",
      "    - -1096.9964555514327\n",
      "    - -1705.6996593608442\n",
      "    - -914.2798755453567\n",
      "    - -985.161711138878\n",
      "    - -1183.5244532243232\n",
      "    - -1191.25708061539\n",
      "    - -890.678396644931\n",
      "    - -989.9619998607812\n",
      "    - -1291.699110734095\n",
      "    - -993.989027258439\n",
      "    - -995.7196771616143\n",
      "    - -1722.1880120353592\n",
      "    - -892.6501435783612\n",
      "    - -964.7474255056819\n",
      "    - -988.3825203254642\n",
      "    - -1038.5311297259577\n",
      "    - -893.371245492103\n",
      "    - -1007.2575661710343\n",
      "    - -1693.3753506384862\n",
      "    - -885.871458318439\n",
      "    - -883.5795202087377\n",
      "    - -986.7077977492555\n",
      "    - -1533.991986458041\n",
      "    - -897.5431877770819\n",
      "    - -967.5587821124977\n",
      "    - -942.6694848773506\n",
      "    - -891.9080765822731\n",
      "    - -1177.882595346043\n",
      "    - -1068.0526397295919\n",
      "    - -938.5046963358227\n",
      "    - -1068.90059121492\n",
      "    - -1197.1938305258268\n",
      "    - -894.6364866031613\n",
      "    - -1418.7862318899113\n",
      "    - -1386.8104189579815\n",
      "    - -888.6308611535331\n",
      "    - -1167.0096040658336\n",
      "    - -903.5980609932186\n",
      "    - -1731.9956532874005\n",
      "    - -1285.7103934440597\n",
      "    - -1316.196680194159\n",
      "    - -980.8085991233505\n",
      "    - -1253.552890520307\n",
      "    - -905.2172096565674\n",
      "    - -1067.0333923062776\n",
      "    - -1612.6764077992907\n",
      "    - -1235.7011280348495\n",
      "    - -1497.310233671297\n",
      "    - -1573.0313457931309\n",
      "    - -1030.1981165603927\n",
      "    - -1503.2482678266092\n",
      "    - -889.2287204658791\n",
      "    - -1052.3105068488617\n",
      "    - -890.6588968847192\n",
      "    - -1026.1627444904773\n",
      "    - -874.7506394950451\n",
      "    - -1097.8739383391955\n",
      "    - -901.2091890880025\n",
      "    - -1338.319809570784\n",
      "    - -924.3744054821954\n",
      "    - -1547.126641955675\n",
      "    - -1076.8695307460805\n",
      "    - -965.4945414431063\n",
      "    - -1611.9804443984976\n",
      "    - -1012.9187842652138\n",
      "    - -895.264985831948\n",
      "    - -1567.06631999244\n",
      "    - -891.768923212791\n",
      "    - -892.4800459113643\n",
      "    - -1571.5139749483615\n",
      "    - -994.1251585539895\n",
      "    - -884.727031976493\n",
      "    - -984.5795025040751\n",
      "    - -1304.6563689120585\n",
      "    - -1100.3187711589499\n",
      "    - -1016.2725258419459\n",
      "    - -1556.2058627954543\n",
      "    - -913.8912971947939\n",
      "    - -1325.7721567706576\n",
      "    - -994.1319072096975\n",
      "    - -1069.5498064520139\n",
      "    - -1723.4981832778358\n",
      "    - -1682.4274194856694\n",
      "    - -892.0227514359974\n",
      "    - -989.0944351567309\n",
      "    - -1222.8561566883027\n",
      "    - -1721.454976341572\n",
      "    - -1571.2899640051476\n",
      "    - -890.495175942533\n",
      "    - -1554.0762093511469\n",
      "    - -1522.790971397533\n",
      "    - -1408.00858285526\n",
      "    - -888.4647083066299\n",
      "    - -1065.549002090792\n",
      "    - -985.0800205855986\n",
      "    - -902.9269780532229\n",
      "    - -894.0880366080055\n",
      "    - -1698.2724980310825\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19610506993282023\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15140632907391308\n",
      "    mean_inference_ms: 0.9997463423210857\n",
      "    mean_raw_obs_processing_ms: 0.1233095448383999\n",
      "time_since_restore: 1484.0119988918304\n",
      "time_this_iter_s: 9.446717977523804\n",
      "time_total_s: 1484.0119988918304\n",
      "timers:\n",
      "  learn_throughput: 867.282\n",
      "  learn_time_ms: 4612.112\n",
      "  load_throughput: 18400105.286\n",
      "  load_time_ms: 0.217\n",
      "  training_iteration_time_ms: 10598.865\n",
      "  update_time_ms: 2.336\n",
      "timestamp: 1660565407\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 556000\n",
      "training_iteration: 139\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 560000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 560000\n",
      "  num_agent_steps_trained: 560000\n",
      "  num_env_steps_sampled: 560000\n",
      "  num_env_steps_trained: 560000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-10-17\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -874.7506394950451\n",
      "episode_reward_mean: -1152.7935964939475\n",
      "episode_reward_min: -1731.9956532874005\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2800\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5131568908691406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7987483143806458\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012838960625231266\n",
      "        model: {}\n",
      "        policy_loss: 0.011384526267647743\n",
      "        total_loss: 9.669679641723633\n",
      "        vf_explained_var: -0.026778843253850937\n",
      "        vf_loss: 9.651707649230957\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 560000\n",
      "  num_agent_steps_trained: 560000\n",
      "  num_env_steps_sampled: 560000\n",
      "  num_env_steps_trained: 560000\n",
      "iterations_since_restore: 140\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 560000\n",
      "num_agent_steps_trained: 560000\n",
      "num_env_steps_sampled: 560000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 560000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 22.95\n",
      "  ram_util_percent: 60.307142857142836\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19601538532422721\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15134003636849466\n",
      "  mean_inference_ms: 0.9992789486790841\n",
      "  mean_raw_obs_processing_ms: 0.12325490863325207\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -874.7506394950451\n",
      "  episode_reward_mean: -1152.7935964939475\n",
      "  episode_reward_min: -1731.9956532874005\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1693.3753506384862\n",
      "    - -885.871458318439\n",
      "    - -883.5795202087377\n",
      "    - -986.7077977492555\n",
      "    - -1533.991986458041\n",
      "    - -897.5431877770819\n",
      "    - -967.5587821124977\n",
      "    - -942.6694848773506\n",
      "    - -891.9080765822731\n",
      "    - -1177.882595346043\n",
      "    - -1068.0526397295919\n",
      "    - -938.5046963358227\n",
      "    - -1068.90059121492\n",
      "    - -1197.1938305258268\n",
      "    - -894.6364866031613\n",
      "    - -1418.7862318899113\n",
      "    - -1386.8104189579815\n",
      "    - -888.6308611535331\n",
      "    - -1167.0096040658336\n",
      "    - -903.5980609932186\n",
      "    - -1731.9956532874005\n",
      "    - -1285.7103934440597\n",
      "    - -1316.196680194159\n",
      "    - -980.8085991233505\n",
      "    - -1253.552890520307\n",
      "    - -905.2172096565674\n",
      "    - -1067.0333923062776\n",
      "    - -1612.6764077992907\n",
      "    - -1235.7011280348495\n",
      "    - -1497.310233671297\n",
      "    - -1573.0313457931309\n",
      "    - -1030.1981165603927\n",
      "    - -1503.2482678266092\n",
      "    - -889.2287204658791\n",
      "    - -1052.3105068488617\n",
      "    - -890.6588968847192\n",
      "    - -1026.1627444904773\n",
      "    - -874.7506394950451\n",
      "    - -1097.8739383391955\n",
      "    - -901.2091890880025\n",
      "    - -1338.319809570784\n",
      "    - -924.3744054821954\n",
      "    - -1547.126641955675\n",
      "    - -1076.8695307460805\n",
      "    - -965.4945414431063\n",
      "    - -1611.9804443984976\n",
      "    - -1012.9187842652138\n",
      "    - -895.264985831948\n",
      "    - -1567.06631999244\n",
      "    - -891.768923212791\n",
      "    - -892.4800459113643\n",
      "    - -1571.5139749483615\n",
      "    - -994.1251585539895\n",
      "    - -884.727031976493\n",
      "    - -984.5795025040751\n",
      "    - -1304.6563689120585\n",
      "    - -1100.3187711589499\n",
      "    - -1016.2725258419459\n",
      "    - -1556.2058627954543\n",
      "    - -913.8912971947939\n",
      "    - -1325.7721567706576\n",
      "    - -994.1319072096975\n",
      "    - -1069.5498064520139\n",
      "    - -1723.4981832778358\n",
      "    - -1682.4274194856694\n",
      "    - -892.0227514359974\n",
      "    - -989.0944351567309\n",
      "    - -1222.8561566883027\n",
      "    - -1721.454976341572\n",
      "    - -1571.2899640051476\n",
      "    - -890.495175942533\n",
      "    - -1554.0762093511469\n",
      "    - -1522.790971397533\n",
      "    - -1408.00858285526\n",
      "    - -888.4647083066299\n",
      "    - -1065.549002090792\n",
      "    - -985.0800205855986\n",
      "    - -902.9269780532229\n",
      "    - -894.0880366080055\n",
      "    - -1698.2724980310825\n",
      "    - -1299.8667358873192\n",
      "    - -1034.9038854685962\n",
      "    - -1549.1707891155907\n",
      "    - -882.1675938573442\n",
      "    - -888.1080039138672\n",
      "    - -910.1407850284602\n",
      "    - -1675.6515070248581\n",
      "    - -874.9864408683121\n",
      "    - -1180.686797389787\n",
      "    - -894.1842498374639\n",
      "    - -891.8951674996224\n",
      "    - -901.2140602438221\n",
      "    - -1378.8579782178056\n",
      "    - -880.397305519999\n",
      "    - -885.1078441714999\n",
      "    - -900.2653449764849\n",
      "    - -1172.5306813262596\n",
      "    - -1065.685349291034\n",
      "    - -1422.1292112208325\n",
      "    - -1021.5184364262664\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19601538532422721\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15134003636849466\n",
      "    mean_inference_ms: 0.9992789486790841\n",
      "    mean_raw_obs_processing_ms: 0.12325490863325207\n",
      "time_since_restore: 1493.570971250534\n",
      "time_this_iter_s: 9.558972358703613\n",
      "time_total_s: 1493.570971250534\n",
      "timers:\n",
      "  learn_throughput: 922.74\n",
      "  learn_time_ms: 4334.914\n",
      "  load_throughput: 18347786.527\n",
      "  load_time_ms: 0.218\n",
      "  training_iteration_time_ms: 10093.733\n",
      "  update_time_ms: 2.317\n",
      "timestamp: 1660565417\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 560000\n",
      "training_iteration: 140\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 564000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 564000\n",
      "  num_agent_steps_trained: 564000\n",
      "  num_env_steps_sampled: 564000\n",
      "  num_env_steps_trained: 564000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-10-26\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -874.7506394950451\n",
      "episode_reward_mean: -1163.1526730197381\n",
      "episode_reward_min: -1731.9956532874005\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2820\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5131568908691406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5886855125427246\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020421691238880157\n",
      "        model: {}\n",
      "        policy_loss: 0.015742000192403793\n",
      "        total_loss: 9.780407905578613\n",
      "        vf_explained_var: -0.019787553697824478\n",
      "        vf_loss: 9.754186630249023\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 564000\n",
      "  num_agent_steps_trained: 564000\n",
      "  num_env_steps_sampled: 564000\n",
      "  num_env_steps_trained: 564000\n",
      "iterations_since_restore: 141\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 564000\n",
      "num_agent_steps_trained: 564000\n",
      "num_env_steps_sampled: 564000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 564000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.292307692307695\n",
      "  ram_util_percent: 60.29230769230767\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.195900572782392\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15125820419407585\n",
      "  mean_inference_ms: 0.9986917275431668\n",
      "  mean_raw_obs_processing_ms: 0.12318823687725913\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -874.7506394950451\n",
      "  episode_reward_mean: -1163.1526730197381\n",
      "  episode_reward_min: -1731.9956532874005\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1731.9956532874005\n",
      "    - -1285.7103934440597\n",
      "    - -1316.196680194159\n",
      "    - -980.8085991233505\n",
      "    - -1253.552890520307\n",
      "    - -905.2172096565674\n",
      "    - -1067.0333923062776\n",
      "    - -1612.6764077992907\n",
      "    - -1235.7011280348495\n",
      "    - -1497.310233671297\n",
      "    - -1573.0313457931309\n",
      "    - -1030.1981165603927\n",
      "    - -1503.2482678266092\n",
      "    - -889.2287204658791\n",
      "    - -1052.3105068488617\n",
      "    - -890.6588968847192\n",
      "    - -1026.1627444904773\n",
      "    - -874.7506394950451\n",
      "    - -1097.8739383391955\n",
      "    - -901.2091890880025\n",
      "    - -1338.319809570784\n",
      "    - -924.3744054821954\n",
      "    - -1547.126641955675\n",
      "    - -1076.8695307460805\n",
      "    - -965.4945414431063\n",
      "    - -1611.9804443984976\n",
      "    - -1012.9187842652138\n",
      "    - -895.264985831948\n",
      "    - -1567.06631999244\n",
      "    - -891.768923212791\n",
      "    - -892.4800459113643\n",
      "    - -1571.5139749483615\n",
      "    - -994.1251585539895\n",
      "    - -884.727031976493\n",
      "    - -984.5795025040751\n",
      "    - -1304.6563689120585\n",
      "    - -1100.3187711589499\n",
      "    - -1016.2725258419459\n",
      "    - -1556.2058627954543\n",
      "    - -913.8912971947939\n",
      "    - -1325.7721567706576\n",
      "    - -994.1319072096975\n",
      "    - -1069.5498064520139\n",
      "    - -1723.4981832778358\n",
      "    - -1682.4274194856694\n",
      "    - -892.0227514359974\n",
      "    - -989.0944351567309\n",
      "    - -1222.8561566883027\n",
      "    - -1721.454976341572\n",
      "    - -1571.2899640051476\n",
      "    - -890.495175942533\n",
      "    - -1554.0762093511469\n",
      "    - -1522.790971397533\n",
      "    - -1408.00858285526\n",
      "    - -888.4647083066299\n",
      "    - -1065.549002090792\n",
      "    - -985.0800205855986\n",
      "    - -902.9269780532229\n",
      "    - -894.0880366080055\n",
      "    - -1698.2724980310825\n",
      "    - -1299.8667358873192\n",
      "    - -1034.9038854685962\n",
      "    - -1549.1707891155907\n",
      "    - -882.1675938573442\n",
      "    - -888.1080039138672\n",
      "    - -910.1407850284602\n",
      "    - -1675.6515070248581\n",
      "    - -874.9864408683121\n",
      "    - -1180.686797389787\n",
      "    - -894.1842498374639\n",
      "    - -891.8951674996224\n",
      "    - -901.2140602438221\n",
      "    - -1378.8579782178056\n",
      "    - -880.397305519999\n",
      "    - -885.1078441714999\n",
      "    - -900.2653449764849\n",
      "    - -1172.5306813262596\n",
      "    - -1065.685349291034\n",
      "    - -1422.1292112208325\n",
      "    - -1021.5184364262664\n",
      "    - -1090.3452474312203\n",
      "    - -892.2298579872014\n",
      "    - -1624.844048080507\n",
      "    - -894.726807015641\n",
      "    - -908.5811447651637\n",
      "    - -1595.0461446409952\n",
      "    - -1636.2927935335406\n",
      "    - -1606.1237342985596\n",
      "    - -889.2772368253061\n",
      "    - -885.1601688300099\n",
      "    - -1432.8460980436864\n",
      "    - -1176.6271599160116\n",
      "    - -899.9719914220738\n",
      "    - -1268.5752317499625\n",
      "    - -1317.9097413055588\n",
      "    - -950.9952554826866\n",
      "    - -885.5591126081935\n",
      "    - -1087.861604574657\n",
      "    - -895.859710976924\n",
      "    - -890.2862246291633\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.195900572782392\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15125820419407585\n",
      "    mean_inference_ms: 0.9986917275431668\n",
      "    mean_raw_obs_processing_ms: 0.12318823687725913\n",
      "time_since_restore: 1503.1014866828918\n",
      "time_this_iter_s: 9.530515432357788\n",
      "time_total_s: 1503.1014866828918\n",
      "timers:\n",
      "  learn_throughput: 928.911\n",
      "  learn_time_ms: 4306.119\n",
      "  load_throughput: 19134598.54\n",
      "  load_time_ms: 0.209\n",
      "  training_iteration_time_ms: 10029.472\n",
      "  update_time_ms: 2.361\n",
      "timestamp: 1660565426\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 564000\n",
      "training_iteration: 141\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 568000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 568000\n",
      "  num_agent_steps_trained: 568000\n",
      "  num_env_steps_sampled: 568000\n",
      "  num_env_steps_trained: 568000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-10-36\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -874.9864408683121\n",
      "episode_reward_mean: -1162.5462462924056\n",
      "episode_reward_min: -1723.4981832778358\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2840\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7697353363037109\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4076292514801025\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009576096199452877\n",
      "        model: {}\n",
      "        policy_loss: 0.011654101312160492\n",
      "        total_loss: 9.820188522338867\n",
      "        vf_explained_var: -0.032444942742586136\n",
      "        vf_loss: 9.801164627075195\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 568000\n",
      "  num_agent_steps_trained: 568000\n",
      "  num_env_steps_sampled: 568000\n",
      "  num_env_steps_trained: 568000\n",
      "iterations_since_restore: 142\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 568000\n",
      "num_agent_steps_trained: 568000\n",
      "num_env_steps_sampled: 568000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 568000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.37857142857143\n",
      "  ram_util_percent: 60.22142857142859\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19578719628369773\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15117295768539868\n",
      "  mean_inference_ms: 0.9980982291042441\n",
      "  mean_raw_obs_processing_ms: 0.12311624104282874\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -874.9864408683121\n",
      "  episode_reward_mean: -1162.5462462924056\n",
      "  episode_reward_min: -1723.4981832778358\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1338.319809570784\n",
      "    - -924.3744054821954\n",
      "    - -1547.126641955675\n",
      "    - -1076.8695307460805\n",
      "    - -965.4945414431063\n",
      "    - -1611.9804443984976\n",
      "    - -1012.9187842652138\n",
      "    - -895.264985831948\n",
      "    - -1567.06631999244\n",
      "    - -891.768923212791\n",
      "    - -892.4800459113643\n",
      "    - -1571.5139749483615\n",
      "    - -994.1251585539895\n",
      "    - -884.727031976493\n",
      "    - -984.5795025040751\n",
      "    - -1304.6563689120585\n",
      "    - -1100.3187711589499\n",
      "    - -1016.2725258419459\n",
      "    - -1556.2058627954543\n",
      "    - -913.8912971947939\n",
      "    - -1325.7721567706576\n",
      "    - -994.1319072096975\n",
      "    - -1069.5498064520139\n",
      "    - -1723.4981832778358\n",
      "    - -1682.4274194856694\n",
      "    - -892.0227514359974\n",
      "    - -989.0944351567309\n",
      "    - -1222.8561566883027\n",
      "    - -1721.454976341572\n",
      "    - -1571.2899640051476\n",
      "    - -890.495175942533\n",
      "    - -1554.0762093511469\n",
      "    - -1522.790971397533\n",
      "    - -1408.00858285526\n",
      "    - -888.4647083066299\n",
      "    - -1065.549002090792\n",
      "    - -985.0800205855986\n",
      "    - -902.9269780532229\n",
      "    - -894.0880366080055\n",
      "    - -1698.2724980310825\n",
      "    - -1299.8667358873192\n",
      "    - -1034.9038854685962\n",
      "    - -1549.1707891155907\n",
      "    - -882.1675938573442\n",
      "    - -888.1080039138672\n",
      "    - -910.1407850284602\n",
      "    - -1675.6515070248581\n",
      "    - -874.9864408683121\n",
      "    - -1180.686797389787\n",
      "    - -894.1842498374639\n",
      "    - -891.8951674996224\n",
      "    - -901.2140602438221\n",
      "    - -1378.8579782178056\n",
      "    - -880.397305519999\n",
      "    - -885.1078441714999\n",
      "    - -900.2653449764849\n",
      "    - -1172.5306813262596\n",
      "    - -1065.685349291034\n",
      "    - -1422.1292112208325\n",
      "    - -1021.5184364262664\n",
      "    - -1090.3452474312203\n",
      "    - -892.2298579872014\n",
      "    - -1624.844048080507\n",
      "    - -894.726807015641\n",
      "    - -908.5811447651637\n",
      "    - -1595.0461446409952\n",
      "    - -1636.2927935335406\n",
      "    - -1606.1237342985596\n",
      "    - -889.2772368253061\n",
      "    - -885.1601688300099\n",
      "    - -1432.8460980436864\n",
      "    - -1176.6271599160116\n",
      "    - -899.9719914220738\n",
      "    - -1268.5752317499625\n",
      "    - -1317.9097413055588\n",
      "    - -950.9952554826866\n",
      "    - -885.5591126081935\n",
      "    - -1087.861604574657\n",
      "    - -895.859710976924\n",
      "    - -890.2862246291633\n",
      "    - -895.9673102676105\n",
      "    - -889.9743427625682\n",
      "    - -988.2486530890266\n",
      "    - -892.1156347440913\n",
      "    - -1537.89528015057\n",
      "    - -1601.8502508617512\n",
      "    - -883.156922278387\n",
      "    - -1505.6206579068335\n",
      "    - -1703.6021113526726\n",
      "    - -978.7676307031868\n",
      "    - -878.7478158164038\n",
      "    - -1357.5962844901976\n",
      "    - -1174.143269151162\n",
      "    - -1673.6654613054222\n",
      "    - -984.0996943181457\n",
      "    - -1613.9687593256187\n",
      "    - -898.8209361439267\n",
      "    - -879.7438940792277\n",
      "    - -1432.6678037202307\n",
      "    - -893.5795686296027\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19578719628369773\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15117295768539868\n",
      "    mean_inference_ms: 0.9980982291042441\n",
      "    mean_raw_obs_processing_ms: 0.12311624104282874\n",
      "time_since_restore: 1512.6292133331299\n",
      "time_this_iter_s: 9.527726650238037\n",
      "time_total_s: 1512.6292133331299\n",
      "timers:\n",
      "  learn_throughput: 930.084\n",
      "  learn_time_ms: 4300.688\n",
      "  load_throughput: 19246548.124\n",
      "  load_time_ms: 0.208\n",
      "  training_iteration_time_ms: 10004.786\n",
      "  update_time_ms: 2.334\n",
      "timestamp: 1660565436\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 568000\n",
      "training_iteration: 142\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 572000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 572000\n",
      "  num_agent_steps_trained: 572000\n",
      "  num_env_steps_sampled: 572000\n",
      "  num_env_steps_trained: 572000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-10-45\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -854.4800306519518\n",
      "episode_reward_mean: -1162.2131455098872\n",
      "episode_reward_min: -1723.4981832778358\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2860\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7697353363037109\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9296271800994873\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009751110337674618\n",
      "        model: {}\n",
      "        policy_loss: 0.010138479061424732\n",
      "        total_loss: 9.84009075164795\n",
      "        vf_explained_var: -0.025157108902931213\n",
      "        vf_loss: 9.8224458694458\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 572000\n",
      "  num_agent_steps_trained: 572000\n",
      "  num_env_steps_sampled: 572000\n",
      "  num_env_steps_trained: 572000\n",
      "iterations_since_restore: 143\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 572000\n",
      "num_agent_steps_trained: 572000\n",
      "num_env_steps_sampled: 572000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 572000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 41.16153846153846\n",
      "  ram_util_percent: 60.28461538461537\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1956718070440801\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1510908162094667\n",
      "  mean_inference_ms: 0.9975147818497834\n",
      "  mean_raw_obs_processing_ms: 0.12304643631868327\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -854.4800306519518\n",
      "  episode_reward_mean: -1162.2131455098872\n",
      "  episode_reward_min: -1723.4981832778358\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1325.7721567706576\n",
      "    - -994.1319072096975\n",
      "    - -1069.5498064520139\n",
      "    - -1723.4981832778358\n",
      "    - -1682.4274194856694\n",
      "    - -892.0227514359974\n",
      "    - -989.0944351567309\n",
      "    - -1222.8561566883027\n",
      "    - -1721.454976341572\n",
      "    - -1571.2899640051476\n",
      "    - -890.495175942533\n",
      "    - -1554.0762093511469\n",
      "    - -1522.790971397533\n",
      "    - -1408.00858285526\n",
      "    - -888.4647083066299\n",
      "    - -1065.549002090792\n",
      "    - -985.0800205855986\n",
      "    - -902.9269780532229\n",
      "    - -894.0880366080055\n",
      "    - -1698.2724980310825\n",
      "    - -1299.8667358873192\n",
      "    - -1034.9038854685962\n",
      "    - -1549.1707891155907\n",
      "    - -882.1675938573442\n",
      "    - -888.1080039138672\n",
      "    - -910.1407850284602\n",
      "    - -1675.6515070248581\n",
      "    - -874.9864408683121\n",
      "    - -1180.686797389787\n",
      "    - -894.1842498374639\n",
      "    - -891.8951674996224\n",
      "    - -901.2140602438221\n",
      "    - -1378.8579782178056\n",
      "    - -880.397305519999\n",
      "    - -885.1078441714999\n",
      "    - -900.2653449764849\n",
      "    - -1172.5306813262596\n",
      "    - -1065.685349291034\n",
      "    - -1422.1292112208325\n",
      "    - -1021.5184364262664\n",
      "    - -1090.3452474312203\n",
      "    - -892.2298579872014\n",
      "    - -1624.844048080507\n",
      "    - -894.726807015641\n",
      "    - -908.5811447651637\n",
      "    - -1595.0461446409952\n",
      "    - -1636.2927935335406\n",
      "    - -1606.1237342985596\n",
      "    - -889.2772368253061\n",
      "    - -885.1601688300099\n",
      "    - -1432.8460980436864\n",
      "    - -1176.6271599160116\n",
      "    - -899.9719914220738\n",
      "    - -1268.5752317499625\n",
      "    - -1317.9097413055588\n",
      "    - -950.9952554826866\n",
      "    - -885.5591126081935\n",
      "    - -1087.861604574657\n",
      "    - -895.859710976924\n",
      "    - -890.2862246291633\n",
      "    - -895.9673102676105\n",
      "    - -889.9743427625682\n",
      "    - -988.2486530890266\n",
      "    - -892.1156347440913\n",
      "    - -1537.89528015057\n",
      "    - -1601.8502508617512\n",
      "    - -883.156922278387\n",
      "    - -1505.6206579068335\n",
      "    - -1703.6021113526726\n",
      "    - -978.7676307031868\n",
      "    - -878.7478158164038\n",
      "    - -1357.5962844901976\n",
      "    - -1174.143269151162\n",
      "    - -1673.6654613054222\n",
      "    - -984.0996943181457\n",
      "    - -1613.9687593256187\n",
      "    - -898.8209361439267\n",
      "    - -879.7438940792277\n",
      "    - -1432.6678037202307\n",
      "    - -893.5795686296027\n",
      "    - -1660.1054402722225\n",
      "    - -886.2044642696043\n",
      "    - -895.4063172286593\n",
      "    - -1479.463625552622\n",
      "    - -1074.3115123840632\n",
      "    - -884.0101615697178\n",
      "    - -1650.2781590162404\n",
      "    - -888.2318931964767\n",
      "    - -1629.8258164435738\n",
      "    - -889.8212507206317\n",
      "    - -1171.8224213362928\n",
      "    - -1455.440027809881\n",
      "    - -1064.1277428758724\n",
      "    - -1463.4599491886713\n",
      "    - -854.4800306519518\n",
      "    - -992.8849430881131\n",
      "    - -969.7746660153074\n",
      "    - -908.7476719237078\n",
      "    - -1261.7990320610638\n",
      "    - -936.4497228396903\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1956718070440801\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1510908162094667\n",
      "    mean_inference_ms: 0.9975147818497834\n",
      "    mean_raw_obs_processing_ms: 0.12304643631868327\n",
      "time_since_restore: 1522.088737487793\n",
      "time_this_iter_s: 9.459524154663086\n",
      "time_total_s: 1522.088737487793\n",
      "timers:\n",
      "  learn_throughput: 951.755\n",
      "  learn_time_ms: 4202.763\n",
      "  load_throughput: 18377934.056\n",
      "  load_time_ms: 0.218\n",
      "  training_iteration_time_ms: 9862.238\n",
      "  update_time_ms: 2.329\n",
      "timestamp: 1660565445\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 572000\n",
      "training_iteration: 143\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 576000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 576000\n",
      "  num_agent_steps_trained: 576000\n",
      "  num_env_steps_sampled: 576000\n",
      "  num_env_steps_trained: 576000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-10-55\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -854.4800306519518\n",
      "episode_reward_mean: -1145.3602067697882\n",
      "episode_reward_min: -1737.441833970341\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2880\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7697353363037109\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9188510179519653\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011333554051816463\n",
      "        model: {}\n",
      "        policy_loss: 0.012440004386007786\n",
      "        total_loss: 9.80447006225586\n",
      "        vf_explained_var: -0.017670199275016785\n",
      "        vf_loss: 9.783306121826172\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 576000\n",
      "  num_agent_steps_trained: 576000\n",
      "  num_env_steps_sampled: 576000\n",
      "  num_env_steps_trained: 576000\n",
      "iterations_since_restore: 144\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 576000\n",
      "num_agent_steps_trained: 576000\n",
      "num_env_steps_sampled: 576000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 576000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 41.82857142857143\n",
      "  ram_util_percent: 60.29999999999999\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19556192375075102\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1510064363646984\n",
      "  mean_inference_ms: 0.9969342575815108\n",
      "  mean_raw_obs_processing_ms: 0.12297603022519851\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -854.4800306519518\n",
      "  episode_reward_mean: -1145.3602067697882\n",
      "  episode_reward_min: -1737.441833970341\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1299.8667358873192\n",
      "    - -1034.9038854685962\n",
      "    - -1549.1707891155907\n",
      "    - -882.1675938573442\n",
      "    - -888.1080039138672\n",
      "    - -910.1407850284602\n",
      "    - -1675.6515070248581\n",
      "    - -874.9864408683121\n",
      "    - -1180.686797389787\n",
      "    - -894.1842498374639\n",
      "    - -891.8951674996224\n",
      "    - -901.2140602438221\n",
      "    - -1378.8579782178056\n",
      "    - -880.397305519999\n",
      "    - -885.1078441714999\n",
      "    - -900.2653449764849\n",
      "    - -1172.5306813262596\n",
      "    - -1065.685349291034\n",
      "    - -1422.1292112208325\n",
      "    - -1021.5184364262664\n",
      "    - -1090.3452474312203\n",
      "    - -892.2298579872014\n",
      "    - -1624.844048080507\n",
      "    - -894.726807015641\n",
      "    - -908.5811447651637\n",
      "    - -1595.0461446409952\n",
      "    - -1636.2927935335406\n",
      "    - -1606.1237342985596\n",
      "    - -889.2772368253061\n",
      "    - -885.1601688300099\n",
      "    - -1432.8460980436864\n",
      "    - -1176.6271599160116\n",
      "    - -899.9719914220738\n",
      "    - -1268.5752317499625\n",
      "    - -1317.9097413055588\n",
      "    - -950.9952554826866\n",
      "    - -885.5591126081935\n",
      "    - -1087.861604574657\n",
      "    - -895.859710976924\n",
      "    - -890.2862246291633\n",
      "    - -895.9673102676105\n",
      "    - -889.9743427625682\n",
      "    - -988.2486530890266\n",
      "    - -892.1156347440913\n",
      "    - -1537.89528015057\n",
      "    - -1601.8502508617512\n",
      "    - -883.156922278387\n",
      "    - -1505.6206579068335\n",
      "    - -1703.6021113526726\n",
      "    - -978.7676307031868\n",
      "    - -878.7478158164038\n",
      "    - -1357.5962844901976\n",
      "    - -1174.143269151162\n",
      "    - -1673.6654613054222\n",
      "    - -984.0996943181457\n",
      "    - -1613.9687593256187\n",
      "    - -898.8209361439267\n",
      "    - -879.7438940792277\n",
      "    - -1432.6678037202307\n",
      "    - -893.5795686296027\n",
      "    - -1660.1054402722225\n",
      "    - -886.2044642696043\n",
      "    - -895.4063172286593\n",
      "    - -1479.463625552622\n",
      "    - -1074.3115123840632\n",
      "    - -884.0101615697178\n",
      "    - -1650.2781590162404\n",
      "    - -888.2318931964767\n",
      "    - -1629.8258164435738\n",
      "    - -889.8212507206317\n",
      "    - -1171.8224213362928\n",
      "    - -1455.440027809881\n",
      "    - -1064.1277428758724\n",
      "    - -1463.4599491886713\n",
      "    - -854.4800306519518\n",
      "    - -992.8849430881131\n",
      "    - -969.7746660153074\n",
      "    - -908.7476719237078\n",
      "    - -1261.7990320610638\n",
      "    - -936.4497228396903\n",
      "    - -896.9983392612052\n",
      "    - -1687.258533951384\n",
      "    - -990.1453501472228\n",
      "    - -958.1507508653264\n",
      "    - -1446.021005157159\n",
      "    - -1137.7644288571066\n",
      "    - -1417.2904474653778\n",
      "    - -889.6300561075745\n",
      "    - -1024.7336094389607\n",
      "    - -977.0012547745882\n",
      "    - -1490.6859742329589\n",
      "    - -895.2960127907014\n",
      "    - -1167.8359544660987\n",
      "    - -991.3479168819795\n",
      "    - -979.3497355703981\n",
      "    - -1096.1116151355438\n",
      "    - -1072.7393568458465\n",
      "    - -1737.441833970341\n",
      "    - -967.19415167813\n",
      "    - -1493.559738437616\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19556192375075102\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1510064363646984\n",
      "    mean_inference_ms: 0.9969342575815108\n",
      "    mean_raw_obs_processing_ms: 0.12297603022519851\n",
      "time_since_restore: 1531.592918395996\n",
      "time_this_iter_s: 9.504180908203125\n",
      "time_total_s: 1531.592918395996\n",
      "timers:\n",
      "  learn_throughput: 960.729\n",
      "  learn_time_ms: 4163.506\n",
      "  load_throughput: 18339763.883\n",
      "  load_time_ms: 0.218\n",
      "  training_iteration_time_ms: 9704.612\n",
      "  update_time_ms: 2.334\n",
      "timestamp: 1660565455\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 576000\n",
      "training_iteration: 144\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 580000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 580000\n",
      "  num_agent_steps_trained: 580000\n",
      "  num_env_steps_sampled: 580000\n",
      "  num_env_steps_trained: 580000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-11-04\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -854.4800306519518\n",
      "episode_reward_mean: -1134.4391342848774\n",
      "episode_reward_min: -1737.441833970341\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2900\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7697353363037109\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.03944439813494682\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010375824756920338\n",
      "        model: {}\n",
      "        policy_loss: 0.011190204881131649\n",
      "        total_loss: 9.71434211730957\n",
      "        vf_explained_var: -0.026799220591783524\n",
      "        vf_loss: 9.695165634155273\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 580000\n",
      "  num_agent_steps_trained: 580000\n",
      "  num_env_steps_sampled: 580000\n",
      "  num_env_steps_trained: 580000\n",
      "iterations_since_restore: 145\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 580000\n",
      "num_agent_steps_trained: 580000\n",
      "num_env_steps_sampled: 580000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 580000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.84999999999999\n",
      "  ram_util_percent: 60.29285714285712\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19544853469553247\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15092428863339713\n",
      "  mean_inference_ms: 0.9963541100030925\n",
      "  mean_raw_obs_processing_ms: 0.12290927433426338\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -854.4800306519518\n",
      "  episode_reward_mean: -1134.4391342848774\n",
      "  episode_reward_min: -1737.441833970341\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1090.3452474312203\n",
      "    - -892.2298579872014\n",
      "    - -1624.844048080507\n",
      "    - -894.726807015641\n",
      "    - -908.5811447651637\n",
      "    - -1595.0461446409952\n",
      "    - -1636.2927935335406\n",
      "    - -1606.1237342985596\n",
      "    - -889.2772368253061\n",
      "    - -885.1601688300099\n",
      "    - -1432.8460980436864\n",
      "    - -1176.6271599160116\n",
      "    - -899.9719914220738\n",
      "    - -1268.5752317499625\n",
      "    - -1317.9097413055588\n",
      "    - -950.9952554826866\n",
      "    - -885.5591126081935\n",
      "    - -1087.861604574657\n",
      "    - -895.859710976924\n",
      "    - -890.2862246291633\n",
      "    - -895.9673102676105\n",
      "    - -889.9743427625682\n",
      "    - -988.2486530890266\n",
      "    - -892.1156347440913\n",
      "    - -1537.89528015057\n",
      "    - -1601.8502508617512\n",
      "    - -883.156922278387\n",
      "    - -1505.6206579068335\n",
      "    - -1703.6021113526726\n",
      "    - -978.7676307031868\n",
      "    - -878.7478158164038\n",
      "    - -1357.5962844901976\n",
      "    - -1174.143269151162\n",
      "    - -1673.6654613054222\n",
      "    - -984.0996943181457\n",
      "    - -1613.9687593256187\n",
      "    - -898.8209361439267\n",
      "    - -879.7438940792277\n",
      "    - -1432.6678037202307\n",
      "    - -893.5795686296027\n",
      "    - -1660.1054402722225\n",
      "    - -886.2044642696043\n",
      "    - -895.4063172286593\n",
      "    - -1479.463625552622\n",
      "    - -1074.3115123840632\n",
      "    - -884.0101615697178\n",
      "    - -1650.2781590162404\n",
      "    - -888.2318931964767\n",
      "    - -1629.8258164435738\n",
      "    - -889.8212507206317\n",
      "    - -1171.8224213362928\n",
      "    - -1455.440027809881\n",
      "    - -1064.1277428758724\n",
      "    - -1463.4599491886713\n",
      "    - -854.4800306519518\n",
      "    - -992.8849430881131\n",
      "    - -969.7746660153074\n",
      "    - -908.7476719237078\n",
      "    - -1261.7990320610638\n",
      "    - -936.4497228396903\n",
      "    - -896.9983392612052\n",
      "    - -1687.258533951384\n",
      "    - -990.1453501472228\n",
      "    - -958.1507508653264\n",
      "    - -1446.021005157159\n",
      "    - -1137.7644288571066\n",
      "    - -1417.2904474653778\n",
      "    - -889.6300561075745\n",
      "    - -1024.7336094389607\n",
      "    - -977.0012547745882\n",
      "    - -1490.6859742329589\n",
      "    - -895.2960127907014\n",
      "    - -1167.8359544660987\n",
      "    - -991.3479168819795\n",
      "    - -979.3497355703981\n",
      "    - -1096.1116151355438\n",
      "    - -1072.7393568458465\n",
      "    - -1737.441833970341\n",
      "    - -967.19415167813\n",
      "    - -1493.559738437616\n",
      "    - -875.7753349070881\n",
      "    - -1388.6901767063432\n",
      "    - -912.9721937046883\n",
      "    - -892.4014995035656\n",
      "    - -1481.5242510329465\n",
      "    - -890.797759185585\n",
      "    - -981.7935780331239\n",
      "    - -892.7628125613498\n",
      "    - -869.783026937473\n",
      "    - -1165.8574661717607\n",
      "    - -1089.7636121275962\n",
      "    - -891.9997983164245\n",
      "    - -1187.0470818862646\n",
      "    - -900.0040290882952\n",
      "    - -1094.334778994522\n",
      "    - -985.7779594974711\n",
      "    - -956.3408023903338\n",
      "    - -892.822718478695\n",
      "    - -1311.510658190914\n",
      "    - -955.401381079712\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19544853469553247\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15092428863339713\n",
      "    mean_inference_ms: 0.9963541100030925\n",
      "    mean_raw_obs_processing_ms: 0.12290927433426338\n",
      "time_since_restore: 1541.1078658103943\n",
      "time_this_iter_s: 9.514947414398193\n",
      "time_total_s: 1541.1078658103943\n",
      "timers:\n",
      "  learn_throughput: 979.946\n",
      "  learn_time_ms: 4081.857\n",
      "  load_throughput: 18456783.278\n",
      "  load_time_ms: 0.217\n",
      "  training_iteration_time_ms: 9590.833\n",
      "  update_time_ms: 2.235\n",
      "timestamp: 1660565464\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 580000\n",
      "training_iteration: 145\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 584000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 584000\n",
      "  num_agent_steps_trained: 584000\n",
      "  num_env_steps_sampled: 584000\n",
      "  num_env_steps_trained: 584000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-11-14\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -854.4800306519518\n",
      "episode_reward_mean: -1141.966756592002\n",
      "episode_reward_min: -1737.441833970341\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2920\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7697353363037109\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.2509589195251465\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00970765482634306\n",
      "        model: {}\n",
      "        policy_loss: 0.013367709703743458\n",
      "        total_loss: 9.830586433410645\n",
      "        vf_explained_var: -0.01785055361688137\n",
      "        vf_loss: 9.809744834899902\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 584000\n",
      "  num_agent_steps_trained: 584000\n",
      "  num_env_steps_sampled: 584000\n",
      "  num_env_steps_trained: 584000\n",
      "iterations_since_restore: 146\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 584000\n",
      "num_agent_steps_trained: 584000\n",
      "num_env_steps_sampled: 584000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 584000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.099999999999994\n",
      "  ram_util_percent: 60.28461538461537\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1953382706123343\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15084200499974412\n",
      "  mean_inference_ms: 0.9957850052018616\n",
      "  mean_raw_obs_processing_ms: 0.12284105231494731\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -854.4800306519518\n",
      "  episode_reward_mean: -1141.966756592002\n",
      "  episode_reward_min: -1737.441833970341\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -895.9673102676105\n",
      "    - -889.9743427625682\n",
      "    - -988.2486530890266\n",
      "    - -892.1156347440913\n",
      "    - -1537.89528015057\n",
      "    - -1601.8502508617512\n",
      "    - -883.156922278387\n",
      "    - -1505.6206579068335\n",
      "    - -1703.6021113526726\n",
      "    - -978.7676307031868\n",
      "    - -878.7478158164038\n",
      "    - -1357.5962844901976\n",
      "    - -1174.143269151162\n",
      "    - -1673.6654613054222\n",
      "    - -984.0996943181457\n",
      "    - -1613.9687593256187\n",
      "    - -898.8209361439267\n",
      "    - -879.7438940792277\n",
      "    - -1432.6678037202307\n",
      "    - -893.5795686296027\n",
      "    - -1660.1054402722225\n",
      "    - -886.2044642696043\n",
      "    - -895.4063172286593\n",
      "    - -1479.463625552622\n",
      "    - -1074.3115123840632\n",
      "    - -884.0101615697178\n",
      "    - -1650.2781590162404\n",
      "    - -888.2318931964767\n",
      "    - -1629.8258164435738\n",
      "    - -889.8212507206317\n",
      "    - -1171.8224213362928\n",
      "    - -1455.440027809881\n",
      "    - -1064.1277428758724\n",
      "    - -1463.4599491886713\n",
      "    - -854.4800306519518\n",
      "    - -992.8849430881131\n",
      "    - -969.7746660153074\n",
      "    - -908.7476719237078\n",
      "    - -1261.7990320610638\n",
      "    - -936.4497228396903\n",
      "    - -896.9983392612052\n",
      "    - -1687.258533951384\n",
      "    - -990.1453501472228\n",
      "    - -958.1507508653264\n",
      "    - -1446.021005157159\n",
      "    - -1137.7644288571066\n",
      "    - -1417.2904474653778\n",
      "    - -889.6300561075745\n",
      "    - -1024.7336094389607\n",
      "    - -977.0012547745882\n",
      "    - -1490.6859742329589\n",
      "    - -895.2960127907014\n",
      "    - -1167.8359544660987\n",
      "    - -991.3479168819795\n",
      "    - -979.3497355703981\n",
      "    - -1096.1116151355438\n",
      "    - -1072.7393568458465\n",
      "    - -1737.441833970341\n",
      "    - -967.19415167813\n",
      "    - -1493.559738437616\n",
      "    - -875.7753349070881\n",
      "    - -1388.6901767063432\n",
      "    - -912.9721937046883\n",
      "    - -892.4014995035656\n",
      "    - -1481.5242510329465\n",
      "    - -890.797759185585\n",
      "    - -981.7935780331239\n",
      "    - -892.7628125613498\n",
      "    - -869.783026937473\n",
      "    - -1165.8574661717607\n",
      "    - -1089.7636121275962\n",
      "    - -891.9997983164245\n",
      "    - -1187.0470818862646\n",
      "    - -900.0040290882952\n",
      "    - -1094.334778994522\n",
      "    - -985.7779594974711\n",
      "    - -956.3408023903338\n",
      "    - -892.822718478695\n",
      "    - -1311.510658190914\n",
      "    - -955.401381079712\n",
      "    - -1336.9585484780464\n",
      "    - -882.7024292550866\n",
      "    - -925.0476316853435\n",
      "    - -889.951625990872\n",
      "    - -888.8469715841813\n",
      "    - -1075.5155817425577\n",
      "    - -872.375758502517\n",
      "    - -1435.7447740591772\n",
      "    - -1079.3901728894414\n",
      "    - -886.2055417058722\n",
      "    - -1668.5204760461902\n",
      "    - -992.3474920261582\n",
      "    - -1708.3555250995707\n",
      "    - -1420.9314198586353\n",
      "    - -1577.487861716656\n",
      "    - -1587.9189574929333\n",
      "    - -1083.7784292639465\n",
      "    - -1205.0068928979367\n",
      "    - -1172.3356077977455\n",
      "    - -892.4598467366662\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1953382706123343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15084200499974412\n",
      "    mean_inference_ms: 0.9957850052018616\n",
      "    mean_raw_obs_processing_ms: 0.12284105231494731\n",
      "time_since_restore: 1550.6519484519958\n",
      "time_this_iter_s: 9.544082641601562\n",
      "time_total_s: 1550.6519484519958\n",
      "timers:\n",
      "  learn_throughput: 982.336\n",
      "  learn_time_ms: 4071.925\n",
      "  load_throughput: 18210372.3\n",
      "  load_time_ms: 0.22\n",
      "  training_iteration_time_ms: 9530.888\n",
      "  update_time_ms: 2.219\n",
      "timestamp: 1660565474\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 584000\n",
      "training_iteration: 146\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 588000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 588000\n",
      "  num_agent_steps_trained: 588000\n",
      "  num_env_steps_sampled: 588000\n",
      "  num_env_steps_trained: 588000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-11-24\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -854.4800306519518\n",
      "episode_reward_mean: -1149.8296377114284\n",
      "episode_reward_min: -1743.8171049985622\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2940\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7697353363037109\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.869413137435913\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013897384516894817\n",
      "        model: {}\n",
      "        policy_loss: 0.010525492019951344\n",
      "        total_loss: 9.864801406860352\n",
      "        vf_explained_var: -0.016200508922338486\n",
      "        vf_loss: 9.843579292297363\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 588000\n",
      "  num_agent_steps_trained: 588000\n",
      "  num_env_steps_sampled: 588000\n",
      "  num_env_steps_trained: 588000\n",
      "iterations_since_restore: 147\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 588000\n",
      "num_agent_steps_trained: 588000\n",
      "num_env_steps_sampled: 588000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 588000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.24999999999999\n",
      "  ram_util_percent: 60.32142857142855\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1952256860107154\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15076254418281576\n",
      "  mean_inference_ms: 0.9952190447574014\n",
      "  mean_raw_obs_processing_ms: 0.12277661007779654\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -854.4800306519518\n",
      "  episode_reward_mean: -1149.8296377114284\n",
      "  episode_reward_min: -1743.8171049985622\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1660.1054402722225\n",
      "    - -886.2044642696043\n",
      "    - -895.4063172286593\n",
      "    - -1479.463625552622\n",
      "    - -1074.3115123840632\n",
      "    - -884.0101615697178\n",
      "    - -1650.2781590162404\n",
      "    - -888.2318931964767\n",
      "    - -1629.8258164435738\n",
      "    - -889.8212507206317\n",
      "    - -1171.8224213362928\n",
      "    - -1455.440027809881\n",
      "    - -1064.1277428758724\n",
      "    - -1463.4599491886713\n",
      "    - -854.4800306519518\n",
      "    - -992.8849430881131\n",
      "    - -969.7746660153074\n",
      "    - -908.7476719237078\n",
      "    - -1261.7990320610638\n",
      "    - -936.4497228396903\n",
      "    - -896.9983392612052\n",
      "    - -1687.258533951384\n",
      "    - -990.1453501472228\n",
      "    - -958.1507508653264\n",
      "    - -1446.021005157159\n",
      "    - -1137.7644288571066\n",
      "    - -1417.2904474653778\n",
      "    - -889.6300561075745\n",
      "    - -1024.7336094389607\n",
      "    - -977.0012547745882\n",
      "    - -1490.6859742329589\n",
      "    - -895.2960127907014\n",
      "    - -1167.8359544660987\n",
      "    - -991.3479168819795\n",
      "    - -979.3497355703981\n",
      "    - -1096.1116151355438\n",
      "    - -1072.7393568458465\n",
      "    - -1737.441833970341\n",
      "    - -967.19415167813\n",
      "    - -1493.559738437616\n",
      "    - -875.7753349070881\n",
      "    - -1388.6901767063432\n",
      "    - -912.9721937046883\n",
      "    - -892.4014995035656\n",
      "    - -1481.5242510329465\n",
      "    - -890.797759185585\n",
      "    - -981.7935780331239\n",
      "    - -892.7628125613498\n",
      "    - -869.783026937473\n",
      "    - -1165.8574661717607\n",
      "    - -1089.7636121275962\n",
      "    - -891.9997983164245\n",
      "    - -1187.0470818862646\n",
      "    - -900.0040290882952\n",
      "    - -1094.334778994522\n",
      "    - -985.7779594974711\n",
      "    - -956.3408023903338\n",
      "    - -892.822718478695\n",
      "    - -1311.510658190914\n",
      "    - -955.401381079712\n",
      "    - -1336.9585484780464\n",
      "    - -882.7024292550866\n",
      "    - -925.0476316853435\n",
      "    - -889.951625990872\n",
      "    - -888.8469715841813\n",
      "    - -1075.5155817425577\n",
      "    - -872.375758502517\n",
      "    - -1435.7447740591772\n",
      "    - -1079.3901728894414\n",
      "    - -886.2055417058722\n",
      "    - -1668.5204760461902\n",
      "    - -992.3474920261582\n",
      "    - -1708.3555250995707\n",
      "    - -1420.9314198586353\n",
      "    - -1577.487861716656\n",
      "    - -1587.9189574929333\n",
      "    - -1083.7784292639465\n",
      "    - -1205.0068928979367\n",
      "    - -1172.3356077977455\n",
      "    - -892.4598467366662\n",
      "    - -1083.5820226132557\n",
      "    - -882.9803017575294\n",
      "    - -1323.453777086682\n",
      "    - -1249.3256807429905\n",
      "    - -1212.2066294924307\n",
      "    - -1467.3810756931694\n",
      "    - -1430.4474686863337\n",
      "    - -882.7829468940323\n",
      "    - -1529.3966646064284\n",
      "    - -1743.8171049985622\n",
      "    - -1313.0368107812524\n",
      "    - -1136.8223516270436\n",
      "    - -940.0173505287696\n",
      "    - -1163.223917029895\n",
      "    - -1373.3455968986245\n",
      "    - -1178.547785105757\n",
      "    - -881.4181406725165\n",
      "    - -1438.8851761674994\n",
      "    - -894.703486123693\n",
      "    - -1325.1461055328211\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1952256860107154\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15076254418281576\n",
      "    mean_inference_ms: 0.9952190447574014\n",
      "    mean_raw_obs_processing_ms: 0.12277661007779654\n",
      "time_since_restore: 1560.1563730239868\n",
      "time_this_iter_s: 9.504424571990967\n",
      "time_total_s: 1560.1563730239868\n",
      "timers:\n",
      "  learn_throughput: 982.523\n",
      "  learn_time_ms: 4071.152\n",
      "  load_throughput: 18212349.11\n",
      "  load_time_ms: 0.22\n",
      "  training_iteration_time_ms: 9521.962\n",
      "  update_time_ms: 2.218\n",
      "timestamp: 1660565484\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 588000\n",
      "training_iteration: 147\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 592000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 592000\n",
      "  num_agent_steps_trained: 592000\n",
      "  num_env_steps_sampled: 592000\n",
      "  num_env_steps_trained: 592000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-11-33\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -869.783026937473\n",
      "episode_reward_mean: -1131.0266527352935\n",
      "episode_reward_min: -1743.8171049985622\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2960\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7697353363037109\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4572838246822357\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009765473194420338\n",
      "        model: {}\n",
      "        policy_loss: 0.010429532267153263\n",
      "        total_loss: 9.802515983581543\n",
      "        vf_explained_var: -0.029067102819681168\n",
      "        vf_loss: 9.784570693969727\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 592000\n",
      "  num_agent_steps_trained: 592000\n",
      "  num_env_steps_sampled: 592000\n",
      "  num_env_steps_trained: 592000\n",
      "iterations_since_restore: 148\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 592000\n",
      "num_agent_steps_trained: 592000\n",
      "num_env_steps_sampled: 592000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 592000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.63846153846154\n",
      "  ram_util_percent: 60.29999999999998\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19511497480231627\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15067927668556205\n",
      "  mean_inference_ms: 0.9946419369319527\n",
      "  mean_raw_obs_processing_ms: 0.12270885175861722\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -869.783026937473\n",
      "  episode_reward_mean: -1131.0266527352935\n",
      "  episode_reward_min: -1743.8171049985622\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -896.9983392612052\n",
      "    - -1687.258533951384\n",
      "    - -990.1453501472228\n",
      "    - -958.1507508653264\n",
      "    - -1446.021005157159\n",
      "    - -1137.7644288571066\n",
      "    - -1417.2904474653778\n",
      "    - -889.6300561075745\n",
      "    - -1024.7336094389607\n",
      "    - -977.0012547745882\n",
      "    - -1490.6859742329589\n",
      "    - -895.2960127907014\n",
      "    - -1167.8359544660987\n",
      "    - -991.3479168819795\n",
      "    - -979.3497355703981\n",
      "    - -1096.1116151355438\n",
      "    - -1072.7393568458465\n",
      "    - -1737.441833970341\n",
      "    - -967.19415167813\n",
      "    - -1493.559738437616\n",
      "    - -875.7753349070881\n",
      "    - -1388.6901767063432\n",
      "    - -912.9721937046883\n",
      "    - -892.4014995035656\n",
      "    - -1481.5242510329465\n",
      "    - -890.797759185585\n",
      "    - -981.7935780331239\n",
      "    - -892.7628125613498\n",
      "    - -869.783026937473\n",
      "    - -1165.8574661717607\n",
      "    - -1089.7636121275962\n",
      "    - -891.9997983164245\n",
      "    - -1187.0470818862646\n",
      "    - -900.0040290882952\n",
      "    - -1094.334778994522\n",
      "    - -985.7779594974711\n",
      "    - -956.3408023903338\n",
      "    - -892.822718478695\n",
      "    - -1311.510658190914\n",
      "    - -955.401381079712\n",
      "    - -1336.9585484780464\n",
      "    - -882.7024292550866\n",
      "    - -925.0476316853435\n",
      "    - -889.951625990872\n",
      "    - -888.8469715841813\n",
      "    - -1075.5155817425577\n",
      "    - -872.375758502517\n",
      "    - -1435.7447740591772\n",
      "    - -1079.3901728894414\n",
      "    - -886.2055417058722\n",
      "    - -1668.5204760461902\n",
      "    - -992.3474920261582\n",
      "    - -1708.3555250995707\n",
      "    - -1420.9314198586353\n",
      "    - -1577.487861716656\n",
      "    - -1587.9189574929333\n",
      "    - -1083.7784292639465\n",
      "    - -1205.0068928979367\n",
      "    - -1172.3356077977455\n",
      "    - -892.4598467366662\n",
      "    - -1083.5820226132557\n",
      "    - -882.9803017575294\n",
      "    - -1323.453777086682\n",
      "    - -1249.3256807429905\n",
      "    - -1212.2066294924307\n",
      "    - -1467.3810756931694\n",
      "    - -1430.4474686863337\n",
      "    - -882.7829468940323\n",
      "    - -1529.3966646064284\n",
      "    - -1743.8171049985622\n",
      "    - -1313.0368107812524\n",
      "    - -1136.8223516270436\n",
      "    - -940.0173505287696\n",
      "    - -1163.223917029895\n",
      "    - -1373.3455968986245\n",
      "    - -1178.547785105757\n",
      "    - -881.4181406725165\n",
      "    - -1438.8851761674994\n",
      "    - -894.703486123693\n",
      "    - -1325.1461055328211\n",
      "    - -910.5053531721694\n",
      "    - -1532.2185717606621\n",
      "    - -988.4354599254037\n",
      "    - -979.1010012476248\n",
      "    - -1506.358410768735\n",
      "    - -1153.0669473890941\n",
      "    - -877.4146546423009\n",
      "    - -894.5803587055217\n",
      "    - -1092.5342771775781\n",
      "    - -895.1120878849082\n",
      "    - -1165.1543705574763\n",
      "    - -981.7068142491336\n",
      "    - -872.376302358168\n",
      "    - -1430.408243129801\n",
      "    - -879.0386667193285\n",
      "    - -889.9913424332275\n",
      "    - -1185.8047479845227\n",
      "    - -1006.6560836288053\n",
      "    - -896.1983761802643\n",
      "    - -999.6842809161229\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19511497480231627\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15067927668556205\n",
      "    mean_inference_ms: 0.9946419369319527\n",
      "    mean_raw_obs_processing_ms: 0.12270885175861722\n",
      "time_since_restore: 1569.5934982299805\n",
      "time_this_iter_s: 9.437125205993652\n",
      "time_total_s: 1569.5934982299805\n",
      "timers:\n",
      "  learn_throughput: 986.655\n",
      "  learn_time_ms: 4054.103\n",
      "  load_throughput: 17483551.48\n",
      "  load_time_ms: 0.229\n",
      "  training_iteration_time_ms: 9496.835\n",
      "  update_time_ms: 2.13\n",
      "timestamp: 1660565493\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 592000\n",
      "training_iteration: 148\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 596000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 596000\n",
      "  num_agent_steps_trained: 596000\n",
      "  num_env_steps_sampled: 596000\n",
      "  num_env_steps_trained: 596000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-11-43\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -869.783026937473\n",
      "episode_reward_mean: -1104.3215881837662\n",
      "episode_reward_min: -1743.8171049985622\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 2980\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7697353363037109\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.06271228194236755\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007998798973858356\n",
      "        model: {}\n",
      "        policy_loss: 0.011325467377901077\n",
      "        total_loss: 9.787989616394043\n",
      "        vf_explained_var: -0.03173988685011864\n",
      "        vf_loss: 9.770506858825684\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 596000\n",
      "  num_agent_steps_trained: 596000\n",
      "  num_env_steps_sampled: 596000\n",
      "  num_env_steps_trained: 596000\n",
      "iterations_since_restore: 149\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 596000\n",
      "num_agent_steps_trained: 596000\n",
      "num_env_steps_sampled: 596000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 596000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.135714285714286\n",
      "  ram_util_percent: 60.20714285714288\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19500872192878974\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15060351901374994\n",
      "  mean_inference_ms: 0.9941218885590556\n",
      "  mean_raw_obs_processing_ms: 0.12264772244542715\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -869.783026937473\n",
      "  episode_reward_mean: -1104.3215881837662\n",
      "  episode_reward_min: -1743.8171049985622\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -875.7753349070881\n",
      "    - -1388.6901767063432\n",
      "    - -912.9721937046883\n",
      "    - -892.4014995035656\n",
      "    - -1481.5242510329465\n",
      "    - -890.797759185585\n",
      "    - -981.7935780331239\n",
      "    - -892.7628125613498\n",
      "    - -869.783026937473\n",
      "    - -1165.8574661717607\n",
      "    - -1089.7636121275962\n",
      "    - -891.9997983164245\n",
      "    - -1187.0470818862646\n",
      "    - -900.0040290882952\n",
      "    - -1094.334778994522\n",
      "    - -985.7779594974711\n",
      "    - -956.3408023903338\n",
      "    - -892.822718478695\n",
      "    - -1311.510658190914\n",
      "    - -955.401381079712\n",
      "    - -1336.9585484780464\n",
      "    - -882.7024292550866\n",
      "    - -925.0476316853435\n",
      "    - -889.951625990872\n",
      "    - -888.8469715841813\n",
      "    - -1075.5155817425577\n",
      "    - -872.375758502517\n",
      "    - -1435.7447740591772\n",
      "    - -1079.3901728894414\n",
      "    - -886.2055417058722\n",
      "    - -1668.5204760461902\n",
      "    - -992.3474920261582\n",
      "    - -1708.3555250995707\n",
      "    - -1420.9314198586353\n",
      "    - -1577.487861716656\n",
      "    - -1587.9189574929333\n",
      "    - -1083.7784292639465\n",
      "    - -1205.0068928979367\n",
      "    - -1172.3356077977455\n",
      "    - -892.4598467366662\n",
      "    - -1083.5820226132557\n",
      "    - -882.9803017575294\n",
      "    - -1323.453777086682\n",
      "    - -1249.3256807429905\n",
      "    - -1212.2066294924307\n",
      "    - -1467.3810756931694\n",
      "    - -1430.4474686863337\n",
      "    - -882.7829468940323\n",
      "    - -1529.3966646064284\n",
      "    - -1743.8171049985622\n",
      "    - -1313.0368107812524\n",
      "    - -1136.8223516270436\n",
      "    - -940.0173505287696\n",
      "    - -1163.223917029895\n",
      "    - -1373.3455968986245\n",
      "    - -1178.547785105757\n",
      "    - -881.4181406725165\n",
      "    - -1438.8851761674994\n",
      "    - -894.703486123693\n",
      "    - -1325.1461055328211\n",
      "    - -910.5053531721694\n",
      "    - -1532.2185717606621\n",
      "    - -988.4354599254037\n",
      "    - -979.1010012476248\n",
      "    - -1506.358410768735\n",
      "    - -1153.0669473890941\n",
      "    - -877.4146546423009\n",
      "    - -894.5803587055217\n",
      "    - -1092.5342771775781\n",
      "    - -895.1120878849082\n",
      "    - -1165.1543705574763\n",
      "    - -981.7068142491336\n",
      "    - -872.376302358168\n",
      "    - -1430.408243129801\n",
      "    - -879.0386667193285\n",
      "    - -889.9913424332275\n",
      "    - -1185.8047479845227\n",
      "    - -1006.6560836288053\n",
      "    - -896.1983761802643\n",
      "    - -999.6842809161229\n",
      "    - -977.9984842368008\n",
      "    - -1130.078156772574\n",
      "    - -904.0485262093057\n",
      "    - -1130.0554014916643\n",
      "    - -892.863945682144\n",
      "    - -889.0321343490192\n",
      "    - -1286.897191133949\n",
      "    - -890.1509913952094\n",
      "    - -982.3503530819916\n",
      "    - -1066.0284170432865\n",
      "    - -892.9532942962991\n",
      "    - -987.5887703437241\n",
      "    - -897.7272958037491\n",
      "    - -982.9371131027739\n",
      "    - -893.5618200339485\n",
      "    - -1315.96455826425\n",
      "    - -1547.7843571318435\n",
      "    - -905.0284236818542\n",
      "    - -891.5409349538515\n",
      "    - -1181.459441874556\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19500872192878974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15060351901374994\n",
      "    mean_inference_ms: 0.9941218885590556\n",
      "    mean_raw_obs_processing_ms: 0.12264772244542715\n",
      "time_since_restore: 1579.24405002594\n",
      "time_this_iter_s: 9.650551795959473\n",
      "time_total_s: 1579.24405002594\n",
      "timers:\n",
      "  learn_throughput: 985.932\n",
      "  learn_time_ms: 4057.075\n",
      "  load_throughput: 17499964.535\n",
      "  load_time_ms: 0.229\n",
      "  training_iteration_time_ms: 9517.112\n",
      "  update_time_ms: 2.13\n",
      "timestamp: 1660565503\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 596000\n",
      "training_iteration: 149\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 600000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 600000\n",
      "  num_agent_steps_trained: 600000\n",
      "  num_env_steps_sampled: 600000\n",
      "  num_env_steps_trained: 600000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-11-52\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -872.375758502517\n",
      "episode_reward_mean: -1127.230397617861\n",
      "episode_reward_min: -1775.6990802593618\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3000\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7697353363037109\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5393638610839844\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01266868319362402\n",
      "        model: {}\n",
      "        policy_loss: 0.010185463353991508\n",
      "        total_loss: 9.935857772827148\n",
      "        vf_explained_var: -0.023820873349905014\n",
      "        vf_loss: 9.91592025756836\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 600000\n",
      "  num_agent_steps_trained: 600000\n",
      "  num_env_steps_sampled: 600000\n",
      "  num_env_steps_trained: 600000\n",
      "iterations_since_restore: 150\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 600000\n",
      "num_agent_steps_trained: 600000\n",
      "num_env_steps_sampled: 600000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 600000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.314285714285724\n",
      "  ram_util_percent: 60.20714285714288\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19490333781437472\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15052563463755977\n",
      "  mean_inference_ms: 0.9935788161254234\n",
      "  mean_raw_obs_processing_ms: 0.12258396481413149\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -872.375758502517\n",
      "  episode_reward_mean: -1127.230397617861\n",
      "  episode_reward_min: -1775.6990802593618\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1336.9585484780464\n",
      "    - -882.7024292550866\n",
      "    - -925.0476316853435\n",
      "    - -889.951625990872\n",
      "    - -888.8469715841813\n",
      "    - -1075.5155817425577\n",
      "    - -872.375758502517\n",
      "    - -1435.7447740591772\n",
      "    - -1079.3901728894414\n",
      "    - -886.2055417058722\n",
      "    - -1668.5204760461902\n",
      "    - -992.3474920261582\n",
      "    - -1708.3555250995707\n",
      "    - -1420.9314198586353\n",
      "    - -1577.487861716656\n",
      "    - -1587.9189574929333\n",
      "    - -1083.7784292639465\n",
      "    - -1205.0068928979367\n",
      "    - -1172.3356077977455\n",
      "    - -892.4598467366662\n",
      "    - -1083.5820226132557\n",
      "    - -882.9803017575294\n",
      "    - -1323.453777086682\n",
      "    - -1249.3256807429905\n",
      "    - -1212.2066294924307\n",
      "    - -1467.3810756931694\n",
      "    - -1430.4474686863337\n",
      "    - -882.7829468940323\n",
      "    - -1529.3966646064284\n",
      "    - -1743.8171049985622\n",
      "    - -1313.0368107812524\n",
      "    - -1136.8223516270436\n",
      "    - -940.0173505287696\n",
      "    - -1163.223917029895\n",
      "    - -1373.3455968986245\n",
      "    - -1178.547785105757\n",
      "    - -881.4181406725165\n",
      "    - -1438.8851761674994\n",
      "    - -894.703486123693\n",
      "    - -1325.1461055328211\n",
      "    - -910.5053531721694\n",
      "    - -1532.2185717606621\n",
      "    - -988.4354599254037\n",
      "    - -979.1010012476248\n",
      "    - -1506.358410768735\n",
      "    - -1153.0669473890941\n",
      "    - -877.4146546423009\n",
      "    - -894.5803587055217\n",
      "    - -1092.5342771775781\n",
      "    - -895.1120878849082\n",
      "    - -1165.1543705574763\n",
      "    - -981.7068142491336\n",
      "    - -872.376302358168\n",
      "    - -1430.408243129801\n",
      "    - -879.0386667193285\n",
      "    - -889.9913424332275\n",
      "    - -1185.8047479845227\n",
      "    - -1006.6560836288053\n",
      "    - -896.1983761802643\n",
      "    - -999.6842809161229\n",
      "    - -977.9984842368008\n",
      "    - -1130.078156772574\n",
      "    - -904.0485262093057\n",
      "    - -1130.0554014916643\n",
      "    - -892.863945682144\n",
      "    - -889.0321343490192\n",
      "    - -1286.897191133949\n",
      "    - -890.1509913952094\n",
      "    - -982.3503530819916\n",
      "    - -1066.0284170432865\n",
      "    - -892.9532942962991\n",
      "    - -987.5887703437241\n",
      "    - -897.7272958037491\n",
      "    - -982.9371131027739\n",
      "    - -893.5618200339485\n",
      "    - -1315.96455826425\n",
      "    - -1547.7843571318435\n",
      "    - -905.0284236818542\n",
      "    - -891.5409349538515\n",
      "    - -1181.459441874556\n",
      "    - -990.4914833504557\n",
      "    - -921.8143621201\n",
      "    - -889.6271366519869\n",
      "    - -1363.4848045518102\n",
      "    - -987.293522355044\n",
      "    - -896.5807322724444\n",
      "    - -1169.4823828765152\n",
      "    - -899.1788328439802\n",
      "    - -1350.8998859029514\n",
      "    - -1006.6818328586787\n",
      "    - -1163.231349658169\n",
      "    - -1442.8425084561982\n",
      "    - -1248.6671624581988\n",
      "    - -1117.14212939523\n",
      "    - -1313.2152004021752\n",
      "    - -1551.8284296196996\n",
      "    - -934.3290985050311\n",
      "    - -938.0283301989006\n",
      "    - -947.7235974667161\n",
      "    - -1775.6990802593618\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19490333781437472\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15052563463755977\n",
      "    mean_inference_ms: 0.9935788161254234\n",
      "    mean_raw_obs_processing_ms: 0.12258396481413149\n",
      "time_since_restore: 1588.643482208252\n",
      "time_this_iter_s: 9.399432182312012\n",
      "time_total_s: 1588.643482208252\n",
      "timers:\n",
      "  learn_throughput: 986.51\n",
      "  learn_time_ms: 4054.7\n",
      "  load_throughput: 16216137.638\n",
      "  load_time_ms: 0.247\n",
      "  training_iteration_time_ms: 9501.133\n",
      "  update_time_ms: 2.136\n",
      "timestamp: 1660565512\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 600000\n",
      "training_iteration: 150\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 604000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 604000\n",
      "  num_agent_steps_trained: 604000\n",
      "  num_env_steps_sampled: 604000\n",
      "  num_env_steps_trained: 604000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-12-02\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -872.376302358168\n",
      "episode_reward_mean: -1104.8948162765666\n",
      "episode_reward_min: -1775.6990802593618\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3020\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7697353363037109\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6339331865310669\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013200495392084122\n",
      "        model: {}\n",
      "        policy_loss: 0.007864768616855145\n",
      "        total_loss: 9.887301445007324\n",
      "        vf_explained_var: -0.02477526292204857\n",
      "        vf_loss: 9.869275093078613\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 604000\n",
      "  num_agent_steps_trained: 604000\n",
      "  num_env_steps_sampled: 604000\n",
      "  num_env_steps_trained: 604000\n",
      "iterations_since_restore: 151\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 604000\n",
      "num_agent_steps_trained: 604000\n",
      "num_env_steps_sampled: 604000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 604000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.628571428571426\n",
      "  ram_util_percent: 60.20714285714288\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19479873246970833\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1504516907073732\n",
      "  mean_inference_ms: 0.9930527903092655\n",
      "  mean_raw_obs_processing_ms: 0.12252577110398953\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -872.376302358168\n",
      "  episode_reward_mean: -1104.8948162765666\n",
      "  episode_reward_min: -1775.6990802593618\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1083.5820226132557\n",
      "    - -882.9803017575294\n",
      "    - -1323.453777086682\n",
      "    - -1249.3256807429905\n",
      "    - -1212.2066294924307\n",
      "    - -1467.3810756931694\n",
      "    - -1430.4474686863337\n",
      "    - -882.7829468940323\n",
      "    - -1529.3966646064284\n",
      "    - -1743.8171049985622\n",
      "    - -1313.0368107812524\n",
      "    - -1136.8223516270436\n",
      "    - -940.0173505287696\n",
      "    - -1163.223917029895\n",
      "    - -1373.3455968986245\n",
      "    - -1178.547785105757\n",
      "    - -881.4181406725165\n",
      "    - -1438.8851761674994\n",
      "    - -894.703486123693\n",
      "    - -1325.1461055328211\n",
      "    - -910.5053531721694\n",
      "    - -1532.2185717606621\n",
      "    - -988.4354599254037\n",
      "    - -979.1010012476248\n",
      "    - -1506.358410768735\n",
      "    - -1153.0669473890941\n",
      "    - -877.4146546423009\n",
      "    - -894.5803587055217\n",
      "    - -1092.5342771775781\n",
      "    - -895.1120878849082\n",
      "    - -1165.1543705574763\n",
      "    - -981.7068142491336\n",
      "    - -872.376302358168\n",
      "    - -1430.408243129801\n",
      "    - -879.0386667193285\n",
      "    - -889.9913424332275\n",
      "    - -1185.8047479845227\n",
      "    - -1006.6560836288053\n",
      "    - -896.1983761802643\n",
      "    - -999.6842809161229\n",
      "    - -977.9984842368008\n",
      "    - -1130.078156772574\n",
      "    - -904.0485262093057\n",
      "    - -1130.0554014916643\n",
      "    - -892.863945682144\n",
      "    - -889.0321343490192\n",
      "    - -1286.897191133949\n",
      "    - -890.1509913952094\n",
      "    - -982.3503530819916\n",
      "    - -1066.0284170432865\n",
      "    - -892.9532942962991\n",
      "    - -987.5887703437241\n",
      "    - -897.7272958037491\n",
      "    - -982.9371131027739\n",
      "    - -893.5618200339485\n",
      "    - -1315.96455826425\n",
      "    - -1547.7843571318435\n",
      "    - -905.0284236818542\n",
      "    - -891.5409349538515\n",
      "    - -1181.459441874556\n",
      "    - -990.4914833504557\n",
      "    - -921.8143621201\n",
      "    - -889.6271366519869\n",
      "    - -1363.4848045518102\n",
      "    - -987.293522355044\n",
      "    - -896.5807322724444\n",
      "    - -1169.4823828765152\n",
      "    - -899.1788328439802\n",
      "    - -1350.8998859029514\n",
      "    - -1006.6818328586787\n",
      "    - -1163.231349658169\n",
      "    - -1442.8425084561982\n",
      "    - -1248.6671624581988\n",
      "    - -1117.14212939523\n",
      "    - -1313.2152004021752\n",
      "    - -1551.8284296196996\n",
      "    - -934.3290985050311\n",
      "    - -938.0283301989006\n",
      "    - -947.7235974667161\n",
      "    - -1775.6990802593618\n",
      "    - -906.1852610549591\n",
      "    - -911.4136072585053\n",
      "    - -930.3908074863888\n",
      "    - -977.1182649244146\n",
      "    - -979.723302091227\n",
      "    - -1264.1157288735506\n",
      "    - -982.9621825652664\n",
      "    - -1304.6644408856446\n",
      "    - -1190.1093181369401\n",
      "    - -918.0964217007158\n",
      "    - -1462.5320278704692\n",
      "    - -896.5100110561652\n",
      "    - -916.5292041625429\n",
      "    - -890.6266124854437\n",
      "    - -990.7074769928892\n",
      "    - -1647.074150413391\n",
      "    - -874.8536114120093\n",
      "    - -898.3579926841509\n",
      "    - -1482.6507932783625\n",
      "    - -923.7021953670683\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19479873246970833\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1504516907073732\n",
      "    mean_inference_ms: 0.9930527903092655\n",
      "    mean_raw_obs_processing_ms: 0.12252577110398953\n",
      "time_since_restore: 1598.3404533863068\n",
      "time_this_iter_s: 9.69697117805481\n",
      "time_total_s: 1598.3404533863068\n",
      "timers:\n",
      "  learn_throughput: 983.957\n",
      "  learn_time_ms: 4065.218\n",
      "  load_throughput: 16181728.395\n",
      "  load_time_ms: 0.247\n",
      "  training_iteration_time_ms: 9517.661\n",
      "  update_time_ms: 2.087\n",
      "timestamp: 1660565522\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 604000\n",
      "training_iteration: 151\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 608000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 608000\n",
      "  num_agent_steps_trained: 608000\n",
      "  num_env_steps_sampled: 608000\n",
      "  num_env_steps_trained: 608000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-12-11\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -777.7543553993999\n",
      "episode_reward_mean: -1091.0514636009848\n",
      "episode_reward_min: -1775.6990802593618\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3040\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7697353363037109\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8716038465499878\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013256973586976528\n",
      "        model: {}\n",
      "        policy_loss: 0.013969194144010544\n",
      "        total_loss: 9.871663093566895\n",
      "        vf_explained_var: -0.01770908758044243\n",
      "        vf_loss: 9.847488403320312\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 608000\n",
      "  num_agent_steps_trained: 608000\n",
      "  num_env_steps_sampled: 608000\n",
      "  num_env_steps_trained: 608000\n",
      "iterations_since_restore: 152\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 608000\n",
      "num_agent_steps_trained: 608000\n",
      "num_env_steps_sampled: 608000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 608000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.41538461538461\n",
      "  ram_util_percent: 60.29999999999998\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19469749969588931\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15037512467658226\n",
      "  mean_inference_ms: 0.9925090834600602\n",
      "  mean_raw_obs_processing_ms: 0.12246467747118414\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -777.7543553993999\n",
      "  episode_reward_mean: -1091.0514636009848\n",
      "  episode_reward_min: -1775.6990802593618\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -910.5053531721694\n",
      "    - -1532.2185717606621\n",
      "    - -988.4354599254037\n",
      "    - -979.1010012476248\n",
      "    - -1506.358410768735\n",
      "    - -1153.0669473890941\n",
      "    - -877.4146546423009\n",
      "    - -894.5803587055217\n",
      "    - -1092.5342771775781\n",
      "    - -895.1120878849082\n",
      "    - -1165.1543705574763\n",
      "    - -981.7068142491336\n",
      "    - -872.376302358168\n",
      "    - -1430.408243129801\n",
      "    - -879.0386667193285\n",
      "    - -889.9913424332275\n",
      "    - -1185.8047479845227\n",
      "    - -1006.6560836288053\n",
      "    - -896.1983761802643\n",
      "    - -999.6842809161229\n",
      "    - -977.9984842368008\n",
      "    - -1130.078156772574\n",
      "    - -904.0485262093057\n",
      "    - -1130.0554014916643\n",
      "    - -892.863945682144\n",
      "    - -889.0321343490192\n",
      "    - -1286.897191133949\n",
      "    - -890.1509913952094\n",
      "    - -982.3503530819916\n",
      "    - -1066.0284170432865\n",
      "    - -892.9532942962991\n",
      "    - -987.5887703437241\n",
      "    - -897.7272958037491\n",
      "    - -982.9371131027739\n",
      "    - -893.5618200339485\n",
      "    - -1315.96455826425\n",
      "    - -1547.7843571318435\n",
      "    - -905.0284236818542\n",
      "    - -891.5409349538515\n",
      "    - -1181.459441874556\n",
      "    - -990.4914833504557\n",
      "    - -921.8143621201\n",
      "    - -889.6271366519869\n",
      "    - -1363.4848045518102\n",
      "    - -987.293522355044\n",
      "    - -896.5807322724444\n",
      "    - -1169.4823828765152\n",
      "    - -899.1788328439802\n",
      "    - -1350.8998859029514\n",
      "    - -1006.6818328586787\n",
      "    - -1163.231349658169\n",
      "    - -1442.8425084561982\n",
      "    - -1248.6671624581988\n",
      "    - -1117.14212939523\n",
      "    - -1313.2152004021752\n",
      "    - -1551.8284296196996\n",
      "    - -934.3290985050311\n",
      "    - -938.0283301989006\n",
      "    - -947.7235974667161\n",
      "    - -1775.6990802593618\n",
      "    - -906.1852610549591\n",
      "    - -911.4136072585053\n",
      "    - -930.3908074863888\n",
      "    - -977.1182649244146\n",
      "    - -979.723302091227\n",
      "    - -1264.1157288735506\n",
      "    - -982.9621825652664\n",
      "    - -1304.6644408856446\n",
      "    - -1190.1093181369401\n",
      "    - -918.0964217007158\n",
      "    - -1462.5320278704692\n",
      "    - -896.5100110561652\n",
      "    - -916.5292041625429\n",
      "    - -890.6266124854437\n",
      "    - -990.7074769928892\n",
      "    - -1647.074150413391\n",
      "    - -874.8536114120093\n",
      "    - -898.3579926841509\n",
      "    - -1482.6507932783625\n",
      "    - -923.7021953670683\n",
      "    - -987.5238882648368\n",
      "    - -1302.8244274281806\n",
      "    - -917.4207394983587\n",
      "    - -1077.4121313255794\n",
      "    - -977.6504284928984\n",
      "    - -904.1801523108858\n",
      "    - -1391.8902326169464\n",
      "    - -1193.4758662167699\n",
      "    - -1078.3214093771944\n",
      "    - -1605.5319277695244\n",
      "    - -1106.6935590509156\n",
      "    - -1362.6323444480427\n",
      "    - -983.7064979172397\n",
      "    - -1483.28309824133\n",
      "    - -777.7543553993999\n",
      "    - -902.368388545607\n",
      "    - -948.1581698218574\n",
      "    - -1225.623430459808\n",
      "    - -1545.165686231689\n",
      "    - -1294.5683920640272\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19469749969588931\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15037512467658226\n",
      "    mean_inference_ms: 0.9925090834600602\n",
      "    mean_raw_obs_processing_ms: 0.12246467747118414\n",
      "time_since_restore: 1607.7250363826752\n",
      "time_this_iter_s: 9.384582996368408\n",
      "time_total_s: 1607.7250363826752\n",
      "timers:\n",
      "  learn_throughput: 984.911\n",
      "  learn_time_ms: 4061.28\n",
      "  load_throughput: 16191098.244\n",
      "  load_time_ms: 0.247\n",
      "  training_iteration_time_ms: 9503.266\n",
      "  update_time_ms: 2.097\n",
      "timestamp: 1660565531\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 608000\n",
      "training_iteration: 152\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 612000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 612000\n",
      "  num_agent_steps_trained: 612000\n",
      "  num_env_steps_sampled: 612000\n",
      "  num_env_steps_trained: 612000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-12-21\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -777.7543553993999\n",
      "episode_reward_mean: -1101.8578037057366\n",
      "episode_reward_min: -1775.6990802593618\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3060\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7697353363037109\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0983513593673706\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01014595665037632\n",
      "        model: {}\n",
      "        policy_loss: 0.010012684389948845\n",
      "        total_loss: 9.892647743225098\n",
      "        vf_explained_var: -0.025192338973283768\n",
      "        vf_loss: 9.874824523925781\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 612000\n",
      "  num_agent_steps_trained: 612000\n",
      "  num_env_steps_sampled: 612000\n",
      "  num_env_steps_trained: 612000\n",
      "iterations_since_restore: 153\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 612000\n",
      "num_agent_steps_trained: 612000\n",
      "num_env_steps_sampled: 612000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 612000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.40714285714286\n",
      "  ram_util_percent: 60.22142857142859\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19460336465180583\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15030687836981163\n",
      "  mean_inference_ms: 0.9920197464964626\n",
      "  mean_raw_obs_processing_ms: 0.12241129715554173\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -777.7543553993999\n",
      "  episode_reward_mean: -1101.8578037057366\n",
      "  episode_reward_min: -1775.6990802593618\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -977.9984842368008\n",
      "    - -1130.078156772574\n",
      "    - -904.0485262093057\n",
      "    - -1130.0554014916643\n",
      "    - -892.863945682144\n",
      "    - -889.0321343490192\n",
      "    - -1286.897191133949\n",
      "    - -890.1509913952094\n",
      "    - -982.3503530819916\n",
      "    - -1066.0284170432865\n",
      "    - -892.9532942962991\n",
      "    - -987.5887703437241\n",
      "    - -897.7272958037491\n",
      "    - -982.9371131027739\n",
      "    - -893.5618200339485\n",
      "    - -1315.96455826425\n",
      "    - -1547.7843571318435\n",
      "    - -905.0284236818542\n",
      "    - -891.5409349538515\n",
      "    - -1181.459441874556\n",
      "    - -990.4914833504557\n",
      "    - -921.8143621201\n",
      "    - -889.6271366519869\n",
      "    - -1363.4848045518102\n",
      "    - -987.293522355044\n",
      "    - -896.5807322724444\n",
      "    - -1169.4823828765152\n",
      "    - -899.1788328439802\n",
      "    - -1350.8998859029514\n",
      "    - -1006.6818328586787\n",
      "    - -1163.231349658169\n",
      "    - -1442.8425084561982\n",
      "    - -1248.6671624581988\n",
      "    - -1117.14212939523\n",
      "    - -1313.2152004021752\n",
      "    - -1551.8284296196996\n",
      "    - -934.3290985050311\n",
      "    - -938.0283301989006\n",
      "    - -947.7235974667161\n",
      "    - -1775.6990802593618\n",
      "    - -906.1852610549591\n",
      "    - -911.4136072585053\n",
      "    - -930.3908074863888\n",
      "    - -977.1182649244146\n",
      "    - -979.723302091227\n",
      "    - -1264.1157288735506\n",
      "    - -982.9621825652664\n",
      "    - -1304.6644408856446\n",
      "    - -1190.1093181369401\n",
      "    - -918.0964217007158\n",
      "    - -1462.5320278704692\n",
      "    - -896.5100110561652\n",
      "    - -916.5292041625429\n",
      "    - -890.6266124854437\n",
      "    - -990.7074769928892\n",
      "    - -1647.074150413391\n",
      "    - -874.8536114120093\n",
      "    - -898.3579926841509\n",
      "    - -1482.6507932783625\n",
      "    - -923.7021953670683\n",
      "    - -987.5238882648368\n",
      "    - -1302.8244274281806\n",
      "    - -917.4207394983587\n",
      "    - -1077.4121313255794\n",
      "    - -977.6504284928984\n",
      "    - -904.1801523108858\n",
      "    - -1391.8902326169464\n",
      "    - -1193.4758662167699\n",
      "    - -1078.3214093771944\n",
      "    - -1605.5319277695244\n",
      "    - -1106.6935590509156\n",
      "    - -1362.6323444480427\n",
      "    - -983.7064979172397\n",
      "    - -1483.28309824133\n",
      "    - -777.7543553993999\n",
      "    - -902.368388545607\n",
      "    - -948.1581698218574\n",
      "    - -1225.623430459808\n",
      "    - -1545.165686231689\n",
      "    - -1294.5683920640272\n",
      "    - -1098.6861525693548\n",
      "    - -905.332337989731\n",
      "    - -1442.4916966970227\n",
      "    - -907.3965497132043\n",
      "    - -884.2356809644511\n",
      "    - -884.7562744820129\n",
      "    - -972.0196384178922\n",
      "    - -970.2602420451213\n",
      "    - -1718.9880026934773\n",
      "    - -981.6077213115746\n",
      "    - -1065.43178245466\n",
      "    - -992.3188517226314\n",
      "    - -1553.4467592249493\n",
      "    - -1321.099853520643\n",
      "    - -1116.5875646810432\n",
      "    - -897.2505217986746\n",
      "    - -960.0588393604827\n",
      "    - -1672.129400396481\n",
      "    - -885.701836634069\n",
      "    - -987.1806546285569\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19460336465180583\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15030687836981163\n",
      "    mean_inference_ms: 0.9920197464964626\n",
      "    mean_raw_obs_processing_ms: 0.12241129715554173\n",
      "time_since_restore: 1617.4263322353363\n",
      "time_this_iter_s: 9.701295852661133\n",
      "time_total_s: 1617.4263322353363\n",
      "timers:\n",
      "  learn_throughput: 982.1\n",
      "  learn_time_ms: 4072.904\n",
      "  load_throughput: 16824324.108\n",
      "  load_time_ms: 0.238\n",
      "  training_iteration_time_ms: 9527.173\n",
      "  update_time_ms: 2.121\n",
      "timestamp: 1660565541\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 612000\n",
      "training_iteration: 153\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 616000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 616000\n",
      "  num_agent_steps_trained: 616000\n",
      "  num_env_steps_sampled: 616000\n",
      "  num_env_steps_trained: 616000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-12-31\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -777.7543553993999\n",
      "episode_reward_mean: -1116.2030144369842\n",
      "episode_reward_min: -1775.6990802593618\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3080\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7697353363037109\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.036834955215454\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013013863936066628\n",
      "        model: {}\n",
      "        policy_loss: 0.009645260870456696\n",
      "        total_loss: 9.792754173278809\n",
      "        vf_explained_var: -0.028547650203108788\n",
      "        vf_loss: 9.773091316223145\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 616000\n",
      "  num_agent_steps_trained: 616000\n",
      "  num_env_steps_sampled: 616000\n",
      "  num_env_steps_trained: 616000\n",
      "iterations_since_restore: 154\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 616000\n",
      "num_agent_steps_trained: 616000\n",
      "num_env_steps_sampled: 616000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 616000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.3\n",
      "  ram_util_percent: 60.29999999999998\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19450559810124254\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15023368478821192\n",
      "  mean_inference_ms: 0.9914775949917924\n",
      "  mean_raw_obs_processing_ms: 0.12235272518590085\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -777.7543553993999\n",
      "  episode_reward_mean: -1116.2030144369842\n",
      "  episode_reward_min: -1775.6990802593618\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -990.4914833504557\n",
      "    - -921.8143621201\n",
      "    - -889.6271366519869\n",
      "    - -1363.4848045518102\n",
      "    - -987.293522355044\n",
      "    - -896.5807322724444\n",
      "    - -1169.4823828765152\n",
      "    - -899.1788328439802\n",
      "    - -1350.8998859029514\n",
      "    - -1006.6818328586787\n",
      "    - -1163.231349658169\n",
      "    - -1442.8425084561982\n",
      "    - -1248.6671624581988\n",
      "    - -1117.14212939523\n",
      "    - -1313.2152004021752\n",
      "    - -1551.8284296196996\n",
      "    - -934.3290985050311\n",
      "    - -938.0283301989006\n",
      "    - -947.7235974667161\n",
      "    - -1775.6990802593618\n",
      "    - -906.1852610549591\n",
      "    - -911.4136072585053\n",
      "    - -930.3908074863888\n",
      "    - -977.1182649244146\n",
      "    - -979.723302091227\n",
      "    - -1264.1157288735506\n",
      "    - -982.9621825652664\n",
      "    - -1304.6644408856446\n",
      "    - -1190.1093181369401\n",
      "    - -918.0964217007158\n",
      "    - -1462.5320278704692\n",
      "    - -896.5100110561652\n",
      "    - -916.5292041625429\n",
      "    - -890.6266124854437\n",
      "    - -990.7074769928892\n",
      "    - -1647.074150413391\n",
      "    - -874.8536114120093\n",
      "    - -898.3579926841509\n",
      "    - -1482.6507932783625\n",
      "    - -923.7021953670683\n",
      "    - -987.5238882648368\n",
      "    - -1302.8244274281806\n",
      "    - -917.4207394983587\n",
      "    - -1077.4121313255794\n",
      "    - -977.6504284928984\n",
      "    - -904.1801523108858\n",
      "    - -1391.8902326169464\n",
      "    - -1193.4758662167699\n",
      "    - -1078.3214093771944\n",
      "    - -1605.5319277695244\n",
      "    - -1106.6935590509156\n",
      "    - -1362.6323444480427\n",
      "    - -983.7064979172397\n",
      "    - -1483.28309824133\n",
      "    - -777.7543553993999\n",
      "    - -902.368388545607\n",
      "    - -948.1581698218574\n",
      "    - -1225.623430459808\n",
      "    - -1545.165686231689\n",
      "    - -1294.5683920640272\n",
      "    - -1098.6861525693548\n",
      "    - -905.332337989731\n",
      "    - -1442.4916966970227\n",
      "    - -907.3965497132043\n",
      "    - -884.2356809644511\n",
      "    - -884.7562744820129\n",
      "    - -972.0196384178922\n",
      "    - -970.2602420451213\n",
      "    - -1718.9880026934773\n",
      "    - -981.6077213115746\n",
      "    - -1065.43178245466\n",
      "    - -992.3188517226314\n",
      "    - -1553.4467592249493\n",
      "    - -1321.099853520643\n",
      "    - -1116.5875646810432\n",
      "    - -897.2505217986746\n",
      "    - -960.0588393604827\n",
      "    - -1672.129400396481\n",
      "    - -885.701836634069\n",
      "    - -987.1806546285569\n",
      "    - -1118.2934083417804\n",
      "    - -989.2867816775001\n",
      "    - -1152.661757168655\n",
      "    - -1465.5349485456948\n",
      "    - -975.7875696877085\n",
      "    - -1026.657849112449\n",
      "    - -893.3939489022223\n",
      "    - -1399.0489041075807\n",
      "    - -1029.11238025547\n",
      "    - -1026.7434022935909\n",
      "    - -1004.2102535871569\n",
      "    - -1355.7499707666093\n",
      "    - -1012.906445495028\n",
      "    - -929.3762581804199\n",
      "    - -888.3111818753994\n",
      "    - -917.858646970191\n",
      "    - -1376.1432577421526\n",
      "    - -1736.2234311329535\n",
      "    - -891.0024565989509\n",
      "    - -892.2678315660314\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19450559810124254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15023368478821192\n",
      "    mean_inference_ms: 0.9914775949917924\n",
      "    mean_raw_obs_processing_ms: 0.12235272518590085\n",
      "time_since_restore: 1626.8665523529053\n",
      "time_this_iter_s: 9.44022011756897\n",
      "time_total_s: 1626.8665523529053\n",
      "timers:\n",
      "  learn_throughput: 982.75\n",
      "  learn_time_ms: 4070.21\n",
      "  load_throughput: 16738716.951\n",
      "  load_time_ms: 0.239\n",
      "  training_iteration_time_ms: 9520.675\n",
      "  update_time_ms: 2.131\n",
      "timestamp: 1660565551\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 616000\n",
      "training_iteration: 154\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 620000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 620000\n",
      "  num_agent_steps_trained: 620000\n",
      "  num_env_steps_sampled: 620000\n",
      "  num_env_steps_trained: 620000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-12-42\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -777.7543553993999\n",
      "episode_reward_mean: -1112.5623209333419\n",
      "episode_reward_min: -1774.9876891167094\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3100\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7697353363037109\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.250554084777832\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012182837352156639\n",
      "        model: {}\n",
      "        policy_loss: 0.013296079821884632\n",
      "        total_loss: 9.880975723266602\n",
      "        vf_explained_var: -0.02590700425207615\n",
      "        vf_loss: 9.858302116394043\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 620000\n",
      "  num_agent_steps_trained: 620000\n",
      "  num_env_steps_sampled: 620000\n",
      "  num_env_steps_trained: 620000\n",
      "iterations_since_restore: 155\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 620000\n",
      "num_agent_steps_trained: 620000\n",
      "num_env_steps_sampled: 620000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 620000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 63.08125\n",
      "  ram_util_percent: 60.33125\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19441222554180895\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15016690832725157\n",
      "  mean_inference_ms: 0.9909998071646489\n",
      "  mean_raw_obs_processing_ms: 0.12230092567122988\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -777.7543553993999\n",
      "  episode_reward_mean: -1112.5623209333419\n",
      "  episode_reward_min: -1774.9876891167094\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -906.1852610549591\n",
      "    - -911.4136072585053\n",
      "    - -930.3908074863888\n",
      "    - -977.1182649244146\n",
      "    - -979.723302091227\n",
      "    - -1264.1157288735506\n",
      "    - -982.9621825652664\n",
      "    - -1304.6644408856446\n",
      "    - -1190.1093181369401\n",
      "    - -918.0964217007158\n",
      "    - -1462.5320278704692\n",
      "    - -896.5100110561652\n",
      "    - -916.5292041625429\n",
      "    - -890.6266124854437\n",
      "    - -990.7074769928892\n",
      "    - -1647.074150413391\n",
      "    - -874.8536114120093\n",
      "    - -898.3579926841509\n",
      "    - -1482.6507932783625\n",
      "    - -923.7021953670683\n",
      "    - -987.5238882648368\n",
      "    - -1302.8244274281806\n",
      "    - -917.4207394983587\n",
      "    - -1077.4121313255794\n",
      "    - -977.6504284928984\n",
      "    - -904.1801523108858\n",
      "    - -1391.8902326169464\n",
      "    - -1193.4758662167699\n",
      "    - -1078.3214093771944\n",
      "    - -1605.5319277695244\n",
      "    - -1106.6935590509156\n",
      "    - -1362.6323444480427\n",
      "    - -983.7064979172397\n",
      "    - -1483.28309824133\n",
      "    - -777.7543553993999\n",
      "    - -902.368388545607\n",
      "    - -948.1581698218574\n",
      "    - -1225.623430459808\n",
      "    - -1545.165686231689\n",
      "    - -1294.5683920640272\n",
      "    - -1098.6861525693548\n",
      "    - -905.332337989731\n",
      "    - -1442.4916966970227\n",
      "    - -907.3965497132043\n",
      "    - -884.2356809644511\n",
      "    - -884.7562744820129\n",
      "    - -972.0196384178922\n",
      "    - -970.2602420451213\n",
      "    - -1718.9880026934773\n",
      "    - -981.6077213115746\n",
      "    - -1065.43178245466\n",
      "    - -992.3188517226314\n",
      "    - -1553.4467592249493\n",
      "    - -1321.099853520643\n",
      "    - -1116.5875646810432\n",
      "    - -897.2505217986746\n",
      "    - -960.0588393604827\n",
      "    - -1672.129400396481\n",
      "    - -885.701836634069\n",
      "    - -987.1806546285569\n",
      "    - -1118.2934083417804\n",
      "    - -989.2867816775001\n",
      "    - -1152.661757168655\n",
      "    - -1465.5349485456948\n",
      "    - -975.7875696877085\n",
      "    - -1026.657849112449\n",
      "    - -893.3939489022223\n",
      "    - -1399.0489041075807\n",
      "    - -1029.11238025547\n",
      "    - -1026.7434022935909\n",
      "    - -1004.2102535871569\n",
      "    - -1355.7499707666093\n",
      "    - -1012.906445495028\n",
      "    - -929.3762581804199\n",
      "    - -888.3111818753994\n",
      "    - -917.858646970191\n",
      "    - -1376.1432577421526\n",
      "    - -1736.2234311329535\n",
      "    - -891.0024565989509\n",
      "    - -892.2678315660314\n",
      "    - -920.6139516195251\n",
      "    - -1643.9536780357275\n",
      "    - -982.81960306999\n",
      "    - -907.3925086828694\n",
      "    - -990.9334954578038\n",
      "    - -898.7252608081441\n",
      "    - -890.3204010806204\n",
      "    - -1401.253258489419\n",
      "    - -984.1068697824344\n",
      "    - -1774.9876891167094\n",
      "    - -993.7887710694374\n",
      "    - -1575.1071079361814\n",
      "    - -997.4166237744938\n",
      "    - -1530.7075212171696\n",
      "    - -996.4466670259902\n",
      "    - -890.2310550301489\n",
      "    - -998.1039232840728\n",
      "    - -998.4270763657801\n",
      "    - -1176.590026714361\n",
      "    - -992.2470232785573\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19441222554180895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15016690832725157\n",
      "    mean_inference_ms: 0.9909998071646489\n",
      "    mean_raw_obs_processing_ms: 0.12230092567122988\n",
      "time_since_restore: 1638.0574872493744\n",
      "time_this_iter_s: 11.190934896469116\n",
      "time_total_s: 1638.0574872493744\n",
      "timers:\n",
      "  learn_throughput: 946.283\n",
      "  learn_time_ms: 4227.067\n",
      "  load_throughput: 16762130.083\n",
      "  load_time_ms: 0.239\n",
      "  training_iteration_time_ms: 9688.192\n",
      "  update_time_ms: 2.139\n",
      "timestamp: 1660565562\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 620000\n",
      "training_iteration: 155\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 624000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 624000\n",
      "  num_agent_steps_trained: 624000\n",
      "  num_env_steps_sampled: 624000\n",
      "  num_env_steps_trained: 624000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-12-52\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -777.7543553993999\n",
      "episode_reward_mean: -1125.6631887116148\n",
      "episode_reward_min: -1774.9876891167094\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3120\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7697353363037109\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4510687589645386\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012630895711481571\n",
      "        model: {}\n",
      "        policy_loss: 0.009871493093669415\n",
      "        total_loss: 9.914444923400879\n",
      "        vf_explained_var: -0.025249987840652466\n",
      "        vf_loss: 9.894850730895996\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 624000\n",
      "  num_agent_steps_trained: 624000\n",
      "  num_env_steps_sampled: 624000\n",
      "  num_env_steps_trained: 624000\n",
      "iterations_since_restore: 156\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 624000\n",
      "num_agent_steps_trained: 624000\n",
      "num_env_steps_sampled: 624000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 624000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.8875\n",
      "  ram_util_percent: 60.4625\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19434039622855628\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15011067953120566\n",
      "  mean_inference_ms: 0.9905916229380684\n",
      "  mean_raw_obs_processing_ms: 0.12225648780632926\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -777.7543553993999\n",
      "  episode_reward_mean: -1125.6631887116148\n",
      "  episode_reward_min: -1774.9876891167094\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -987.5238882648368\n",
      "    - -1302.8244274281806\n",
      "    - -917.4207394983587\n",
      "    - -1077.4121313255794\n",
      "    - -977.6504284928984\n",
      "    - -904.1801523108858\n",
      "    - -1391.8902326169464\n",
      "    - -1193.4758662167699\n",
      "    - -1078.3214093771944\n",
      "    - -1605.5319277695244\n",
      "    - -1106.6935590509156\n",
      "    - -1362.6323444480427\n",
      "    - -983.7064979172397\n",
      "    - -1483.28309824133\n",
      "    - -777.7543553993999\n",
      "    - -902.368388545607\n",
      "    - -948.1581698218574\n",
      "    - -1225.623430459808\n",
      "    - -1545.165686231689\n",
      "    - -1294.5683920640272\n",
      "    - -1098.6861525693548\n",
      "    - -905.332337989731\n",
      "    - -1442.4916966970227\n",
      "    - -907.3965497132043\n",
      "    - -884.2356809644511\n",
      "    - -884.7562744820129\n",
      "    - -972.0196384178922\n",
      "    - -970.2602420451213\n",
      "    - -1718.9880026934773\n",
      "    - -981.6077213115746\n",
      "    - -1065.43178245466\n",
      "    - -992.3188517226314\n",
      "    - -1553.4467592249493\n",
      "    - -1321.099853520643\n",
      "    - -1116.5875646810432\n",
      "    - -897.2505217986746\n",
      "    - -960.0588393604827\n",
      "    - -1672.129400396481\n",
      "    - -885.701836634069\n",
      "    - -987.1806546285569\n",
      "    - -1118.2934083417804\n",
      "    - -989.2867816775001\n",
      "    - -1152.661757168655\n",
      "    - -1465.5349485456948\n",
      "    - -975.7875696877085\n",
      "    - -1026.657849112449\n",
      "    - -893.3939489022223\n",
      "    - -1399.0489041075807\n",
      "    - -1029.11238025547\n",
      "    - -1026.7434022935909\n",
      "    - -1004.2102535871569\n",
      "    - -1355.7499707666093\n",
      "    - -1012.906445495028\n",
      "    - -929.3762581804199\n",
      "    - -888.3111818753994\n",
      "    - -917.858646970191\n",
      "    - -1376.1432577421526\n",
      "    - -1736.2234311329535\n",
      "    - -891.0024565989509\n",
      "    - -892.2678315660314\n",
      "    - -920.6139516195251\n",
      "    - -1643.9536780357275\n",
      "    - -982.81960306999\n",
      "    - -907.3925086828694\n",
      "    - -990.9334954578038\n",
      "    - -898.7252608081441\n",
      "    - -890.3204010806204\n",
      "    - -1401.253258489419\n",
      "    - -984.1068697824344\n",
      "    - -1774.9876891167094\n",
      "    - -993.7887710694374\n",
      "    - -1575.1071079361814\n",
      "    - -997.4166237744938\n",
      "    - -1530.7075212171696\n",
      "    - -996.4466670259902\n",
      "    - -890.2310550301489\n",
      "    - -998.1039232840728\n",
      "    - -998.4270763657801\n",
      "    - -1176.590026714361\n",
      "    - -992.2470232785573\n",
      "    - -1056.9481634719446\n",
      "    - -1519.0694493373662\n",
      "    - -971.9454534183508\n",
      "    - -900.5313672151183\n",
      "    - -890.7111567987803\n",
      "    - -1626.8810885751993\n",
      "    - -1441.3378796310797\n",
      "    - -888.7850652019254\n",
      "    - -1062.4823574583934\n",
      "    - -939.9898760189003\n",
      "    - -886.8890879084053\n",
      "    - -990.7083737626275\n",
      "    - -1113.5213294647162\n",
      "    - -969.8093417436602\n",
      "    - -1017.6146189947401\n",
      "    - -1546.5097986846797\n",
      "    - -891.7886792138459\n",
      "    - -979.0159481036502\n",
      "    - -1460.079104406989\n",
      "    - -1503.7920491169878\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19434039622855628\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15011067953120566\n",
      "    mean_inference_ms: 0.9905916229380684\n",
      "    mean_raw_obs_processing_ms: 0.12225648780632926\n",
      "time_since_restore: 1648.60289478302\n",
      "time_this_iter_s: 10.54540753364563\n",
      "time_total_s: 1648.60289478302\n",
      "timers:\n",
      "  learn_throughput: 932.363\n",
      "  learn_time_ms: 4290.176\n",
      "  load_throughput: 17027520.552\n",
      "  load_time_ms: 0.235\n",
      "  training_iteration_time_ms: 9788.238\n",
      "  update_time_ms: 2.164\n",
      "timestamp: 1660565572\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 624000\n",
      "training_iteration: 156\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 628000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 628000\n",
      "  num_agent_steps_trained: 628000\n",
      "  num_env_steps_sampled: 628000\n",
      "  num_env_steps_trained: 628000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-13-05\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -883.8383167385047\n",
      "episode_reward_mean: -1114.1094391761333\n",
      "episode_reward_min: -1774.9876891167094\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3140\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7697353363037109\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0307613611221313\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01223339606076479\n",
      "        model: {}\n",
      "        policy_loss: 0.0133752953261137\n",
      "        total_loss: 9.838329315185547\n",
      "        vf_explained_var: -0.019933177158236504\n",
      "        vf_loss: 9.815536499023438\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 628000\n",
      "  num_agent_steps_trained: 628000\n",
      "  num_env_steps_sampled: 628000\n",
      "  num_env_steps_trained: 628000\n",
      "iterations_since_restore: 157\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 628000\n",
      "num_agent_steps_trained: 628000\n",
      "num_env_steps_sampled: 628000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 628000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 65.88333333333334\n",
      "  ram_util_percent: 60.37777777777779\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19433467440848737\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15010156589980092\n",
      "  mean_inference_ms: 0.9905368309027675\n",
      "  mean_raw_obs_processing_ms: 0.12225080856557359\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -883.8383167385047\n",
      "  episode_reward_mean: -1114.1094391761333\n",
      "  episode_reward_min: -1774.9876891167094\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1098.6861525693548\n",
      "    - -905.332337989731\n",
      "    - -1442.4916966970227\n",
      "    - -907.3965497132043\n",
      "    - -884.2356809644511\n",
      "    - -884.7562744820129\n",
      "    - -972.0196384178922\n",
      "    - -970.2602420451213\n",
      "    - -1718.9880026934773\n",
      "    - -981.6077213115746\n",
      "    - -1065.43178245466\n",
      "    - -992.3188517226314\n",
      "    - -1553.4467592249493\n",
      "    - -1321.099853520643\n",
      "    - -1116.5875646810432\n",
      "    - -897.2505217986746\n",
      "    - -960.0588393604827\n",
      "    - -1672.129400396481\n",
      "    - -885.701836634069\n",
      "    - -987.1806546285569\n",
      "    - -1118.2934083417804\n",
      "    - -989.2867816775001\n",
      "    - -1152.661757168655\n",
      "    - -1465.5349485456948\n",
      "    - -975.7875696877085\n",
      "    - -1026.657849112449\n",
      "    - -893.3939489022223\n",
      "    - -1399.0489041075807\n",
      "    - -1029.11238025547\n",
      "    - -1026.7434022935909\n",
      "    - -1004.2102535871569\n",
      "    - -1355.7499707666093\n",
      "    - -1012.906445495028\n",
      "    - -929.3762581804199\n",
      "    - -888.3111818753994\n",
      "    - -917.858646970191\n",
      "    - -1376.1432577421526\n",
      "    - -1736.2234311329535\n",
      "    - -891.0024565989509\n",
      "    - -892.2678315660314\n",
      "    - -920.6139516195251\n",
      "    - -1643.9536780357275\n",
      "    - -982.81960306999\n",
      "    - -907.3925086828694\n",
      "    - -990.9334954578038\n",
      "    - -898.7252608081441\n",
      "    - -890.3204010806204\n",
      "    - -1401.253258489419\n",
      "    - -984.1068697824344\n",
      "    - -1774.9876891167094\n",
      "    - -993.7887710694374\n",
      "    - -1575.1071079361814\n",
      "    - -997.4166237744938\n",
      "    - -1530.7075212171696\n",
      "    - -996.4466670259902\n",
      "    - -890.2310550301489\n",
      "    - -998.1039232840728\n",
      "    - -998.4270763657801\n",
      "    - -1176.590026714361\n",
      "    - -992.2470232785573\n",
      "    - -1056.9481634719446\n",
      "    - -1519.0694493373662\n",
      "    - -971.9454534183508\n",
      "    - -900.5313672151183\n",
      "    - -890.7111567987803\n",
      "    - -1626.8810885751993\n",
      "    - -1441.3378796310797\n",
      "    - -888.7850652019254\n",
      "    - -1062.4823574583934\n",
      "    - -939.9898760189003\n",
      "    - -886.8890879084053\n",
      "    - -990.7083737626275\n",
      "    - -1113.5213294647162\n",
      "    - -969.8093417436602\n",
      "    - -1017.6146189947401\n",
      "    - -1546.5097986846797\n",
      "    - -891.7886792138459\n",
      "    - -979.0159481036502\n",
      "    - -1460.079104406989\n",
      "    - -1503.7920491169878\n",
      "    - -984.6123367693611\n",
      "    - -1363.9924919018508\n",
      "    - -1012.061787566009\n",
      "    - -1088.1739440747776\n",
      "    - -894.224788337115\n",
      "    - -919.9086388785416\n",
      "    - -1227.8932989837442\n",
      "    - -901.7776357225237\n",
      "    - -1528.8532342758435\n",
      "    - -916.2242068856906\n",
      "    - -894.1468851403879\n",
      "    - -893.2058740869231\n",
      "    - -883.8383167385047\n",
      "    - -1430.793963321021\n",
      "    - -1522.7395079198122\n",
      "    - -1307.0968821350534\n",
      "    - -1183.322600598851\n",
      "    - -980.0312890063709\n",
      "    - -974.930532392924\n",
      "    - -1002.9819571976473\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19433467440848737\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15010156589980092\n",
      "    mean_inference_ms: 0.9905368309027675\n",
      "    mean_raw_obs_processing_ms: 0.12225080856557359\n",
      "time_since_restore: 1661.376999616623\n",
      "time_this_iter_s: 12.774104833602905\n",
      "time_total_s: 1661.376999616623\n",
      "timers:\n",
      "  learn_throughput: 894.893\n",
      "  learn_time_ms: 4469.806\n",
      "  load_throughput: 16963817.998\n",
      "  load_time_ms: 0.236\n",
      "  training_iteration_time_ms: 10115.143\n",
      "  update_time_ms: 2.165\n",
      "timestamp: 1660565585\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 628000\n",
      "training_iteration: 157\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 632000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 632000\n",
      "  num_agent_steps_trained: 632000\n",
      "  num_env_steps_sampled: 632000\n",
      "  num_env_steps_trained: 632000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-13-16\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -883.8383167385047\n",
      "episode_reward_mean: -1125.3142262329163\n",
      "episode_reward_min: -1774.9876891167094\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3160\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7697353363037109\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0547280311584473\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012039266526699066\n",
      "        model: {}\n",
      "        policy_loss: 0.011334652081131935\n",
      "        total_loss: 9.808231353759766\n",
      "        vf_explained_var: -0.015937674790620804\n",
      "        vf_loss: 9.787630081176758\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 632000\n",
      "  num_agent_steps_trained: 632000\n",
      "  num_env_steps_sampled: 632000\n",
      "  num_env_steps_trained: 632000\n",
      "iterations_since_restore: 158\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 632000\n",
      "num_agent_steps_trained: 632000\n",
      "num_env_steps_sampled: 632000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 632000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 61.22\n",
      "  ram_util_percent: 60.526666666666664\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19435771822859393\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15010652236724728\n",
      "  mean_inference_ms: 0.9906022635639876\n",
      "  mean_raw_obs_processing_ms: 0.12225508498814104\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -883.8383167385047\n",
      "  episode_reward_mean: -1125.3142262329163\n",
      "  episode_reward_min: -1774.9876891167094\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1118.2934083417804\n",
      "    - -989.2867816775001\n",
      "    - -1152.661757168655\n",
      "    - -1465.5349485456948\n",
      "    - -975.7875696877085\n",
      "    - -1026.657849112449\n",
      "    - -893.3939489022223\n",
      "    - -1399.0489041075807\n",
      "    - -1029.11238025547\n",
      "    - -1026.7434022935909\n",
      "    - -1004.2102535871569\n",
      "    - -1355.7499707666093\n",
      "    - -1012.906445495028\n",
      "    - -929.3762581804199\n",
      "    - -888.3111818753994\n",
      "    - -917.858646970191\n",
      "    - -1376.1432577421526\n",
      "    - -1736.2234311329535\n",
      "    - -891.0024565989509\n",
      "    - -892.2678315660314\n",
      "    - -920.6139516195251\n",
      "    - -1643.9536780357275\n",
      "    - -982.81960306999\n",
      "    - -907.3925086828694\n",
      "    - -990.9334954578038\n",
      "    - -898.7252608081441\n",
      "    - -890.3204010806204\n",
      "    - -1401.253258489419\n",
      "    - -984.1068697824344\n",
      "    - -1774.9876891167094\n",
      "    - -993.7887710694374\n",
      "    - -1575.1071079361814\n",
      "    - -997.4166237744938\n",
      "    - -1530.7075212171696\n",
      "    - -996.4466670259902\n",
      "    - -890.2310550301489\n",
      "    - -998.1039232840728\n",
      "    - -998.4270763657801\n",
      "    - -1176.590026714361\n",
      "    - -992.2470232785573\n",
      "    - -1056.9481634719446\n",
      "    - -1519.0694493373662\n",
      "    - -971.9454534183508\n",
      "    - -900.5313672151183\n",
      "    - -890.7111567987803\n",
      "    - -1626.8810885751993\n",
      "    - -1441.3378796310797\n",
      "    - -888.7850652019254\n",
      "    - -1062.4823574583934\n",
      "    - -939.9898760189003\n",
      "    - -886.8890879084053\n",
      "    - -990.7083737626275\n",
      "    - -1113.5213294647162\n",
      "    - -969.8093417436602\n",
      "    - -1017.6146189947401\n",
      "    - -1546.5097986846797\n",
      "    - -891.7886792138459\n",
      "    - -979.0159481036502\n",
      "    - -1460.079104406989\n",
      "    - -1503.7920491169878\n",
      "    - -984.6123367693611\n",
      "    - -1363.9924919018508\n",
      "    - -1012.061787566009\n",
      "    - -1088.1739440747776\n",
      "    - -894.224788337115\n",
      "    - -919.9086388785416\n",
      "    - -1227.8932989837442\n",
      "    - -901.7776357225237\n",
      "    - -1528.8532342758435\n",
      "    - -916.2242068856906\n",
      "    - -894.1468851403879\n",
      "    - -893.2058740869231\n",
      "    - -883.8383167385047\n",
      "    - -1430.793963321021\n",
      "    - -1522.7395079198122\n",
      "    - -1307.0968821350534\n",
      "    - -1183.322600598851\n",
      "    - -980.0312890063709\n",
      "    - -974.930532392924\n",
      "    - -1002.9819571976473\n",
      "    - -1348.6731151347915\n",
      "    - -887.4167179956188\n",
      "    - -1623.5354650663526\n",
      "    - -1437.8760521479844\n",
      "    - -1236.309037298451\n",
      "    - -1469.8541278335106\n",
      "    - -1086.7558684384155\n",
      "    - -1576.8243968719746\n",
      "    - -893.3589813441251\n",
      "    - -1087.8718436269344\n",
      "    - -907.1652962796271\n",
      "    - -894.6170720211139\n",
      "    - -894.2803055778012\n",
      "    - -982.5526033916076\n",
      "    - -1126.700423565178\n",
      "    - -1477.0642946584937\n",
      "    - -1076.7876644180783\n",
      "    - -903.2455167635895\n",
      "    - -1030.7517253838023\n",
      "    - -1395.8185591668946\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19435771822859393\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15010652236724728\n",
      "    mean_inference_ms: 0.9906022635639876\n",
      "    mean_raw_obs_processing_ms: 0.12225508498814104\n",
      "time_since_restore: 1672.2837481498718\n",
      "time_this_iter_s: 10.906748533248901\n",
      "time_total_s: 1672.2837481498718\n",
      "timers:\n",
      "  learn_throughput: 880.623\n",
      "  learn_time_ms: 4542.238\n",
      "  load_throughput: 17663946.094\n",
      "  load_time_ms: 0.226\n",
      "  training_iteration_time_ms: 10262.075\n",
      "  update_time_ms: 2.164\n",
      "timestamp: 1660565596\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 632000\n",
      "training_iteration: 158\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 636000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 636000\n",
      "  num_agent_steps_trained: 636000\n",
      "  num_env_steps_sampled: 636000\n",
      "  num_env_steps_trained: 636000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-13-28\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -869.6542182053144\n",
      "episode_reward_mean: -1133.0986391832555\n",
      "episode_reward_min: -1774.9876891167094\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3180\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7697353363037109\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6328728199005127\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015297161415219307\n",
      "        model: {}\n",
      "        policy_loss: 0.01563509926199913\n",
      "        total_loss: 9.802901268005371\n",
      "        vf_explained_var: -0.017001349478960037\n",
      "        vf_loss: 9.775491714477539\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 636000\n",
      "  num_agent_steps_trained: 636000\n",
      "  num_env_steps_sampled: 636000\n",
      "  num_env_steps_trained: 636000\n",
      "iterations_since_restore: 159\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 636000\n",
      "num_agent_steps_trained: 636000\n",
      "num_env_steps_sampled: 636000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 636000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 67.66666666666667\n",
      "  ram_util_percent: 60.6111111111111\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.194465831160055\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15017586900037883\n",
      "  mean_inference_ms: 0.9911339835372286\n",
      "  mean_raw_obs_processing_ms: 0.12231012736974309\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -869.6542182053144\n",
      "  episode_reward_mean: -1133.0986391832555\n",
      "  episode_reward_min: -1774.9876891167094\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -920.6139516195251\n",
      "    - -1643.9536780357275\n",
      "    - -982.81960306999\n",
      "    - -907.3925086828694\n",
      "    - -990.9334954578038\n",
      "    - -898.7252608081441\n",
      "    - -890.3204010806204\n",
      "    - -1401.253258489419\n",
      "    - -984.1068697824344\n",
      "    - -1774.9876891167094\n",
      "    - -993.7887710694374\n",
      "    - -1575.1071079361814\n",
      "    - -997.4166237744938\n",
      "    - -1530.7075212171696\n",
      "    - -996.4466670259902\n",
      "    - -890.2310550301489\n",
      "    - -998.1039232840728\n",
      "    - -998.4270763657801\n",
      "    - -1176.590026714361\n",
      "    - -992.2470232785573\n",
      "    - -1056.9481634719446\n",
      "    - -1519.0694493373662\n",
      "    - -971.9454534183508\n",
      "    - -900.5313672151183\n",
      "    - -890.7111567987803\n",
      "    - -1626.8810885751993\n",
      "    - -1441.3378796310797\n",
      "    - -888.7850652019254\n",
      "    - -1062.4823574583934\n",
      "    - -939.9898760189003\n",
      "    - -886.8890879084053\n",
      "    - -990.7083737626275\n",
      "    - -1113.5213294647162\n",
      "    - -969.8093417436602\n",
      "    - -1017.6146189947401\n",
      "    - -1546.5097986846797\n",
      "    - -891.7886792138459\n",
      "    - -979.0159481036502\n",
      "    - -1460.079104406989\n",
      "    - -1503.7920491169878\n",
      "    - -984.6123367693611\n",
      "    - -1363.9924919018508\n",
      "    - -1012.061787566009\n",
      "    - -1088.1739440747776\n",
      "    - -894.224788337115\n",
      "    - -919.9086388785416\n",
      "    - -1227.8932989837442\n",
      "    - -901.7776357225237\n",
      "    - -1528.8532342758435\n",
      "    - -916.2242068856906\n",
      "    - -894.1468851403879\n",
      "    - -893.2058740869231\n",
      "    - -883.8383167385047\n",
      "    - -1430.793963321021\n",
      "    - -1522.7395079198122\n",
      "    - -1307.0968821350534\n",
      "    - -1183.322600598851\n",
      "    - -980.0312890063709\n",
      "    - -974.930532392924\n",
      "    - -1002.9819571976473\n",
      "    - -1348.6731151347915\n",
      "    - -887.4167179956188\n",
      "    - -1623.5354650663526\n",
      "    - -1437.8760521479844\n",
      "    - -1236.309037298451\n",
      "    - -1469.8541278335106\n",
      "    - -1086.7558684384155\n",
      "    - -1576.8243968719746\n",
      "    - -893.3589813441251\n",
      "    - -1087.8718436269344\n",
      "    - -907.1652962796271\n",
      "    - -894.6170720211139\n",
      "    - -894.2803055778012\n",
      "    - -982.5526033916076\n",
      "    - -1126.700423565178\n",
      "    - -1477.0642946584937\n",
      "    - -1076.7876644180783\n",
      "    - -903.2455167635895\n",
      "    - -1030.7517253838023\n",
      "    - -1395.8185591668946\n",
      "    - -993.6288599191308\n",
      "    - -869.6542182053144\n",
      "    - -1009.5912648156988\n",
      "    - -1479.0442734188373\n",
      "    - -1231.869935996151\n",
      "    - -989.0625298893258\n",
      "    - -1580.6611247958058\n",
      "    - -1331.4362362462255\n",
      "    - -1568.323405383677\n",
      "    - -1371.772715749867\n",
      "    - -982.3725787591239\n",
      "    - -1258.6118495566618\n",
      "    - -1076.2278648576907\n",
      "    - -892.7136834499199\n",
      "    - -1005.3196121571305\n",
      "    - -974.9367128165024\n",
      "    - -978.2594498097711\n",
      "    - -982.6989621661305\n",
      "    - -985.8291346207777\n",
      "    - -1296.9975664276792\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.194465831160055\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15017586900037883\n",
      "    mean_inference_ms: 0.9911339835372286\n",
      "    mean_raw_obs_processing_ms: 0.12231012736974309\n",
      "time_since_restore: 1684.3552269935608\n",
      "time_this_iter_s: 12.071478843688965\n",
      "time_total_s: 1684.3552269935608\n",
      "timers:\n",
      "  learn_throughput: 870.408\n",
      "  learn_time_ms: 4595.544\n",
      "  load_throughput: 17791321.315\n",
      "  load_time_ms: 0.225\n",
      "  training_iteration_time_ms: 10504.204\n",
      "  update_time_ms: 2.173\n",
      "timestamp: 1660565608\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 636000\n",
      "training_iteration: 159\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 640000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 640000\n",
      "  num_agent_steps_trained: 640000\n",
      "  num_env_steps_sampled: 640000\n",
      "  num_env_steps_trained: 640000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-13-38\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -860.8623161102929\n",
      "episode_reward_mean: -1135.13187683284\n",
      "episode_reward_min: -1722.7863282307871\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3200\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7697353363037109\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6861577033996582\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015808582305908203\n",
      "        model: {}\n",
      "        policy_loss: 0.016589045524597168\n",
      "        total_loss: 9.875039100646973\n",
      "        vf_explained_var: -0.013802072033286095\n",
      "        vf_loss: 9.846281051635742\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 640000\n",
      "  num_agent_steps_trained: 640000\n",
      "  num_env_steps_sampled: 640000\n",
      "  num_env_steps_trained: 640000\n",
      "iterations_since_restore: 160\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 640000\n",
      "num_agent_steps_trained: 640000\n",
      "num_env_steps_sampled: 640000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 640000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 58.41428571428572\n",
      "  ram_util_percent: 60.671428571428585\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19458915985750372\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1502513286062186\n",
      "  mean_inference_ms: 0.9917173997280705\n",
      "  mean_raw_obs_processing_ms: 0.12236948867364195\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -860.8623161102929\n",
      "  episode_reward_mean: -1135.13187683284\n",
      "  episode_reward_min: -1722.7863282307871\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1056.9481634719446\n",
      "    - -1519.0694493373662\n",
      "    - -971.9454534183508\n",
      "    - -900.5313672151183\n",
      "    - -890.7111567987803\n",
      "    - -1626.8810885751993\n",
      "    - -1441.3378796310797\n",
      "    - -888.7850652019254\n",
      "    - -1062.4823574583934\n",
      "    - -939.9898760189003\n",
      "    - -886.8890879084053\n",
      "    - -990.7083737626275\n",
      "    - -1113.5213294647162\n",
      "    - -969.8093417436602\n",
      "    - -1017.6146189947401\n",
      "    - -1546.5097986846797\n",
      "    - -891.7886792138459\n",
      "    - -979.0159481036502\n",
      "    - -1460.079104406989\n",
      "    - -1503.7920491169878\n",
      "    - -984.6123367693611\n",
      "    - -1363.9924919018508\n",
      "    - -1012.061787566009\n",
      "    - -1088.1739440747776\n",
      "    - -894.224788337115\n",
      "    - -919.9086388785416\n",
      "    - -1227.8932989837442\n",
      "    - -901.7776357225237\n",
      "    - -1528.8532342758435\n",
      "    - -916.2242068856906\n",
      "    - -894.1468851403879\n",
      "    - -893.2058740869231\n",
      "    - -883.8383167385047\n",
      "    - -1430.793963321021\n",
      "    - -1522.7395079198122\n",
      "    - -1307.0968821350534\n",
      "    - -1183.322600598851\n",
      "    - -980.0312890063709\n",
      "    - -974.930532392924\n",
      "    - -1002.9819571976473\n",
      "    - -1348.6731151347915\n",
      "    - -887.4167179956188\n",
      "    - -1623.5354650663526\n",
      "    - -1437.8760521479844\n",
      "    - -1236.309037298451\n",
      "    - -1469.8541278335106\n",
      "    - -1086.7558684384155\n",
      "    - -1576.8243968719746\n",
      "    - -893.3589813441251\n",
      "    - -1087.8718436269344\n",
      "    - -907.1652962796271\n",
      "    - -894.6170720211139\n",
      "    - -894.2803055778012\n",
      "    - -982.5526033916076\n",
      "    - -1126.700423565178\n",
      "    - -1477.0642946584937\n",
      "    - -1076.7876644180783\n",
      "    - -903.2455167635895\n",
      "    - -1030.7517253838023\n",
      "    - -1395.8185591668946\n",
      "    - -993.6288599191308\n",
      "    - -869.6542182053144\n",
      "    - -1009.5912648156988\n",
      "    - -1479.0442734188373\n",
      "    - -1231.869935996151\n",
      "    - -989.0625298893258\n",
      "    - -1580.6611247958058\n",
      "    - -1331.4362362462255\n",
      "    - -1568.323405383677\n",
      "    - -1371.772715749867\n",
      "    - -982.3725787591239\n",
      "    - -1258.6118495566618\n",
      "    - -1076.2278648576907\n",
      "    - -892.7136834499199\n",
      "    - -1005.3196121571305\n",
      "    - -974.9367128165024\n",
      "    - -978.2594498097711\n",
      "    - -982.6989621661305\n",
      "    - -985.8291346207777\n",
      "    - -1296.9975664276792\n",
      "    - -1538.04313350108\n",
      "    - -1388.9982230483774\n",
      "    - -942.0935210073433\n",
      "    - -1103.9752492146263\n",
      "    - -891.3645123256529\n",
      "    - -1332.026168695188\n",
      "    - -1021.0549154762258\n",
      "    - -1282.7948415908913\n",
      "    - -906.9884991334459\n",
      "    - -1432.4532338058223\n",
      "    - -884.0386519514112\n",
      "    - -892.7430932399924\n",
      "    - -1006.0126102832309\n",
      "    - -886.5581800484988\n",
      "    - -860.8623161102929\n",
      "    - -1425.5289795807405\n",
      "    - -975.0100361105681\n",
      "    - -1359.6661272265262\n",
      "    - -1722.7863282307871\n",
      "    - -894.4976562172168\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19458915985750372\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1502513286062186\n",
      "    mean_inference_ms: 0.9917173997280705\n",
      "    mean_raw_obs_processing_ms: 0.12236948867364195\n",
      "time_since_restore: 1694.4492237567902\n",
      "time_this_iter_s: 10.09399676322937\n",
      "time_total_s: 1694.4492237567902\n",
      "timers:\n",
      "  learn_throughput: 866.37\n",
      "  learn_time_ms: 4616.968\n",
      "  load_throughput: 19301905.2\n",
      "  load_time_ms: 0.207\n",
      "  training_iteration_time_ms: 10573.716\n",
      "  update_time_ms: 2.164\n",
      "timestamp: 1660565618\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 640000\n",
      "training_iteration: 160\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 644000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 644000\n",
      "  num_agent_steps_trained: 644000\n",
      "  num_env_steps_sampled: 644000\n",
      "  num_env_steps_trained: 644000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-13-48\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -860.8623161102929\n",
      "episode_reward_mean: -1157.2924984932342\n",
      "episode_reward_min: -1730.6902843350247\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3220\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7697353363037109\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3398263454437256\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023392843082547188\n",
      "        model: {}\n",
      "        policy_loss: 0.020188122987747192\n",
      "        total_loss: 9.901082038879395\n",
      "        vf_explained_var: -0.020453253760933876\n",
      "        vf_loss: 9.862887382507324\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 644000\n",
      "  num_agent_steps_trained: 644000\n",
      "  num_env_steps_sampled: 644000\n",
      "  num_env_steps_trained: 644000\n",
      "iterations_since_restore: 161\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 644000\n",
      "num_agent_steps_trained: 644000\n",
      "num_env_steps_sampled: 644000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 644000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.40714285714286\n",
      "  ram_util_percent: 60.60000000000001\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19469226359620806\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15031793947104663\n",
      "  mean_inference_ms: 0.9922336246303977\n",
      "  mean_raw_obs_processing_ms: 0.12242289217603478\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -860.8623161102929\n",
      "  episode_reward_mean: -1157.2924984932342\n",
      "  episode_reward_min: -1730.6902843350247\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -984.6123367693611\n",
      "    - -1363.9924919018508\n",
      "    - -1012.061787566009\n",
      "    - -1088.1739440747776\n",
      "    - -894.224788337115\n",
      "    - -919.9086388785416\n",
      "    - -1227.8932989837442\n",
      "    - -901.7776357225237\n",
      "    - -1528.8532342758435\n",
      "    - -916.2242068856906\n",
      "    - -894.1468851403879\n",
      "    - -893.2058740869231\n",
      "    - -883.8383167385047\n",
      "    - -1430.793963321021\n",
      "    - -1522.7395079198122\n",
      "    - -1307.0968821350534\n",
      "    - -1183.322600598851\n",
      "    - -980.0312890063709\n",
      "    - -974.930532392924\n",
      "    - -1002.9819571976473\n",
      "    - -1348.6731151347915\n",
      "    - -887.4167179956188\n",
      "    - -1623.5354650663526\n",
      "    - -1437.8760521479844\n",
      "    - -1236.309037298451\n",
      "    - -1469.8541278335106\n",
      "    - -1086.7558684384155\n",
      "    - -1576.8243968719746\n",
      "    - -893.3589813441251\n",
      "    - -1087.8718436269344\n",
      "    - -907.1652962796271\n",
      "    - -894.6170720211139\n",
      "    - -894.2803055778012\n",
      "    - -982.5526033916076\n",
      "    - -1126.700423565178\n",
      "    - -1477.0642946584937\n",
      "    - -1076.7876644180783\n",
      "    - -903.2455167635895\n",
      "    - -1030.7517253838023\n",
      "    - -1395.8185591668946\n",
      "    - -993.6288599191308\n",
      "    - -869.6542182053144\n",
      "    - -1009.5912648156988\n",
      "    - -1479.0442734188373\n",
      "    - -1231.869935996151\n",
      "    - -989.0625298893258\n",
      "    - -1580.6611247958058\n",
      "    - -1331.4362362462255\n",
      "    - -1568.323405383677\n",
      "    - -1371.772715749867\n",
      "    - -982.3725787591239\n",
      "    - -1258.6118495566618\n",
      "    - -1076.2278648576907\n",
      "    - -892.7136834499199\n",
      "    - -1005.3196121571305\n",
      "    - -974.9367128165024\n",
      "    - -978.2594498097711\n",
      "    - -982.6989621661305\n",
      "    - -985.8291346207777\n",
      "    - -1296.9975664276792\n",
      "    - -1538.04313350108\n",
      "    - -1388.9982230483774\n",
      "    - -942.0935210073433\n",
      "    - -1103.9752492146263\n",
      "    - -891.3645123256529\n",
      "    - -1332.026168695188\n",
      "    - -1021.0549154762258\n",
      "    - -1282.7948415908913\n",
      "    - -906.9884991334459\n",
      "    - -1432.4532338058223\n",
      "    - -884.0386519514112\n",
      "    - -892.7430932399924\n",
      "    - -1006.0126102832309\n",
      "    - -886.5581800484988\n",
      "    - -860.8623161102929\n",
      "    - -1425.5289795807405\n",
      "    - -975.0100361105681\n",
      "    - -1359.6661272265262\n",
      "    - -1722.7863282307871\n",
      "    - -894.4976562172168\n",
      "    - -1213.7037912416665\n",
      "    - -1019.0538678205669\n",
      "    - -1568.0253391932758\n",
      "    - -1443.1927033080779\n",
      "    - -1342.7274209302257\n",
      "    - -974.7377964377183\n",
      "    - -1410.0076411238153\n",
      "    - -991.4991764052121\n",
      "    - -1606.0067254460416\n",
      "    - -1484.8198532027238\n",
      "    - -1730.6902843350247\n",
      "    - -1064.682983265131\n",
      "    - -1260.238136737807\n",
      "    - -893.2041276285969\n",
      "    - -893.8061871113366\n",
      "    - -1511.5152989784829\n",
      "    - -1611.5774482271113\n",
      "    - -1067.514078189824\n",
      "    - -895.5261848062713\n",
      "    - -891.9433101778809\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19469226359620806\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15031793947104663\n",
      "    mean_inference_ms: 0.9922336246303977\n",
      "    mean_raw_obs_processing_ms: 0.12242289217603478\n",
      "time_since_restore: 1704.4038875102997\n",
      "time_this_iter_s: 9.954663753509521\n",
      "time_total_s: 1704.4038875102997\n",
      "timers:\n",
      "  learn_throughput: 861.51\n",
      "  learn_time_ms: 4643.008\n",
      "  load_throughput: 19346420.664\n",
      "  load_time_ms: 0.207\n",
      "  training_iteration_time_ms: 10599.534\n",
      "  update_time_ms: 2.209\n",
      "timestamp: 1660565628\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 644000\n",
      "training_iteration: 161\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 648000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 648000\n",
      "  num_agent_steps_trained: 648000\n",
      "  num_env_steps_sampled: 648000\n",
      "  num_env_steps_trained: 648000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-13-59\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -860.8623161102929\n",
      "episode_reward_mean: -1161.069141358637\n",
      "episode_reward_min: -1730.6902843350247\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3240\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.243564486503601\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012018765322864056\n",
      "        model: {}\n",
      "        policy_loss: 0.015738284215331078\n",
      "        total_loss: 9.87595272064209\n",
      "        vf_explained_var: -0.017648864537477493\n",
      "        vf_loss: 9.84633731842041\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 648000\n",
      "  num_agent_steps_trained: 648000\n",
      "  num_env_steps_sampled: 648000\n",
      "  num_env_steps_trained: 648000\n",
      "iterations_since_restore: 162\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 648000\n",
      "num_agent_steps_trained: 648000\n",
      "num_env_steps_sampled: 648000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 648000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.70625\n",
      "  ram_util_percent: 60.6875\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19474561024194628\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15035016910777593\n",
      "  mean_inference_ms: 0.9925017777805928\n",
      "  mean_raw_obs_processing_ms: 0.12244870681040354\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -860.8623161102929\n",
      "  episode_reward_mean: -1161.069141358637\n",
      "  episode_reward_min: -1730.6902843350247\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1348.6731151347915\n",
      "    - -887.4167179956188\n",
      "    - -1623.5354650663526\n",
      "    - -1437.8760521479844\n",
      "    - -1236.309037298451\n",
      "    - -1469.8541278335106\n",
      "    - -1086.7558684384155\n",
      "    - -1576.8243968719746\n",
      "    - -893.3589813441251\n",
      "    - -1087.8718436269344\n",
      "    - -907.1652962796271\n",
      "    - -894.6170720211139\n",
      "    - -894.2803055778012\n",
      "    - -982.5526033916076\n",
      "    - -1126.700423565178\n",
      "    - -1477.0642946584937\n",
      "    - -1076.7876644180783\n",
      "    - -903.2455167635895\n",
      "    - -1030.7517253838023\n",
      "    - -1395.8185591668946\n",
      "    - -993.6288599191308\n",
      "    - -869.6542182053144\n",
      "    - -1009.5912648156988\n",
      "    - -1479.0442734188373\n",
      "    - -1231.869935996151\n",
      "    - -989.0625298893258\n",
      "    - -1580.6611247958058\n",
      "    - -1331.4362362462255\n",
      "    - -1568.323405383677\n",
      "    - -1371.772715749867\n",
      "    - -982.3725787591239\n",
      "    - -1258.6118495566618\n",
      "    - -1076.2278648576907\n",
      "    - -892.7136834499199\n",
      "    - -1005.3196121571305\n",
      "    - -974.9367128165024\n",
      "    - -978.2594498097711\n",
      "    - -982.6989621661305\n",
      "    - -985.8291346207777\n",
      "    - -1296.9975664276792\n",
      "    - -1538.04313350108\n",
      "    - -1388.9982230483774\n",
      "    - -942.0935210073433\n",
      "    - -1103.9752492146263\n",
      "    - -891.3645123256529\n",
      "    - -1332.026168695188\n",
      "    - -1021.0549154762258\n",
      "    - -1282.7948415908913\n",
      "    - -906.9884991334459\n",
      "    - -1432.4532338058223\n",
      "    - -884.0386519514112\n",
      "    - -892.7430932399924\n",
      "    - -1006.0126102832309\n",
      "    - -886.5581800484988\n",
      "    - -860.8623161102929\n",
      "    - -1425.5289795807405\n",
      "    - -975.0100361105681\n",
      "    - -1359.6661272265262\n",
      "    - -1722.7863282307871\n",
      "    - -894.4976562172168\n",
      "    - -1213.7037912416665\n",
      "    - -1019.0538678205669\n",
      "    - -1568.0253391932758\n",
      "    - -1443.1927033080779\n",
      "    - -1342.7274209302257\n",
      "    - -974.7377964377183\n",
      "    - -1410.0076411238153\n",
      "    - -991.4991764052121\n",
      "    - -1606.0067254460416\n",
      "    - -1484.8198532027238\n",
      "    - -1730.6902843350247\n",
      "    - -1064.682983265131\n",
      "    - -1260.238136737807\n",
      "    - -893.2041276285969\n",
      "    - -893.8061871113366\n",
      "    - -1511.5152989784829\n",
      "    - -1611.5774482271113\n",
      "    - -1067.514078189824\n",
      "    - -895.5261848062713\n",
      "    - -891.9433101778809\n",
      "    - -880.3144845510541\n",
      "    - -985.5630865108824\n",
      "    - -898.514456385616\n",
      "    - -1218.0386769032937\n",
      "    - -969.187913771617\n",
      "    - -893.2204156832246\n",
      "    - -989.1293613791585\n",
      "    - -905.7010768191657\n",
      "    - -1469.787498497993\n",
      "    - -1641.0623165777577\n",
      "    - -892.4496301968833\n",
      "    - -1069.087401931443\n",
      "    - -987.7695608568218\n",
      "    - -1098.5055638687234\n",
      "    - -1226.018968552504\n",
      "    - -1491.9931142607802\n",
      "    - -978.5823975254676\n",
      "    - -1452.1683940339335\n",
      "    - -1342.2778050933123\n",
      "    - -899.1023350735987\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19474561024194628\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15035016910777593\n",
      "    mean_inference_ms: 0.9925017777805928\n",
      "    mean_raw_obs_processing_ms: 0.12244870681040354\n",
      "time_since_restore: 1715.0020542144775\n",
      "time_this_iter_s: 10.598166704177856\n",
      "time_total_s: 1715.0020542144775\n",
      "timers:\n",
      "  learn_throughput: 847.527\n",
      "  learn_time_ms: 4719.613\n",
      "  load_throughput: 19138964.18\n",
      "  load_time_ms: 0.209\n",
      "  training_iteration_time_ms: 10720.743\n",
      "  update_time_ms: 2.251\n",
      "timestamp: 1660565639\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 648000\n",
      "training_iteration: 162\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 652000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 652000\n",
      "  num_agent_steps_trained: 652000\n",
      "  num_env_steps_sampled: 652000\n",
      "  num_env_steps_trained: 652000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-14-12\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -860.8623161102929\n",
      "episode_reward_mean: -1157.3966304110954\n",
      "episode_reward_min: -1735.7306898501733\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3260\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7390841245651245\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007300851866602898\n",
      "        model: {}\n",
      "        policy_loss: 0.009489504620432854\n",
      "        total_loss: 9.927968978881836\n",
      "        vf_explained_var: -0.021511195227503777\n",
      "        vf_loss: 9.910049438476562\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 652000\n",
      "  num_agent_steps_trained: 652000\n",
      "  num_env_steps_sampled: 652000\n",
      "  num_env_steps_trained: 652000\n",
      "iterations_since_restore: 163\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 652000\n",
      "num_agent_steps_trained: 652000\n",
      "num_env_steps_sampled: 652000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 652000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 68.89999999999999\n",
      "  ram_util_percent: 60.68947368421052\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1948076343021693\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15039373946828355\n",
      "  mean_inference_ms: 0.9928481783424006\n",
      "  mean_raw_obs_processing_ms: 0.12248580030625765\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -860.8623161102929\n",
      "  episode_reward_mean: -1157.3966304110954\n",
      "  episode_reward_min: -1735.7306898501733\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -993.6288599191308\n",
      "    - -869.6542182053144\n",
      "    - -1009.5912648156988\n",
      "    - -1479.0442734188373\n",
      "    - -1231.869935996151\n",
      "    - -989.0625298893258\n",
      "    - -1580.6611247958058\n",
      "    - -1331.4362362462255\n",
      "    - -1568.323405383677\n",
      "    - -1371.772715749867\n",
      "    - -982.3725787591239\n",
      "    - -1258.6118495566618\n",
      "    - -1076.2278648576907\n",
      "    - -892.7136834499199\n",
      "    - -1005.3196121571305\n",
      "    - -974.9367128165024\n",
      "    - -978.2594498097711\n",
      "    - -982.6989621661305\n",
      "    - -985.8291346207777\n",
      "    - -1296.9975664276792\n",
      "    - -1538.04313350108\n",
      "    - -1388.9982230483774\n",
      "    - -942.0935210073433\n",
      "    - -1103.9752492146263\n",
      "    - -891.3645123256529\n",
      "    - -1332.026168695188\n",
      "    - -1021.0549154762258\n",
      "    - -1282.7948415908913\n",
      "    - -906.9884991334459\n",
      "    - -1432.4532338058223\n",
      "    - -884.0386519514112\n",
      "    - -892.7430932399924\n",
      "    - -1006.0126102832309\n",
      "    - -886.5581800484988\n",
      "    - -860.8623161102929\n",
      "    - -1425.5289795807405\n",
      "    - -975.0100361105681\n",
      "    - -1359.6661272265262\n",
      "    - -1722.7863282307871\n",
      "    - -894.4976562172168\n",
      "    - -1213.7037912416665\n",
      "    - -1019.0538678205669\n",
      "    - -1568.0253391932758\n",
      "    - -1443.1927033080779\n",
      "    - -1342.7274209302257\n",
      "    - -974.7377964377183\n",
      "    - -1410.0076411238153\n",
      "    - -991.4991764052121\n",
      "    - -1606.0067254460416\n",
      "    - -1484.8198532027238\n",
      "    - -1730.6902843350247\n",
      "    - -1064.682983265131\n",
      "    - -1260.238136737807\n",
      "    - -893.2041276285969\n",
      "    - -893.8061871113366\n",
      "    - -1511.5152989784829\n",
      "    - -1611.5774482271113\n",
      "    - -1067.514078189824\n",
      "    - -895.5261848062713\n",
      "    - -891.9433101778809\n",
      "    - -880.3144845510541\n",
      "    - -985.5630865108824\n",
      "    - -898.514456385616\n",
      "    - -1218.0386769032937\n",
      "    - -969.187913771617\n",
      "    - -893.2204156832246\n",
      "    - -989.1293613791585\n",
      "    - -905.7010768191657\n",
      "    - -1469.787498497993\n",
      "    - -1641.0623165777577\n",
      "    - -892.4496301968833\n",
      "    - -1069.087401931443\n",
      "    - -987.7695608568218\n",
      "    - -1098.5055638687234\n",
      "    - -1226.018968552504\n",
      "    - -1491.9931142607802\n",
      "    - -978.5823975254676\n",
      "    - -1452.1683940339335\n",
      "    - -1342.2778050933123\n",
      "    - -899.1023350735987\n",
      "    - -1523.4393133452681\n",
      "    - -1015.880234485348\n",
      "    - -1117.7306592719872\n",
      "    - -894.5747966124079\n",
      "    - -1015.3969122925618\n",
      "    - -1270.8849412287411\n",
      "    - -995.1318739383004\n",
      "    - -1434.9191167792676\n",
      "    - -920.6265571834172\n",
      "    - -1220.8109168432313\n",
      "    - -956.9333362300417\n",
      "    - -895.3774832058862\n",
      "    - -987.1223502132667\n",
      "    - -1735.7306898501733\n",
      "    - -1471.445616233085\n",
      "    - -921.480679933921\n",
      "    - -987.4374875630742\n",
      "    - -1625.916296535306\n",
      "    - -953.4075660504386\n",
      "    - -1025.9611444344823\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1948076343021693\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15039373946828355\n",
      "    mean_inference_ms: 0.9928481783424006\n",
      "    mean_raw_obs_processing_ms: 0.12248580030625765\n",
      "time_since_restore: 1728.3601636886597\n",
      "time_this_iter_s: 13.358109474182129\n",
      "time_total_s: 1728.3601636886597\n",
      "timers:\n",
      "  learn_throughput: 801.167\n",
      "  learn_time_ms: 4992.718\n",
      "  load_throughput: 17203872.026\n",
      "  load_time_ms: 0.233\n",
      "  training_iteration_time_ms: 11086.185\n",
      "  update_time_ms: 2.357\n",
      "timestamp: 1660565652\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 652000\n",
      "training_iteration: 163\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 656000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 656000\n",
      "  num_agent_steps_trained: 656000\n",
      "  num_env_steps_sampled: 656000\n",
      "  num_env_steps_trained: 656000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-14-33\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -849.7710229376382\n",
      "episode_reward_mean: -1170.2318038490607\n",
      "episode_reward_min: -1735.7306898501733\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3280\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.8460476398468018\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016710694879293442\n",
      "        model: {}\n",
      "        policy_loss: 0.01748557761311531\n",
      "        total_loss: 9.843409538269043\n",
      "        vf_explained_var: -0.011422298848628998\n",
      "        vf_loss: 9.806631088256836\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 656000\n",
      "  num_agent_steps_trained: 656000\n",
      "  num_env_steps_sampled: 656000\n",
      "  num_env_steps_trained: 656000\n",
      "iterations_since_restore: 164\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 656000\n",
      "num_agent_steps_trained: 656000\n",
      "num_env_steps_sampled: 656000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 656000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 89.91428571428571\n",
      "  ram_util_percent: 62.517857142857125\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19500526784376812\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15054041551069308\n",
      "  mean_inference_ms: 0.9939971065212836\n",
      "  mean_raw_obs_processing_ms: 0.1226136394816198\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -849.7710229376382\n",
      "  episode_reward_mean: -1170.2318038490607\n",
      "  episode_reward_min: -1735.7306898501733\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1538.04313350108\n",
      "    - -1388.9982230483774\n",
      "    - -942.0935210073433\n",
      "    - -1103.9752492146263\n",
      "    - -891.3645123256529\n",
      "    - -1332.026168695188\n",
      "    - -1021.0549154762258\n",
      "    - -1282.7948415908913\n",
      "    - -906.9884991334459\n",
      "    - -1432.4532338058223\n",
      "    - -884.0386519514112\n",
      "    - -892.7430932399924\n",
      "    - -1006.0126102832309\n",
      "    - -886.5581800484988\n",
      "    - -860.8623161102929\n",
      "    - -1425.5289795807405\n",
      "    - -975.0100361105681\n",
      "    - -1359.6661272265262\n",
      "    - -1722.7863282307871\n",
      "    - -894.4976562172168\n",
      "    - -1213.7037912416665\n",
      "    - -1019.0538678205669\n",
      "    - -1568.0253391932758\n",
      "    - -1443.1927033080779\n",
      "    - -1342.7274209302257\n",
      "    - -974.7377964377183\n",
      "    - -1410.0076411238153\n",
      "    - -991.4991764052121\n",
      "    - -1606.0067254460416\n",
      "    - -1484.8198532027238\n",
      "    - -1730.6902843350247\n",
      "    - -1064.682983265131\n",
      "    - -1260.238136737807\n",
      "    - -893.2041276285969\n",
      "    - -893.8061871113366\n",
      "    - -1511.5152989784829\n",
      "    - -1611.5774482271113\n",
      "    - -1067.514078189824\n",
      "    - -895.5261848062713\n",
      "    - -891.9433101778809\n",
      "    - -880.3144845510541\n",
      "    - -985.5630865108824\n",
      "    - -898.514456385616\n",
      "    - -1218.0386769032937\n",
      "    - -969.187913771617\n",
      "    - -893.2204156832246\n",
      "    - -989.1293613791585\n",
      "    - -905.7010768191657\n",
      "    - -1469.787498497993\n",
      "    - -1641.0623165777577\n",
      "    - -892.4496301968833\n",
      "    - -1069.087401931443\n",
      "    - -987.7695608568218\n",
      "    - -1098.5055638687234\n",
      "    - -1226.018968552504\n",
      "    - -1491.9931142607802\n",
      "    - -978.5823975254676\n",
      "    - -1452.1683940339335\n",
      "    - -1342.2778050933123\n",
      "    - -899.1023350735987\n",
      "    - -1523.4393133452681\n",
      "    - -1015.880234485348\n",
      "    - -1117.7306592719872\n",
      "    - -894.5747966124079\n",
      "    - -1015.3969122925618\n",
      "    - -1270.8849412287411\n",
      "    - -995.1318739383004\n",
      "    - -1434.9191167792676\n",
      "    - -920.6265571834172\n",
      "    - -1220.8109168432313\n",
      "    - -956.9333362300417\n",
      "    - -895.3774832058862\n",
      "    - -987.1223502132667\n",
      "    - -1735.7306898501733\n",
      "    - -1471.445616233085\n",
      "    - -921.480679933921\n",
      "    - -987.4374875630742\n",
      "    - -1625.916296535306\n",
      "    - -953.4075660504386\n",
      "    - -1025.9611444344823\n",
      "    - -985.90084318293\n",
      "    - -889.5824330999475\n",
      "    - -1075.020025699622\n",
      "    - -1550.1512946763012\n",
      "    - -1196.1956778585761\n",
      "    - -1621.1976563971111\n",
      "    - -1544.8108916166657\n",
      "    - -1206.2841918998693\n",
      "    - -1242.7524043796298\n",
      "    - -1401.4288013077776\n",
      "    - -900.7757353205108\n",
      "    - -1468.2646857387758\n",
      "    - -985.4885935670027\n",
      "    - -1063.9480106249227\n",
      "    - -1478.40218103059\n",
      "    - -975.6549503392117\n",
      "    - -894.1575026162711\n",
      "    - -1669.407431204889\n",
      "    - -849.7710229376382\n",
      "    - -1143.334989339664\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19500526784376812\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15054041551069308\n",
      "    mean_inference_ms: 0.9939971065212836\n",
      "    mean_raw_obs_processing_ms: 0.1226136394816198\n",
      "time_since_restore: 1748.491230249405\n",
      "time_this_iter_s: 20.13106656074524\n",
      "time_total_s: 1748.491230249405\n",
      "timers:\n",
      "  learn_throughput: 731.376\n",
      "  learn_time_ms: 5469.146\n",
      "  load_throughput: 15286757.175\n",
      "  load_time_ms: 0.262\n",
      "  training_iteration_time_ms: 12155.301\n",
      "  update_time_ms: 2.418\n",
      "timestamp: 1660565673\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 656000\n",
      "training_iteration: 164\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 660000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 660000\n",
      "  num_agent_steps_trained: 660000\n",
      "  num_env_steps_sampled: 660000\n",
      "  num_env_steps_trained: 660000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-14-45\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -849.7710229376382\n",
      "episode_reward_mean: -1169.2702773018464\n",
      "episode_reward_min: -1735.7306898501733\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3300\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5556230545043945\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01104887668043375\n",
      "        model: {}\n",
      "        policy_loss: 0.015419652685523033\n",
      "        total_loss: 9.884601593017578\n",
      "        vf_explained_var: -0.02524055726826191\n",
      "        vf_loss: 9.856424331665039\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 660000\n",
      "  num_agent_steps_trained: 660000\n",
      "  num_env_steps_sampled: 660000\n",
      "  num_env_steps_trained: 660000\n",
      "iterations_since_restore: 165\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 660000\n",
      "num_agent_steps_trained: 660000\n",
      "num_env_steps_sampled: 660000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 660000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 68.24999999999999\n",
      "  ram_util_percent: 63.133333333333326\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19524691991853524\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15071880914007163\n",
      "  mean_inference_ms: 0.9953979173216783\n",
      "  mean_raw_obs_processing_ms: 0.12276580689234878\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -849.7710229376382\n",
      "  episode_reward_mean: -1169.2702773018464\n",
      "  episode_reward_min: -1735.7306898501733\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1213.7037912416665\n",
      "    - -1019.0538678205669\n",
      "    - -1568.0253391932758\n",
      "    - -1443.1927033080779\n",
      "    - -1342.7274209302257\n",
      "    - -974.7377964377183\n",
      "    - -1410.0076411238153\n",
      "    - -991.4991764052121\n",
      "    - -1606.0067254460416\n",
      "    - -1484.8198532027238\n",
      "    - -1730.6902843350247\n",
      "    - -1064.682983265131\n",
      "    - -1260.238136737807\n",
      "    - -893.2041276285969\n",
      "    - -893.8061871113366\n",
      "    - -1511.5152989784829\n",
      "    - -1611.5774482271113\n",
      "    - -1067.514078189824\n",
      "    - -895.5261848062713\n",
      "    - -891.9433101778809\n",
      "    - -880.3144845510541\n",
      "    - -985.5630865108824\n",
      "    - -898.514456385616\n",
      "    - -1218.0386769032937\n",
      "    - -969.187913771617\n",
      "    - -893.2204156832246\n",
      "    - -989.1293613791585\n",
      "    - -905.7010768191657\n",
      "    - -1469.787498497993\n",
      "    - -1641.0623165777577\n",
      "    - -892.4496301968833\n",
      "    - -1069.087401931443\n",
      "    - -987.7695608568218\n",
      "    - -1098.5055638687234\n",
      "    - -1226.018968552504\n",
      "    - -1491.9931142607802\n",
      "    - -978.5823975254676\n",
      "    - -1452.1683940339335\n",
      "    - -1342.2778050933123\n",
      "    - -899.1023350735987\n",
      "    - -1523.4393133452681\n",
      "    - -1015.880234485348\n",
      "    - -1117.7306592719872\n",
      "    - -894.5747966124079\n",
      "    - -1015.3969122925618\n",
      "    - -1270.8849412287411\n",
      "    - -995.1318739383004\n",
      "    - -1434.9191167792676\n",
      "    - -920.6265571834172\n",
      "    - -1220.8109168432313\n",
      "    - -956.9333362300417\n",
      "    - -895.3774832058862\n",
      "    - -987.1223502132667\n",
      "    - -1735.7306898501733\n",
      "    - -1471.445616233085\n",
      "    - -921.480679933921\n",
      "    - -987.4374875630742\n",
      "    - -1625.916296535306\n",
      "    - -953.4075660504386\n",
      "    - -1025.9611444344823\n",
      "    - -985.90084318293\n",
      "    - -889.5824330999475\n",
      "    - -1075.020025699622\n",
      "    - -1550.1512946763012\n",
      "    - -1196.1956778585761\n",
      "    - -1621.1976563971111\n",
      "    - -1544.8108916166657\n",
      "    - -1206.2841918998693\n",
      "    - -1242.7524043796298\n",
      "    - -1401.4288013077776\n",
      "    - -900.7757353205108\n",
      "    - -1468.2646857387758\n",
      "    - -985.4885935670027\n",
      "    - -1063.9480106249227\n",
      "    - -1478.40218103059\n",
      "    - -975.6549503392117\n",
      "    - -894.1575026162711\n",
      "    - -1669.407431204889\n",
      "    - -849.7710229376382\n",
      "    - -1143.334989339664\n",
      "    - -887.4343991910315\n",
      "    - -1324.654565300728\n",
      "    - -1017.3161361684279\n",
      "    - -1017.6407020792929\n",
      "    - -1102.9662465074475\n",
      "    - -887.7598060045799\n",
      "    - -975.0427020446863\n",
      "    - -1428.8658080988812\n",
      "    - -1364.3939613936593\n",
      "    - -1016.2461744996864\n",
      "    - -1454.1737903232558\n",
      "    - -880.6961806635356\n",
      "    - -914.877994448608\n",
      "    - -912.6170049516762\n",
      "    - -1422.1722838606802\n",
      "    - -1483.34530382304\n",
      "    - -1309.9434463237872\n",
      "    - -1083.6889282862066\n",
      "    - -954.7418698689568\n",
      "    - -1212.766318238323\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19524691991853524\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15071880914007163\n",
      "    mean_inference_ms: 0.9953979173216783\n",
      "    mean_raw_obs_processing_ms: 0.12276580689234878\n",
      "time_since_restore: 1760.5068373680115\n",
      "time_this_iter_s: 12.015607118606567\n",
      "time_total_s: 1760.5068373680115\n",
      "timers:\n",
      "  learn_throughput: 739.95\n",
      "  learn_time_ms: 5405.769\n",
      "  load_throughput: 15387706.136\n",
      "  load_time_ms: 0.26\n",
      "  training_iteration_time_ms: 12237.782\n",
      "  update_time_ms: 2.425\n",
      "timestamp: 1660565685\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 660000\n",
      "training_iteration: 165\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 664000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 664000\n",
      "  num_agent_steps_trained: 664000\n",
      "  num_env_steps_sampled: 664000\n",
      "  num_env_steps_trained: 664000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-14-58\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -849.7710229376382\n",
      "episode_reward_mean: -1156.5737304585887\n",
      "episode_reward_min: -1735.7306898501733\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3320\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3077309131622314\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009993394836783409\n",
      "        model: {}\n",
      "        policy_loss: 0.01204125676304102\n",
      "        total_loss: 9.922759056091309\n",
      "        vf_explained_var: -0.025430914014577866\n",
      "        vf_loss: 9.899178504943848\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 664000\n",
      "  num_agent_steps_trained: 664000\n",
      "  num_env_steps_sampled: 664000\n",
      "  num_env_steps_trained: 664000\n",
      "iterations_since_restore: 166\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 664000\n",
      "num_agent_steps_trained: 664000\n",
      "num_env_steps_sampled: 664000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 664000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 72.34736842105262\n",
      "  ram_util_percent: 62.90526315789474\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19556115637088867\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15094321493197416\n",
      "  mean_inference_ms: 0.9971646496778734\n",
      "  mean_raw_obs_processing_ms: 0.12295353190900443\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -849.7710229376382\n",
      "  episode_reward_mean: -1156.5737304585887\n",
      "  episode_reward_min: -1735.7306898501733\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -880.3144845510541\n",
      "    - -985.5630865108824\n",
      "    - -898.514456385616\n",
      "    - -1218.0386769032937\n",
      "    - -969.187913771617\n",
      "    - -893.2204156832246\n",
      "    - -989.1293613791585\n",
      "    - -905.7010768191657\n",
      "    - -1469.787498497993\n",
      "    - -1641.0623165777577\n",
      "    - -892.4496301968833\n",
      "    - -1069.087401931443\n",
      "    - -987.7695608568218\n",
      "    - -1098.5055638687234\n",
      "    - -1226.018968552504\n",
      "    - -1491.9931142607802\n",
      "    - -978.5823975254676\n",
      "    - -1452.1683940339335\n",
      "    - -1342.2778050933123\n",
      "    - -899.1023350735987\n",
      "    - -1523.4393133452681\n",
      "    - -1015.880234485348\n",
      "    - -1117.7306592719872\n",
      "    - -894.5747966124079\n",
      "    - -1015.3969122925618\n",
      "    - -1270.8849412287411\n",
      "    - -995.1318739383004\n",
      "    - -1434.9191167792676\n",
      "    - -920.6265571834172\n",
      "    - -1220.8109168432313\n",
      "    - -956.9333362300417\n",
      "    - -895.3774832058862\n",
      "    - -987.1223502132667\n",
      "    - -1735.7306898501733\n",
      "    - -1471.445616233085\n",
      "    - -921.480679933921\n",
      "    - -987.4374875630742\n",
      "    - -1625.916296535306\n",
      "    - -953.4075660504386\n",
      "    - -1025.9611444344823\n",
      "    - -985.90084318293\n",
      "    - -889.5824330999475\n",
      "    - -1075.020025699622\n",
      "    - -1550.1512946763012\n",
      "    - -1196.1956778585761\n",
      "    - -1621.1976563971111\n",
      "    - -1544.8108916166657\n",
      "    - -1206.2841918998693\n",
      "    - -1242.7524043796298\n",
      "    - -1401.4288013077776\n",
      "    - -900.7757353205108\n",
      "    - -1468.2646857387758\n",
      "    - -985.4885935670027\n",
      "    - -1063.9480106249227\n",
      "    - -1478.40218103059\n",
      "    - -975.6549503392117\n",
      "    - -894.1575026162711\n",
      "    - -1669.407431204889\n",
      "    - -849.7710229376382\n",
      "    - -1143.334989339664\n",
      "    - -887.4343991910315\n",
      "    - -1324.654565300728\n",
      "    - -1017.3161361684279\n",
      "    - -1017.6407020792929\n",
      "    - -1102.9662465074475\n",
      "    - -887.7598060045799\n",
      "    - -975.0427020446863\n",
      "    - -1428.8658080988812\n",
      "    - -1364.3939613936593\n",
      "    - -1016.2461744996864\n",
      "    - -1454.1737903232558\n",
      "    - -880.6961806635356\n",
      "    - -914.877994448608\n",
      "    - -912.6170049516762\n",
      "    - -1422.1722838606802\n",
      "    - -1483.34530382304\n",
      "    - -1309.9434463237872\n",
      "    - -1083.6889282862066\n",
      "    - -954.7418698689568\n",
      "    - -1212.766318238323\n",
      "    - -944.0032418682725\n",
      "    - -917.7649728834677\n",
      "    - -909.1319022900439\n",
      "    - -1425.2536195347857\n",
      "    - -999.3164500926389\n",
      "    - -908.6647840631997\n",
      "    - -897.8454413252927\n",
      "    - -1607.2110760513897\n",
      "    - -1401.97174193559\n",
      "    - -1577.4803140363038\n",
      "    - -1373.3872876476537\n",
      "    - -1664.674163793421\n",
      "    - -1067.6821085053994\n",
      "    - -1009.6033996460495\n",
      "    - -1134.322780086397\n",
      "    - -1398.4966677334648\n",
      "    - -890.3994487546493\n",
      "    - -963.5463398658383\n",
      "    - -1619.6809611124431\n",
      "    - -894.3809690147264\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19556115637088867\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15094321493197416\n",
      "    mean_inference_ms: 0.9971646496778734\n",
      "    mean_raw_obs_processing_ms: 0.12295353190900443\n",
      "time_since_restore: 1773.8140697479248\n",
      "time_this_iter_s: 13.30723237991333\n",
      "time_total_s: 1773.8140697479248\n",
      "timers:\n",
      "  learn_throughput: 722.434\n",
      "  learn_time_ms: 5536.836\n",
      "  load_throughput: 15293724.704\n",
      "  load_time_ms: 0.262\n",
      "  training_iteration_time_ms: 12514.095\n",
      "  update_time_ms: 2.437\n",
      "timestamp: 1660565698\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 664000\n",
      "training_iteration: 166\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 668000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 668000\n",
      "  num_agent_steps_trained: 668000\n",
      "  num_env_steps_sampled: 668000\n",
      "  num_env_steps_trained: 668000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-15-09\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -849.7710229376382\n",
      "episode_reward_mean: -1158.4432746852908\n",
      "episode_reward_min: -1735.7306898501733\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3340\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4568849802017212\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010755384340882301\n",
      "        model: {}\n",
      "        policy_loss: 0.01429964043200016\n",
      "        total_loss: 9.874456405639648\n",
      "        vf_explained_var: -0.018528541550040245\n",
      "        vf_loss: 9.847739219665527\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 668000\n",
      "  num_agent_steps_trained: 668000\n",
      "  num_env_steps_sampled: 668000\n",
      "  num_env_steps_trained: 668000\n",
      "iterations_since_restore: 167\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 668000\n",
      "num_agent_steps_trained: 668000\n",
      "num_env_steps_sampled: 668000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 668000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 62.63333333333335\n",
      "  ram_util_percent: 62.81999999999998\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19589075703929176\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15118235495347007\n",
      "  mean_inference_ms: 0.9990357351368864\n",
      "  mean_raw_obs_processing_ms: 0.12315076304305189\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -849.7710229376382\n",
      "  episode_reward_mean: -1158.4432746852908\n",
      "  episode_reward_min: -1735.7306898501733\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1523.4393133452681\n",
      "    - -1015.880234485348\n",
      "    - -1117.7306592719872\n",
      "    - -894.5747966124079\n",
      "    - -1015.3969122925618\n",
      "    - -1270.8849412287411\n",
      "    - -995.1318739383004\n",
      "    - -1434.9191167792676\n",
      "    - -920.6265571834172\n",
      "    - -1220.8109168432313\n",
      "    - -956.9333362300417\n",
      "    - -895.3774832058862\n",
      "    - -987.1223502132667\n",
      "    - -1735.7306898501733\n",
      "    - -1471.445616233085\n",
      "    - -921.480679933921\n",
      "    - -987.4374875630742\n",
      "    - -1625.916296535306\n",
      "    - -953.4075660504386\n",
      "    - -1025.9611444344823\n",
      "    - -985.90084318293\n",
      "    - -889.5824330999475\n",
      "    - -1075.020025699622\n",
      "    - -1550.1512946763012\n",
      "    - -1196.1956778585761\n",
      "    - -1621.1976563971111\n",
      "    - -1544.8108916166657\n",
      "    - -1206.2841918998693\n",
      "    - -1242.7524043796298\n",
      "    - -1401.4288013077776\n",
      "    - -900.7757353205108\n",
      "    - -1468.2646857387758\n",
      "    - -985.4885935670027\n",
      "    - -1063.9480106249227\n",
      "    - -1478.40218103059\n",
      "    - -975.6549503392117\n",
      "    - -894.1575026162711\n",
      "    - -1669.407431204889\n",
      "    - -849.7710229376382\n",
      "    - -1143.334989339664\n",
      "    - -887.4343991910315\n",
      "    - -1324.654565300728\n",
      "    - -1017.3161361684279\n",
      "    - -1017.6407020792929\n",
      "    - -1102.9662465074475\n",
      "    - -887.7598060045799\n",
      "    - -975.0427020446863\n",
      "    - -1428.8658080988812\n",
      "    - -1364.3939613936593\n",
      "    - -1016.2461744996864\n",
      "    - -1454.1737903232558\n",
      "    - -880.6961806635356\n",
      "    - -914.877994448608\n",
      "    - -912.6170049516762\n",
      "    - -1422.1722838606802\n",
      "    - -1483.34530382304\n",
      "    - -1309.9434463237872\n",
      "    - -1083.6889282862066\n",
      "    - -954.7418698689568\n",
      "    - -1212.766318238323\n",
      "    - -944.0032418682725\n",
      "    - -917.7649728834677\n",
      "    - -909.1319022900439\n",
      "    - -1425.2536195347857\n",
      "    - -999.3164500926389\n",
      "    - -908.6647840631997\n",
      "    - -897.8454413252927\n",
      "    - -1607.2110760513897\n",
      "    - -1401.97174193559\n",
      "    - -1577.4803140363038\n",
      "    - -1373.3872876476537\n",
      "    - -1664.674163793421\n",
      "    - -1067.6821085053994\n",
      "    - -1009.6033996460495\n",
      "    - -1134.322780086397\n",
      "    - -1398.4966677334648\n",
      "    - -890.3994487546493\n",
      "    - -963.5463398658383\n",
      "    - -1619.6809611124431\n",
      "    - -894.3809690147264\n",
      "    - -1431.3464197248366\n",
      "    - -888.0425681024795\n",
      "    - -1284.5208096554807\n",
      "    - -1169.638290480547\n",
      "    - -1201.324032643283\n",
      "    - -901.8528938343472\n",
      "    - -1314.6296972927523\n",
      "    - -1534.9074528228266\n",
      "    - -1399.3856598225848\n",
      "    - -1019.3542809207603\n",
      "    - -1097.8483768274064\n",
      "    - -899.1510964654713\n",
      "    - -899.0988087612816\n",
      "    - -902.3312030464718\n",
      "    - -914.1184682647587\n",
      "    - -900.831879533462\n",
      "    - -1292.675749042493\n",
      "    - -970.5896948703155\n",
      "    - -1516.0214462237334\n",
      "    - -937.7600528081538\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19589075703929176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15118235495347007\n",
      "    mean_inference_ms: 0.9990357351368864\n",
      "    mean_raw_obs_processing_ms: 0.12315076304305189\n",
      "time_since_restore: 1784.7912273406982\n",
      "time_this_iter_s: 10.977157592773438\n",
      "time_total_s: 1784.7912273406982\n",
      "timers:\n",
      "  learn_throughput: 738.468\n",
      "  learn_time_ms: 5416.62\n",
      "  load_throughput: 14716856.14\n",
      "  load_time_ms: 0.272\n",
      "  training_iteration_time_ms: 12334.343\n",
      "  update_time_ms: 2.537\n",
      "timestamp: 1660565709\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 668000\n",
      "training_iteration: 167\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 672000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 672000\n",
      "  num_agent_steps_trained: 672000\n",
      "  num_env_steps_sampled: 672000\n",
      "  num_env_steps_trained: 672000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-15-21\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -849.7710229376382\n",
      "episode_reward_mean: -1166.6646861654588\n",
      "episode_reward_min: -1669.407431204889\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3360\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3198728561401367\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011098504066467285\n",
      "        model: {}\n",
      "        policy_loss: 0.015205315314233303\n",
      "        total_loss: 9.875011444091797\n",
      "        vf_explained_var: -0.01818823628127575\n",
      "        vf_loss: 9.846991539001465\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 672000\n",
      "  num_agent_steps_trained: 672000\n",
      "  num_env_steps_sampled: 672000\n",
      "  num_env_steps_trained: 672000\n",
      "iterations_since_restore: 168\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 672000\n",
      "num_agent_steps_trained: 672000\n",
      "num_env_steps_sampled: 672000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 672000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 65.20588235294117\n",
      "  ram_util_percent: 62.88823529411764\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19621247122821622\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1514117313440052\n",
      "  mean_inference_ms: 1.0008610286986501\n",
      "  mean_raw_obs_processing_ms: 0.12333854847391133\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -849.7710229376382\n",
      "  episode_reward_mean: -1166.6646861654588\n",
      "  episode_reward_min: -1669.407431204889\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -985.90084318293\n",
      "    - -889.5824330999475\n",
      "    - -1075.020025699622\n",
      "    - -1550.1512946763012\n",
      "    - -1196.1956778585761\n",
      "    - -1621.1976563971111\n",
      "    - -1544.8108916166657\n",
      "    - -1206.2841918998693\n",
      "    - -1242.7524043796298\n",
      "    - -1401.4288013077776\n",
      "    - -900.7757353205108\n",
      "    - -1468.2646857387758\n",
      "    - -985.4885935670027\n",
      "    - -1063.9480106249227\n",
      "    - -1478.40218103059\n",
      "    - -975.6549503392117\n",
      "    - -894.1575026162711\n",
      "    - -1669.407431204889\n",
      "    - -849.7710229376382\n",
      "    - -1143.334989339664\n",
      "    - -887.4343991910315\n",
      "    - -1324.654565300728\n",
      "    - -1017.3161361684279\n",
      "    - -1017.6407020792929\n",
      "    - -1102.9662465074475\n",
      "    - -887.7598060045799\n",
      "    - -975.0427020446863\n",
      "    - -1428.8658080988812\n",
      "    - -1364.3939613936593\n",
      "    - -1016.2461744996864\n",
      "    - -1454.1737903232558\n",
      "    - -880.6961806635356\n",
      "    - -914.877994448608\n",
      "    - -912.6170049516762\n",
      "    - -1422.1722838606802\n",
      "    - -1483.34530382304\n",
      "    - -1309.9434463237872\n",
      "    - -1083.6889282862066\n",
      "    - -954.7418698689568\n",
      "    - -1212.766318238323\n",
      "    - -944.0032418682725\n",
      "    - -917.7649728834677\n",
      "    - -909.1319022900439\n",
      "    - -1425.2536195347857\n",
      "    - -999.3164500926389\n",
      "    - -908.6647840631997\n",
      "    - -897.8454413252927\n",
      "    - -1607.2110760513897\n",
      "    - -1401.97174193559\n",
      "    - -1577.4803140363038\n",
      "    - -1373.3872876476537\n",
      "    - -1664.674163793421\n",
      "    - -1067.6821085053994\n",
      "    - -1009.6033996460495\n",
      "    - -1134.322780086397\n",
      "    - -1398.4966677334648\n",
      "    - -890.3994487546493\n",
      "    - -963.5463398658383\n",
      "    - -1619.6809611124431\n",
      "    - -894.3809690147264\n",
      "    - -1431.3464197248366\n",
      "    - -888.0425681024795\n",
      "    - -1284.5208096554807\n",
      "    - -1169.638290480547\n",
      "    - -1201.324032643283\n",
      "    - -901.8528938343472\n",
      "    - -1314.6296972927523\n",
      "    - -1534.9074528228266\n",
      "    - -1399.3856598225848\n",
      "    - -1019.3542809207603\n",
      "    - -1097.8483768274064\n",
      "    - -899.1510964654713\n",
      "    - -899.0988087612816\n",
      "    - -902.3312030464718\n",
      "    - -914.1184682647587\n",
      "    - -900.831879533462\n",
      "    - -1292.675749042493\n",
      "    - -970.5896948703155\n",
      "    - -1516.0214462237334\n",
      "    - -937.7600528081538\n",
      "    - -1347.6015680974078\n",
      "    - -983.2330020706013\n",
      "    - -1349.7177063396246\n",
      "    - -1598.5965631384443\n",
      "    - -1439.4632581179715\n",
      "    - -1190.941465721135\n",
      "    - -1217.6016178572243\n",
      "    - -1569.945855474616\n",
      "    - -1171.6623625888856\n",
      "    - -881.4720451733361\n",
      "    - -1208.6621012887665\n",
      "    - -1017.8406621173248\n",
      "    - -944.9706992785228\n",
      "    - -979.5618881001728\n",
      "    - -1405.1694485808305\n",
      "    - -898.4304111366955\n",
      "    - -1338.5201592739775\n",
      "    - -1079.0821924472655\n",
      "    - -1261.9518853951465\n",
      "    - -907.9242280490498\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19621247122821622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1514117313440052\n",
      "    mean_inference_ms: 1.0008610286986501\n",
      "    mean_raw_obs_processing_ms: 0.12333854847391133\n",
      "time_since_restore: 1796.5025806427002\n",
      "time_this_iter_s: 11.711353302001953\n",
      "time_total_s: 1796.5025806427002\n",
      "timers:\n",
      "  learn_throughput: 730.373\n",
      "  learn_time_ms: 5476.653\n",
      "  load_throughput: 14620667.538\n",
      "  load_time_ms: 0.274\n",
      "  training_iteration_time_ms: 12414.855\n",
      "  update_time_ms: 2.522\n",
      "timestamp: 1660565721\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 672000\n",
      "training_iteration: 168\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 676000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 676000\n",
      "  num_agent_steps_trained: 676000\n",
      "  num_env_steps_sampled: 676000\n",
      "  num_env_steps_trained: 676000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-15-31\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -880.6961806635356\n",
      "episode_reward_mean: -1153.708903579318\n",
      "episode_reward_min: -1664.674163793421\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3380\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.700700283050537\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00998338870704174\n",
      "        model: {}\n",
      "        policy_loss: 0.01164732500910759\n",
      "        total_loss: 9.858502388000488\n",
      "        vf_explained_var: -0.023878170177340508\n",
      "        vf_loss: 9.835328102111816\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 676000\n",
      "  num_agent_steps_trained: 676000\n",
      "  num_env_steps_sampled: 676000\n",
      "  num_env_steps_trained: 676000\n",
      "iterations_since_restore: 169\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 676000\n",
      "num_agent_steps_trained: 676000\n",
      "num_env_steps_sampled: 676000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 676000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.31333333333332\n",
      "  ram_util_percent: 62.87999999999999\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19633009302161653\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.151488589727837\n",
      "  mean_inference_ms: 1.0015318646511318\n",
      "  mean_raw_obs_processing_ms: 0.12339716698918414\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -880.6961806635356\n",
      "  episode_reward_mean: -1153.708903579318\n",
      "  episode_reward_min: -1664.674163793421\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -887.4343991910315\n",
      "    - -1324.654565300728\n",
      "    - -1017.3161361684279\n",
      "    - -1017.6407020792929\n",
      "    - -1102.9662465074475\n",
      "    - -887.7598060045799\n",
      "    - -975.0427020446863\n",
      "    - -1428.8658080988812\n",
      "    - -1364.3939613936593\n",
      "    - -1016.2461744996864\n",
      "    - -1454.1737903232558\n",
      "    - -880.6961806635356\n",
      "    - -914.877994448608\n",
      "    - -912.6170049516762\n",
      "    - -1422.1722838606802\n",
      "    - -1483.34530382304\n",
      "    - -1309.9434463237872\n",
      "    - -1083.6889282862066\n",
      "    - -954.7418698689568\n",
      "    - -1212.766318238323\n",
      "    - -944.0032418682725\n",
      "    - -917.7649728834677\n",
      "    - -909.1319022900439\n",
      "    - -1425.2536195347857\n",
      "    - -999.3164500926389\n",
      "    - -908.6647840631997\n",
      "    - -897.8454413252927\n",
      "    - -1607.2110760513897\n",
      "    - -1401.97174193559\n",
      "    - -1577.4803140363038\n",
      "    - -1373.3872876476537\n",
      "    - -1664.674163793421\n",
      "    - -1067.6821085053994\n",
      "    - -1009.6033996460495\n",
      "    - -1134.322780086397\n",
      "    - -1398.4966677334648\n",
      "    - -890.3994487546493\n",
      "    - -963.5463398658383\n",
      "    - -1619.6809611124431\n",
      "    - -894.3809690147264\n",
      "    - -1431.3464197248366\n",
      "    - -888.0425681024795\n",
      "    - -1284.5208096554807\n",
      "    - -1169.638290480547\n",
      "    - -1201.324032643283\n",
      "    - -901.8528938343472\n",
      "    - -1314.6296972927523\n",
      "    - -1534.9074528228266\n",
      "    - -1399.3856598225848\n",
      "    - -1019.3542809207603\n",
      "    - -1097.8483768274064\n",
      "    - -899.1510964654713\n",
      "    - -899.0988087612816\n",
      "    - -902.3312030464718\n",
      "    - -914.1184682647587\n",
      "    - -900.831879533462\n",
      "    - -1292.675749042493\n",
      "    - -970.5896948703155\n",
      "    - -1516.0214462237334\n",
      "    - -937.7600528081538\n",
      "    - -1347.6015680974078\n",
      "    - -983.2330020706013\n",
      "    - -1349.7177063396246\n",
      "    - -1598.5965631384443\n",
      "    - -1439.4632581179715\n",
      "    - -1190.941465721135\n",
      "    - -1217.6016178572243\n",
      "    - -1569.945855474616\n",
      "    - -1171.6623625888856\n",
      "    - -881.4720451733361\n",
      "    - -1208.6621012887665\n",
      "    - -1017.8406621173248\n",
      "    - -944.9706992785228\n",
      "    - -979.5618881001728\n",
      "    - -1405.1694485808305\n",
      "    - -898.4304111366955\n",
      "    - -1338.5201592739775\n",
      "    - -1079.0821924472655\n",
      "    - -1261.9518853951465\n",
      "    - -907.9242280490498\n",
      "    - -909.9598605793245\n",
      "    - -1509.9452118014083\n",
      "    - -1458.2894269266476\n",
      "    - -892.4799206956811\n",
      "    - -1004.9313104314014\n",
      "    - -894.9318632648065\n",
      "    - -1009.5207672779709\n",
      "    - -882.3094418541813\n",
      "    - -1133.4328359542733\n",
      "    - -1167.5039917482607\n",
      "    - -1520.8853541028761\n",
      "    - -1082.695723357434\n",
      "    - -1189.4402309665309\n",
      "    - -935.3945608183401\n",
      "    - -888.7101099963214\n",
      "    - -1508.65862375055\n",
      "    - -1298.1038988812802\n",
      "    - -895.5248280842716\n",
      "    - -1293.1080052321256\n",
      "    - -1371.1250985001552\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19633009302161653\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.151488589727837\n",
      "    mean_inference_ms: 1.0015318646511318\n",
      "    mean_raw_obs_processing_ms: 0.12339716698918414\n",
      "time_since_restore: 1807.1486189365387\n",
      "time_this_iter_s: 10.646038293838501\n",
      "time_total_s: 1807.1486189365387\n",
      "timers:\n",
      "  learn_throughput: 728.422\n",
      "  learn_time_ms: 5491.319\n",
      "  load_throughput: 14548400.971\n",
      "  load_time_ms: 0.275\n",
      "  training_iteration_time_ms: 12272.218\n",
      "  update_time_ms: 2.512\n",
      "timestamp: 1660565731\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 676000\n",
      "training_iteration: 169\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 680000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 680000\n",
      "  num_agent_steps_trained: 680000\n",
      "  num_env_steps_sampled: 680000\n",
      "  num_env_steps_trained: 680000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-15-43\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -879.1410295473336\n",
      "episode_reward_mean: -1161.823757564973\n",
      "episode_reward_min: -1726.3543692772334\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3400\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.136721134185791\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011873367242515087\n",
      "        model: {}\n",
      "        policy_loss: 0.011364174075424671\n",
      "        total_loss: 9.942910194396973\n",
      "        vf_explained_var: -0.02130097895860672\n",
      "        vf_loss: 9.91783618927002\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 680000\n",
      "  num_agent_steps_trained: 680000\n",
      "  num_env_steps_sampled: 680000\n",
      "  num_env_steps_trained: 680000\n",
      "iterations_since_restore: 170\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 680000\n",
      "num_agent_steps_trained: 680000\n",
      "num_env_steps_sampled: 680000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 680000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 66.45882352941177\n",
      "  ram_util_percent: 62.77058823529411\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1964154684373015\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15154369585916716\n",
      "  mean_inference_ms: 1.002026566744955\n",
      "  mean_raw_obs_processing_ms: 0.12343622449546882\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -879.1410295473336\n",
      "  episode_reward_mean: -1161.823757564973\n",
      "  episode_reward_min: -1726.3543692772334\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -944.0032418682725\n",
      "    - -917.7649728834677\n",
      "    - -909.1319022900439\n",
      "    - -1425.2536195347857\n",
      "    - -999.3164500926389\n",
      "    - -908.6647840631997\n",
      "    - -897.8454413252927\n",
      "    - -1607.2110760513897\n",
      "    - -1401.97174193559\n",
      "    - -1577.4803140363038\n",
      "    - -1373.3872876476537\n",
      "    - -1664.674163793421\n",
      "    - -1067.6821085053994\n",
      "    - -1009.6033996460495\n",
      "    - -1134.322780086397\n",
      "    - -1398.4966677334648\n",
      "    - -890.3994487546493\n",
      "    - -963.5463398658383\n",
      "    - -1619.6809611124431\n",
      "    - -894.3809690147264\n",
      "    - -1431.3464197248366\n",
      "    - -888.0425681024795\n",
      "    - -1284.5208096554807\n",
      "    - -1169.638290480547\n",
      "    - -1201.324032643283\n",
      "    - -901.8528938343472\n",
      "    - -1314.6296972927523\n",
      "    - -1534.9074528228266\n",
      "    - -1399.3856598225848\n",
      "    - -1019.3542809207603\n",
      "    - -1097.8483768274064\n",
      "    - -899.1510964654713\n",
      "    - -899.0988087612816\n",
      "    - -902.3312030464718\n",
      "    - -914.1184682647587\n",
      "    - -900.831879533462\n",
      "    - -1292.675749042493\n",
      "    - -970.5896948703155\n",
      "    - -1516.0214462237334\n",
      "    - -937.7600528081538\n",
      "    - -1347.6015680974078\n",
      "    - -983.2330020706013\n",
      "    - -1349.7177063396246\n",
      "    - -1598.5965631384443\n",
      "    - -1439.4632581179715\n",
      "    - -1190.941465721135\n",
      "    - -1217.6016178572243\n",
      "    - -1569.945855474616\n",
      "    - -1171.6623625888856\n",
      "    - -881.4720451733361\n",
      "    - -1208.6621012887665\n",
      "    - -1017.8406621173248\n",
      "    - -944.9706992785228\n",
      "    - -979.5618881001728\n",
      "    - -1405.1694485808305\n",
      "    - -898.4304111366955\n",
      "    - -1338.5201592739775\n",
      "    - -1079.0821924472655\n",
      "    - -1261.9518853951465\n",
      "    - -907.9242280490498\n",
      "    - -909.9598605793245\n",
      "    - -1509.9452118014083\n",
      "    - -1458.2894269266476\n",
      "    - -892.4799206956811\n",
      "    - -1004.9313104314014\n",
      "    - -894.9318632648065\n",
      "    - -1009.5207672779709\n",
      "    - -882.3094418541813\n",
      "    - -1133.4328359542733\n",
      "    - -1167.5039917482607\n",
      "    - -1520.8853541028761\n",
      "    - -1082.695723357434\n",
      "    - -1189.4402309665309\n",
      "    - -935.3945608183401\n",
      "    - -888.7101099963214\n",
      "    - -1508.65862375055\n",
      "    - -1298.1038988812802\n",
      "    - -895.5248280842716\n",
      "    - -1293.1080052321256\n",
      "    - -1371.1250985001552\n",
      "    - -1591.9282995715773\n",
      "    - -920.5511400453233\n",
      "    - -1540.9685226265995\n",
      "    - -1527.4911506893102\n",
      "    - -900.3878517912506\n",
      "    - -899.4470849131117\n",
      "    - -1245.3714675853564\n",
      "    - -1726.3543692772334\n",
      "    - -1174.1355258673634\n",
      "    - -1376.170598242476\n",
      "    - -1434.559691375917\n",
      "    - -931.1312935518695\n",
      "    - -879.1410295473336\n",
      "    - -988.2437439712216\n",
      "    - -954.940544398494\n",
      "    - -885.7910981573458\n",
      "    - -951.7661724285125\n",
      "    - -976.1129917079098\n",
      "    - -902.9879293332334\n",
      "    - -1655.3485155605272\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1964154684373015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15154369585916716\n",
      "    mean_inference_ms: 1.002026566744955\n",
      "    mean_raw_obs_processing_ms: 0.12343622449546882\n",
      "time_since_restore: 1818.581088066101\n",
      "time_this_iter_s: 11.432469129562378\n",
      "time_total_s: 1818.581088066101\n",
      "timers:\n",
      "  learn_throughput: 716.019\n",
      "  learn_time_ms: 5586.443\n",
      "  load_throughput: 14465611.312\n",
      "  load_time_ms: 0.277\n",
      "  training_iteration_time_ms: 12406.056\n",
      "  update_time_ms: 2.518\n",
      "timestamp: 1660565743\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 680000\n",
      "training_iteration: 170\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 684000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 684000\n",
      "  num_agent_steps_trained: 684000\n",
      "  num_env_steps_sampled: 684000\n",
      "  num_env_steps_trained: 684000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-15-54\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -871.9667791053793\n",
      "episode_reward_mean: -1139.7838327530108\n",
      "episode_reward_min: -1726.3543692772334\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3420\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.48892828822135925\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010236264206469059\n",
      "        model: {}\n",
      "        policy_loss: 0.013464946299791336\n",
      "        total_loss: 9.86811351776123\n",
      "        vf_explained_var: -0.029039133340120316\n",
      "        vf_loss: 9.842829704284668\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 684000\n",
      "  num_agent_steps_trained: 684000\n",
      "  num_env_steps_sampled: 684000\n",
      "  num_env_steps_trained: 684000\n",
      "iterations_since_restore: 171\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 684000\n",
      "num_agent_steps_trained: 684000\n",
      "num_env_steps_sampled: 684000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 684000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 63.49375\n",
      "  ram_util_percent: 62.849999999999994\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1964585564620893\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1515746350710279\n",
      "  mean_inference_ms: 1.002324605329107\n",
      "  mean_raw_obs_processing_ms: 0.12345696945916625\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -871.9667791053793\n",
      "  episode_reward_mean: -1139.7838327530108\n",
      "  episode_reward_min: -1726.3543692772334\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1431.3464197248366\n",
      "    - -888.0425681024795\n",
      "    - -1284.5208096554807\n",
      "    - -1169.638290480547\n",
      "    - -1201.324032643283\n",
      "    - -901.8528938343472\n",
      "    - -1314.6296972927523\n",
      "    - -1534.9074528228266\n",
      "    - -1399.3856598225848\n",
      "    - -1019.3542809207603\n",
      "    - -1097.8483768274064\n",
      "    - -899.1510964654713\n",
      "    - -899.0988087612816\n",
      "    - -902.3312030464718\n",
      "    - -914.1184682647587\n",
      "    - -900.831879533462\n",
      "    - -1292.675749042493\n",
      "    - -970.5896948703155\n",
      "    - -1516.0214462237334\n",
      "    - -937.7600528081538\n",
      "    - -1347.6015680974078\n",
      "    - -983.2330020706013\n",
      "    - -1349.7177063396246\n",
      "    - -1598.5965631384443\n",
      "    - -1439.4632581179715\n",
      "    - -1190.941465721135\n",
      "    - -1217.6016178572243\n",
      "    - -1569.945855474616\n",
      "    - -1171.6623625888856\n",
      "    - -881.4720451733361\n",
      "    - -1208.6621012887665\n",
      "    - -1017.8406621173248\n",
      "    - -944.9706992785228\n",
      "    - -979.5618881001728\n",
      "    - -1405.1694485808305\n",
      "    - -898.4304111366955\n",
      "    - -1338.5201592739775\n",
      "    - -1079.0821924472655\n",
      "    - -1261.9518853951465\n",
      "    - -907.9242280490498\n",
      "    - -909.9598605793245\n",
      "    - -1509.9452118014083\n",
      "    - -1458.2894269266476\n",
      "    - -892.4799206956811\n",
      "    - -1004.9313104314014\n",
      "    - -894.9318632648065\n",
      "    - -1009.5207672779709\n",
      "    - -882.3094418541813\n",
      "    - -1133.4328359542733\n",
      "    - -1167.5039917482607\n",
      "    - -1520.8853541028761\n",
      "    - -1082.695723357434\n",
      "    - -1189.4402309665309\n",
      "    - -935.3945608183401\n",
      "    - -888.7101099963214\n",
      "    - -1508.65862375055\n",
      "    - -1298.1038988812802\n",
      "    - -895.5248280842716\n",
      "    - -1293.1080052321256\n",
      "    - -1371.1250985001552\n",
      "    - -1591.9282995715773\n",
      "    - -920.5511400453233\n",
      "    - -1540.9685226265995\n",
      "    - -1527.4911506893102\n",
      "    - -900.3878517912506\n",
      "    - -899.4470849131117\n",
      "    - -1245.3714675853564\n",
      "    - -1726.3543692772334\n",
      "    - -1174.1355258673634\n",
      "    - -1376.170598242476\n",
      "    - -1434.559691375917\n",
      "    - -931.1312935518695\n",
      "    - -879.1410295473336\n",
      "    - -988.2437439712216\n",
      "    - -954.940544398494\n",
      "    - -885.7910981573458\n",
      "    - -951.7661724285125\n",
      "    - -976.1129917079098\n",
      "    - -902.9879293332334\n",
      "    - -1655.3485155605272\n",
      "    - -897.2905006594088\n",
      "    - -935.333900325207\n",
      "    - -1212.6065111858757\n",
      "    - -907.6010839646812\n",
      "    - -1067.8395022158115\n",
      "    - -991.0540495062078\n",
      "    - -1297.9534508852573\n",
      "    - -986.4038740648854\n",
      "    - -997.2583026543599\n",
      "    - -1659.306838043124\n",
      "    - -1034.640906774108\n",
      "    - -871.9667791053793\n",
      "    - -890.4782200062414\n",
      "    - -1196.575834916955\n",
      "    - -1245.5696582979222\n",
      "    - -889.4413439571243\n",
      "    - -906.3884119129688\n",
      "    - -1415.5765915786137\n",
      "    - -1074.7573699589339\n",
      "    - -922.7820590317664\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1964585564620893\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1515746350710279\n",
      "    mean_inference_ms: 1.002324605329107\n",
      "    mean_raw_obs_processing_ms: 0.12345696945916625\n",
      "time_since_restore: 1829.9486899375916\n",
      "time_this_iter_s: 11.367601871490479\n",
      "time_total_s: 1829.9486899375916\n",
      "timers:\n",
      "  learn_throughput: 708.889\n",
      "  learn_time_ms: 5642.633\n",
      "  load_throughput: 13506050.555\n",
      "  load_time_ms: 0.296\n",
      "  training_iteration_time_ms: 12547.34\n",
      "  update_time_ms: 2.482\n",
      "timestamp: 1660565754\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 684000\n",
      "training_iteration: 171\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 688000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 688000\n",
      "  num_agent_steps_trained: 688000\n",
      "  num_env_steps_sampled: 688000\n",
      "  num_env_steps_trained: 688000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-16-10\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -871.9667791053793\n",
      "episode_reward_mean: -1126.0580950314577\n",
      "episode_reward_min: -1726.3543692772334\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3440\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3735780715942383\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007822989486157894\n",
      "        model: {}\n",
      "        policy_loss: 0.011214306578040123\n",
      "        total_loss: 9.868380546569824\n",
      "        vf_explained_var: -0.020162176340818405\n",
      "        vf_loss: 9.84813404083252\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 688000\n",
      "  num_agent_steps_trained: 688000\n",
      "  num_env_steps_sampled: 688000\n",
      "  num_env_steps_trained: 688000\n",
      "iterations_since_restore: 172\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 688000\n",
      "num_agent_steps_trained: 688000\n",
      "num_env_steps_sampled: 688000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 688000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 75.33181818181819\n",
      "  ram_util_percent: 63.181818181818194\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19654416655924833\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15163239092372685\n",
      "  mean_inference_ms: 1.0028021955291326\n",
      "  mean_raw_obs_processing_ms: 0.1235001139066844\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -871.9667791053793\n",
      "  episode_reward_mean: -1126.0580950314577\n",
      "  episode_reward_min: -1726.3543692772334\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1347.6015680974078\n",
      "    - -983.2330020706013\n",
      "    - -1349.7177063396246\n",
      "    - -1598.5965631384443\n",
      "    - -1439.4632581179715\n",
      "    - -1190.941465721135\n",
      "    - -1217.6016178572243\n",
      "    - -1569.945855474616\n",
      "    - -1171.6623625888856\n",
      "    - -881.4720451733361\n",
      "    - -1208.6621012887665\n",
      "    - -1017.8406621173248\n",
      "    - -944.9706992785228\n",
      "    - -979.5618881001728\n",
      "    - -1405.1694485808305\n",
      "    - -898.4304111366955\n",
      "    - -1338.5201592739775\n",
      "    - -1079.0821924472655\n",
      "    - -1261.9518853951465\n",
      "    - -907.9242280490498\n",
      "    - -909.9598605793245\n",
      "    - -1509.9452118014083\n",
      "    - -1458.2894269266476\n",
      "    - -892.4799206956811\n",
      "    - -1004.9313104314014\n",
      "    - -894.9318632648065\n",
      "    - -1009.5207672779709\n",
      "    - -882.3094418541813\n",
      "    - -1133.4328359542733\n",
      "    - -1167.5039917482607\n",
      "    - -1520.8853541028761\n",
      "    - -1082.695723357434\n",
      "    - -1189.4402309665309\n",
      "    - -935.3945608183401\n",
      "    - -888.7101099963214\n",
      "    - -1508.65862375055\n",
      "    - -1298.1038988812802\n",
      "    - -895.5248280842716\n",
      "    - -1293.1080052321256\n",
      "    - -1371.1250985001552\n",
      "    - -1591.9282995715773\n",
      "    - -920.5511400453233\n",
      "    - -1540.9685226265995\n",
      "    - -1527.4911506893102\n",
      "    - -900.3878517912506\n",
      "    - -899.4470849131117\n",
      "    - -1245.3714675853564\n",
      "    - -1726.3543692772334\n",
      "    - -1174.1355258673634\n",
      "    - -1376.170598242476\n",
      "    - -1434.559691375917\n",
      "    - -931.1312935518695\n",
      "    - -879.1410295473336\n",
      "    - -988.2437439712216\n",
      "    - -954.940544398494\n",
      "    - -885.7910981573458\n",
      "    - -951.7661724285125\n",
      "    - -976.1129917079098\n",
      "    - -902.9879293332334\n",
      "    - -1655.3485155605272\n",
      "    - -897.2905006594088\n",
      "    - -935.333900325207\n",
      "    - -1212.6065111858757\n",
      "    - -907.6010839646812\n",
      "    - -1067.8395022158115\n",
      "    - -991.0540495062078\n",
      "    - -1297.9534508852573\n",
      "    - -986.4038740648854\n",
      "    - -997.2583026543599\n",
      "    - -1659.306838043124\n",
      "    - -1034.640906774108\n",
      "    - -871.9667791053793\n",
      "    - -890.4782200062414\n",
      "    - -1196.575834916955\n",
      "    - -1245.5696582979222\n",
      "    - -889.4413439571243\n",
      "    - -906.3884119129688\n",
      "    - -1415.5765915786137\n",
      "    - -1074.7573699589339\n",
      "    - -922.7820590317664\n",
      "    - -983.4484044303276\n",
      "    - -1157.5774077259352\n",
      "    - -985.9916315625759\n",
      "    - -1035.0357168351322\n",
      "    - -1314.2233585790348\n",
      "    - -939.5407245032012\n",
      "    - -1179.4081994545984\n",
      "    - -1005.0375619550944\n",
      "    - -988.6770716549935\n",
      "    - -896.3013908589726\n",
      "    - -1018.2691354503435\n",
      "    - -1191.331457120128\n",
      "    - -883.5790371218633\n",
      "    - -1360.2436109725336\n",
      "    - -892.4776309143772\n",
      "    - -978.2862848006808\n",
      "    - -894.699839792462\n",
      "    - -1602.5296758542781\n",
      "    - -893.8375266705262\n",
      "    - -902.3594427310679\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19654416655924833\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15163239092372685\n",
      "    mean_inference_ms: 1.0028021955291326\n",
      "    mean_raw_obs_processing_ms: 0.1235001139066844\n",
      "time_since_restore: 1845.4552311897278\n",
      "time_this_iter_s: 15.50654125213623\n",
      "time_total_s: 1845.4552311897278\n",
      "timers:\n",
      "  learn_throughput: 668.499\n",
      "  learn_time_ms: 5983.558\n",
      "  load_throughput: 13476757.973\n",
      "  load_time_ms: 0.297\n",
      "  training_iteration_time_ms: 13038.005\n",
      "  update_time_ms: 2.576\n",
      "timestamp: 1660565770\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 688000\n",
      "training_iteration: 172\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 692000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 692000\n",
      "  num_agent_steps_trained: 692000\n",
      "  num_env_steps_sampled: 692000\n",
      "  num_env_steps_trained: 692000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-16-22\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -871.9667791053793\n",
      "episode_reward_mean: -1123.885764484274\n",
      "episode_reward_min: -1726.3543692772334\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3460\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.136200189590454\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009263111278414726\n",
      "        model: {}\n",
      "        policy_loss: 0.010285058990120888\n",
      "        total_loss: 9.7884521484375\n",
      "        vf_explained_var: -0.013150221668183804\n",
      "        vf_loss: 9.767472267150879\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 692000\n",
      "  num_agent_steps_trained: 692000\n",
      "  num_env_steps_sampled: 692000\n",
      "  num_env_steps_trained: 692000\n",
      "iterations_since_restore: 173\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 692000\n",
      "num_agent_steps_trained: 692000\n",
      "num_env_steps_sampled: 692000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 692000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 66.87058823529412\n",
      "  ram_util_percent: 63.07647058823529\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19664406907254958\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1517042220702916\n",
      "  mean_inference_ms: 1.0033724188306283\n",
      "  mean_raw_obs_processing_ms: 0.12355457029055528\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -871.9667791053793\n",
      "  episode_reward_mean: -1123.885764484274\n",
      "  episode_reward_min: -1726.3543692772334\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -909.9598605793245\n",
      "    - -1509.9452118014083\n",
      "    - -1458.2894269266476\n",
      "    - -892.4799206956811\n",
      "    - -1004.9313104314014\n",
      "    - -894.9318632648065\n",
      "    - -1009.5207672779709\n",
      "    - -882.3094418541813\n",
      "    - -1133.4328359542733\n",
      "    - -1167.5039917482607\n",
      "    - -1520.8853541028761\n",
      "    - -1082.695723357434\n",
      "    - -1189.4402309665309\n",
      "    - -935.3945608183401\n",
      "    - -888.7101099963214\n",
      "    - -1508.65862375055\n",
      "    - -1298.1038988812802\n",
      "    - -895.5248280842716\n",
      "    - -1293.1080052321256\n",
      "    - -1371.1250985001552\n",
      "    - -1591.9282995715773\n",
      "    - -920.5511400453233\n",
      "    - -1540.9685226265995\n",
      "    - -1527.4911506893102\n",
      "    - -900.3878517912506\n",
      "    - -899.4470849131117\n",
      "    - -1245.3714675853564\n",
      "    - -1726.3543692772334\n",
      "    - -1174.1355258673634\n",
      "    - -1376.170598242476\n",
      "    - -1434.559691375917\n",
      "    - -931.1312935518695\n",
      "    - -879.1410295473336\n",
      "    - -988.2437439712216\n",
      "    - -954.940544398494\n",
      "    - -885.7910981573458\n",
      "    - -951.7661724285125\n",
      "    - -976.1129917079098\n",
      "    - -902.9879293332334\n",
      "    - -1655.3485155605272\n",
      "    - -897.2905006594088\n",
      "    - -935.333900325207\n",
      "    - -1212.6065111858757\n",
      "    - -907.6010839646812\n",
      "    - -1067.8395022158115\n",
      "    - -991.0540495062078\n",
      "    - -1297.9534508852573\n",
      "    - -986.4038740648854\n",
      "    - -997.2583026543599\n",
      "    - -1659.306838043124\n",
      "    - -1034.640906774108\n",
      "    - -871.9667791053793\n",
      "    - -890.4782200062414\n",
      "    - -1196.575834916955\n",
      "    - -1245.5696582979222\n",
      "    - -889.4413439571243\n",
      "    - -906.3884119129688\n",
      "    - -1415.5765915786137\n",
      "    - -1074.7573699589339\n",
      "    - -922.7820590317664\n",
      "    - -983.4484044303276\n",
      "    - -1157.5774077259352\n",
      "    - -985.9916315625759\n",
      "    - -1035.0357168351322\n",
      "    - -1314.2233585790348\n",
      "    - -939.5407245032012\n",
      "    - -1179.4081994545984\n",
      "    - -1005.0375619550944\n",
      "    - -988.6770716549935\n",
      "    - -896.3013908589726\n",
      "    - -1018.2691354503435\n",
      "    - -1191.331457120128\n",
      "    - -883.5790371218633\n",
      "    - -1360.2436109725336\n",
      "    - -892.4776309143772\n",
      "    - -978.2862848006808\n",
      "    - -894.699839792462\n",
      "    - -1602.5296758542781\n",
      "    - -893.8375266705262\n",
      "    - -902.3594427310679\n",
      "    - -951.8698101217686\n",
      "    - -1175.7399862228212\n",
      "    - -1296.560819130166\n",
      "    - -1720.6484386226907\n",
      "    - -1002.521521896135\n",
      "    - -892.6059573164436\n",
      "    - -1487.7151886651468\n",
      "    - -994.8702425755837\n",
      "    - -885.190601649913\n",
      "    - -1339.4453119061675\n",
      "    - -966.4715287050668\n",
      "    - -931.1693340146325\n",
      "    - -1088.0664161148593\n",
      "    - -1326.2204166835436\n",
      "    - -1070.5187257429582\n",
      "    - -1077.8873104404315\n",
      "    - -1290.5589323362328\n",
      "    - -898.1377412668003\n",
      "    - -1622.747800830892\n",
      "    - -1556.1699812863633\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19664406907254958\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1517042220702916\n",
      "    mean_inference_ms: 1.0033724188306283\n",
      "    mean_raw_obs_processing_ms: 0.12355457029055528\n",
      "time_since_restore: 1857.4536299705505\n",
      "time_this_iter_s: 11.998398780822754\n",
      "time_total_s: 1857.4536299705505\n",
      "timers:\n",
      "  learn_throughput: 687.58\n",
      "  learn_time_ms: 5817.503\n",
      "  load_throughput: 14519442.666\n",
      "  load_time_ms: 0.275\n",
      "  training_iteration_time_ms: 12902.162\n",
      "  update_time_ms: 2.52\n",
      "timestamp: 1660565782\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 692000\n",
      "training_iteration: 173\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 696000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 696000\n",
      "  num_agent_steps_trained: 696000\n",
      "  num_env_steps_sampled: 696000\n",
      "  num_env_steps_trained: 696000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-16-34\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -871.9667791053793\n",
      "episode_reward_mean: -1137.9309633223377\n",
      "episode_reward_min: -1726.3543692772334\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3480\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7831811904907227\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010588191449642181\n",
      "        model: {}\n",
      "        policy_loss: 0.014653673395514488\n",
      "        total_loss: 9.937875747680664\n",
      "        vf_explained_var: -0.01601042039692402\n",
      "        vf_loss: 9.910996437072754\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 696000\n",
      "  num_agent_steps_trained: 696000\n",
      "  num_env_steps_sampled: 696000\n",
      "  num_env_steps_trained: 696000\n",
      "iterations_since_restore: 174\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 696000\n",
      "num_agent_steps_trained: 696000\n",
      "num_env_steps_sampled: 696000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 696000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 66.6277777777778\n",
      "  ram_util_percent: 62.80555555555556\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19677976966100125\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15179355771042866\n",
      "  mean_inference_ms: 1.0040895625253694\n",
      "  mean_raw_obs_processing_ms: 0.12362488878516195\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -871.9667791053793\n",
      "  episode_reward_mean: -1137.9309633223377\n",
      "  episode_reward_min: -1726.3543692772334\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1591.9282995715773\n",
      "    - -920.5511400453233\n",
      "    - -1540.9685226265995\n",
      "    - -1527.4911506893102\n",
      "    - -900.3878517912506\n",
      "    - -899.4470849131117\n",
      "    - -1245.3714675853564\n",
      "    - -1726.3543692772334\n",
      "    - -1174.1355258673634\n",
      "    - -1376.170598242476\n",
      "    - -1434.559691375917\n",
      "    - -931.1312935518695\n",
      "    - -879.1410295473336\n",
      "    - -988.2437439712216\n",
      "    - -954.940544398494\n",
      "    - -885.7910981573458\n",
      "    - -951.7661724285125\n",
      "    - -976.1129917079098\n",
      "    - -902.9879293332334\n",
      "    - -1655.3485155605272\n",
      "    - -897.2905006594088\n",
      "    - -935.333900325207\n",
      "    - -1212.6065111858757\n",
      "    - -907.6010839646812\n",
      "    - -1067.8395022158115\n",
      "    - -991.0540495062078\n",
      "    - -1297.9534508852573\n",
      "    - -986.4038740648854\n",
      "    - -997.2583026543599\n",
      "    - -1659.306838043124\n",
      "    - -1034.640906774108\n",
      "    - -871.9667791053793\n",
      "    - -890.4782200062414\n",
      "    - -1196.575834916955\n",
      "    - -1245.5696582979222\n",
      "    - -889.4413439571243\n",
      "    - -906.3884119129688\n",
      "    - -1415.5765915786137\n",
      "    - -1074.7573699589339\n",
      "    - -922.7820590317664\n",
      "    - -983.4484044303276\n",
      "    - -1157.5774077259352\n",
      "    - -985.9916315625759\n",
      "    - -1035.0357168351322\n",
      "    - -1314.2233585790348\n",
      "    - -939.5407245032012\n",
      "    - -1179.4081994545984\n",
      "    - -1005.0375619550944\n",
      "    - -988.6770716549935\n",
      "    - -896.3013908589726\n",
      "    - -1018.2691354503435\n",
      "    - -1191.331457120128\n",
      "    - -883.5790371218633\n",
      "    - -1360.2436109725336\n",
      "    - -892.4776309143772\n",
      "    - -978.2862848006808\n",
      "    - -894.699839792462\n",
      "    - -1602.5296758542781\n",
      "    - -893.8375266705262\n",
      "    - -902.3594427310679\n",
      "    - -951.8698101217686\n",
      "    - -1175.7399862228212\n",
      "    - -1296.560819130166\n",
      "    - -1720.6484386226907\n",
      "    - -1002.521521896135\n",
      "    - -892.6059573164436\n",
      "    - -1487.7151886651468\n",
      "    - -994.8702425755837\n",
      "    - -885.190601649913\n",
      "    - -1339.4453119061675\n",
      "    - -966.4715287050668\n",
      "    - -931.1693340146325\n",
      "    - -1088.0664161148593\n",
      "    - -1326.2204166835436\n",
      "    - -1070.5187257429582\n",
      "    - -1077.8873104404315\n",
      "    - -1290.5589323362328\n",
      "    - -898.1377412668003\n",
      "    - -1622.747800830892\n",
      "    - -1556.1699812863633\n",
      "    - -1066.4326814802519\n",
      "    - -1295.7724385274075\n",
      "    - -1055.6080565799205\n",
      "    - -1560.3611597016925\n",
      "    - -1642.8090227986515\n",
      "    - -903.2325848941467\n",
      "    - -1207.6928812096114\n",
      "    - -974.0950836433069\n",
      "    - -1575.734153262387\n",
      "    - -1574.547963469326\n",
      "    - -1528.2453497146237\n",
      "    - -990.5477571614413\n",
      "    - -984.9056174715913\n",
      "    - -928.9086212035478\n",
      "    - -1411.274438490127\n",
      "    - -965.5660903683396\n",
      "    - -1293.1473871315175\n",
      "    - -989.9555109010518\n",
      "    - -1395.8856771219175\n",
      "    - -906.7484728993709\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19677976966100125\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15179355771042866\n",
      "    mean_inference_ms: 1.0040895625253694\n",
      "    mean_raw_obs_processing_ms: 0.12362488878516195\n",
      "time_since_restore: 1869.4302468299866\n",
      "time_this_iter_s: 11.976616859436035\n",
      "time_total_s: 1869.4302468299866\n",
      "timers:\n",
      "  learn_throughput: 732.264\n",
      "  learn_time_ms: 5462.51\n",
      "  load_throughput: 16329780.027\n",
      "  load_time_ms: 0.245\n",
      "  training_iteration_time_ms: 12086.682\n",
      "  update_time_ms: 2.486\n",
      "timestamp: 1660565794\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 696000\n",
      "training_iteration: 174\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 700000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 700000\n",
      "  num_agent_steps_trained: 700000\n",
      "  num_env_steps_sampled: 700000\n",
      "  num_env_steps_trained: 700000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-16-45\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -871.9667791053793\n",
      "episode_reward_mean: -1131.9274139929014\n",
      "episode_reward_min: -1720.6484386226907\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3500\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6735364198684692\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00991850532591343\n",
      "        model: {}\n",
      "        policy_loss: 0.008617333136498928\n",
      "        total_loss: 9.850173950195312\n",
      "        vf_explained_var: -0.02436152845621109\n",
      "        vf_loss: 9.830105781555176\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 700000\n",
      "  num_agent_steps_trained: 700000\n",
      "  num_env_steps_sampled: 700000\n",
      "  num_env_steps_trained: 700000\n",
      "iterations_since_restore: 175\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 700000\n",
      "num_agent_steps_trained: 700000\n",
      "num_env_steps_sampled: 700000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 700000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.306666666666665\n",
      "  ram_util_percent: 62.83999999999998\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19691153553776466\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15188227629206444\n",
      "  mean_inference_ms: 1.0047641498045254\n",
      "  mean_raw_obs_processing_ms: 0.12369854589969259\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -871.9667791053793\n",
      "  episode_reward_mean: -1131.9274139929014\n",
      "  episode_reward_min: -1720.6484386226907\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -897.2905006594088\n",
      "    - -935.333900325207\n",
      "    - -1212.6065111858757\n",
      "    - -907.6010839646812\n",
      "    - -1067.8395022158115\n",
      "    - -991.0540495062078\n",
      "    - -1297.9534508852573\n",
      "    - -986.4038740648854\n",
      "    - -997.2583026543599\n",
      "    - -1659.306838043124\n",
      "    - -1034.640906774108\n",
      "    - -871.9667791053793\n",
      "    - -890.4782200062414\n",
      "    - -1196.575834916955\n",
      "    - -1245.5696582979222\n",
      "    - -889.4413439571243\n",
      "    - -906.3884119129688\n",
      "    - -1415.5765915786137\n",
      "    - -1074.7573699589339\n",
      "    - -922.7820590317664\n",
      "    - -983.4484044303276\n",
      "    - -1157.5774077259352\n",
      "    - -985.9916315625759\n",
      "    - -1035.0357168351322\n",
      "    - -1314.2233585790348\n",
      "    - -939.5407245032012\n",
      "    - -1179.4081994545984\n",
      "    - -1005.0375619550944\n",
      "    - -988.6770716549935\n",
      "    - -896.3013908589726\n",
      "    - -1018.2691354503435\n",
      "    - -1191.331457120128\n",
      "    - -883.5790371218633\n",
      "    - -1360.2436109725336\n",
      "    - -892.4776309143772\n",
      "    - -978.2862848006808\n",
      "    - -894.699839792462\n",
      "    - -1602.5296758542781\n",
      "    - -893.8375266705262\n",
      "    - -902.3594427310679\n",
      "    - -951.8698101217686\n",
      "    - -1175.7399862228212\n",
      "    - -1296.560819130166\n",
      "    - -1720.6484386226907\n",
      "    - -1002.521521896135\n",
      "    - -892.6059573164436\n",
      "    - -1487.7151886651468\n",
      "    - -994.8702425755837\n",
      "    - -885.190601649913\n",
      "    - -1339.4453119061675\n",
      "    - -966.4715287050668\n",
      "    - -931.1693340146325\n",
      "    - -1088.0664161148593\n",
      "    - -1326.2204166835436\n",
      "    - -1070.5187257429582\n",
      "    - -1077.8873104404315\n",
      "    - -1290.5589323362328\n",
      "    - -898.1377412668003\n",
      "    - -1622.747800830892\n",
      "    - -1556.1699812863633\n",
      "    - -1066.4326814802519\n",
      "    - -1295.7724385274075\n",
      "    - -1055.6080565799205\n",
      "    - -1560.3611597016925\n",
      "    - -1642.8090227986515\n",
      "    - -903.2325848941467\n",
      "    - -1207.6928812096114\n",
      "    - -974.0950836433069\n",
      "    - -1575.734153262387\n",
      "    - -1574.547963469326\n",
      "    - -1528.2453497146237\n",
      "    - -990.5477571614413\n",
      "    - -984.9056174715913\n",
      "    - -928.9086212035478\n",
      "    - -1411.274438490127\n",
      "    - -965.5660903683396\n",
      "    - -1293.1473871315175\n",
      "    - -989.9555109010518\n",
      "    - -1395.8856771219175\n",
      "    - -906.7484728993709\n",
      "    - -1514.9953789123108\n",
      "    - -890.4238607283737\n",
      "    - -1084.834755599586\n",
      "    - -1074.9239216312726\n",
      "    - -1688.903018466783\n",
      "    - -1307.8482840871854\n",
      "    - -909.5535509240301\n",
      "    - -1451.9972519300406\n",
      "    - -987.5661991022578\n",
      "    - -921.8131749598405\n",
      "    - -900.1611859573119\n",
      "    - -894.7597660910528\n",
      "    - -1005.7671174954577\n",
      "    - -1185.4192521806003\n",
      "    - -971.8141021484267\n",
      "    - -961.1916179717563\n",
      "    - -1670.3772783044894\n",
      "    - -891.7482923392632\n",
      "    - -967.5184859802789\n",
      "    - -1580.8575928880193\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19691153553776466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15188227629206444\n",
      "    mean_inference_ms: 1.0047641498045254\n",
      "    mean_raw_obs_processing_ms: 0.12369854589969259\n",
      "time_since_restore: 1880.1091260910034\n",
      "time_this_iter_s: 10.678879261016846\n",
      "time_total_s: 1880.1091260910034\n",
      "timers:\n",
      "  learn_throughput: 737.769\n",
      "  learn_time_ms: 5421.753\n",
      "  load_throughput: 15589310.537\n",
      "  load_time_ms: 0.257\n",
      "  training_iteration_time_ms: 11952.981\n",
      "  update_time_ms: 2.486\n",
      "timestamp: 1660565805\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 700000\n",
      "training_iteration: 175\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 704000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 704000\n",
      "  num_agent_steps_trained: 704000\n",
      "  num_env_steps_sampled: 704000\n",
      "  num_env_steps_trained: 704000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-16-55\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -881.6760210759493\n",
      "episode_reward_mean: -1136.7519118305881\n",
      "episode_reward_min: -1769.917288347496\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3520\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9778765439987183\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013324909843504429\n",
      "        model: {}\n",
      "        policy_loss: 0.013152074068784714\n",
      "        total_loss: 9.858519554138184\n",
      "        vf_explained_var: -0.010304593481123447\n",
      "        vf_loss: 9.829981803894043\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 704000\n",
      "  num_agent_steps_trained: 704000\n",
      "  num_env_steps_sampled: 704000\n",
      "  num_env_steps_trained: 704000\n",
      "iterations_since_restore: 176\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 704000\n",
      "num_agent_steps_trained: 704000\n",
      "num_env_steps_sampled: 704000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 704000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 58.11333333333334\n",
      "  ram_util_percent: 62.87999999999999\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.197019474823891\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1519498809990076\n",
      "  mean_inference_ms: 1.0052808169069543\n",
      "  mean_raw_obs_processing_ms: 0.12375471878866373\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -881.6760210759493\n",
      "  episode_reward_mean: -1136.7519118305881\n",
      "  episode_reward_min: -1769.917288347496\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -983.4484044303276\n",
      "    - -1157.5774077259352\n",
      "    - -985.9916315625759\n",
      "    - -1035.0357168351322\n",
      "    - -1314.2233585790348\n",
      "    - -939.5407245032012\n",
      "    - -1179.4081994545984\n",
      "    - -1005.0375619550944\n",
      "    - -988.6770716549935\n",
      "    - -896.3013908589726\n",
      "    - -1018.2691354503435\n",
      "    - -1191.331457120128\n",
      "    - -883.5790371218633\n",
      "    - -1360.2436109725336\n",
      "    - -892.4776309143772\n",
      "    - -978.2862848006808\n",
      "    - -894.699839792462\n",
      "    - -1602.5296758542781\n",
      "    - -893.8375266705262\n",
      "    - -902.3594427310679\n",
      "    - -951.8698101217686\n",
      "    - -1175.7399862228212\n",
      "    - -1296.560819130166\n",
      "    - -1720.6484386226907\n",
      "    - -1002.521521896135\n",
      "    - -892.6059573164436\n",
      "    - -1487.7151886651468\n",
      "    - -994.8702425755837\n",
      "    - -885.190601649913\n",
      "    - -1339.4453119061675\n",
      "    - -966.4715287050668\n",
      "    - -931.1693340146325\n",
      "    - -1088.0664161148593\n",
      "    - -1326.2204166835436\n",
      "    - -1070.5187257429582\n",
      "    - -1077.8873104404315\n",
      "    - -1290.5589323362328\n",
      "    - -898.1377412668003\n",
      "    - -1622.747800830892\n",
      "    - -1556.1699812863633\n",
      "    - -1066.4326814802519\n",
      "    - -1295.7724385274075\n",
      "    - -1055.6080565799205\n",
      "    - -1560.3611597016925\n",
      "    - -1642.8090227986515\n",
      "    - -903.2325848941467\n",
      "    - -1207.6928812096114\n",
      "    - -974.0950836433069\n",
      "    - -1575.734153262387\n",
      "    - -1574.547963469326\n",
      "    - -1528.2453497146237\n",
      "    - -990.5477571614413\n",
      "    - -984.9056174715913\n",
      "    - -928.9086212035478\n",
      "    - -1411.274438490127\n",
      "    - -965.5660903683396\n",
      "    - -1293.1473871315175\n",
      "    - -989.9555109010518\n",
      "    - -1395.8856771219175\n",
      "    - -906.7484728993709\n",
      "    - -1514.9953789123108\n",
      "    - -890.4238607283737\n",
      "    - -1084.834755599586\n",
      "    - -1074.9239216312726\n",
      "    - -1688.903018466783\n",
      "    - -1307.8482840871854\n",
      "    - -909.5535509240301\n",
      "    - -1451.9972519300406\n",
      "    - -987.5661991022578\n",
      "    - -921.8131749598405\n",
      "    - -900.1611859573119\n",
      "    - -894.7597660910528\n",
      "    - -1005.7671174954577\n",
      "    - -1185.4192521806003\n",
      "    - -971.8141021484267\n",
      "    - -961.1916179717563\n",
      "    - -1670.3772783044894\n",
      "    - -891.7482923392632\n",
      "    - -967.5184859802789\n",
      "    - -1580.8575928880193\n",
      "    - -1769.917288347496\n",
      "    - -950.989450519108\n",
      "    - -881.6760210759493\n",
      "    - -894.6028113953142\n",
      "    - -1510.7178051137903\n",
      "    - -994.8256742210552\n",
      "    - -897.4177202805885\n",
      "    - -978.1988590024304\n",
      "    - -1462.5097065740486\n",
      "    - -988.6834064803021\n",
      "    - -1632.0620103192691\n",
      "    - -1040.6873843096935\n",
      "    - -1026.8171881936858\n",
      "    - -889.5986229708398\n",
      "    - -992.0029553116179\n",
      "    - -892.3900564217104\n",
      "    - -1018.7614627575747\n",
      "    - -988.8578325547635\n",
      "    - -1097.8835348779348\n",
      "    - -974.6751820863541\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.197019474823891\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1519498809990076\n",
      "    mean_inference_ms: 1.0052808169069543\n",
      "    mean_raw_obs_processing_ms: 0.12375471878866373\n",
      "time_since_restore: 1890.375876903534\n",
      "time_this_iter_s: 10.266750812530518\n",
      "time_total_s: 1890.375876903534\n",
      "timers:\n",
      "  learn_throughput: 756.981\n",
      "  learn_time_ms: 5284.149\n",
      "  load_throughput: 15763615.522\n",
      "  load_time_ms: 0.254\n",
      "  training_iteration_time_ms: 11648.834\n",
      "  update_time_ms: 2.46\n",
      "timestamp: 1660565815\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 704000\n",
      "training_iteration: 176\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 708000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 708000\n",
      "  num_agent_steps_trained: 708000\n",
      "  num_env_steps_sampled: 708000\n",
      "  num_env_steps_trained: 708000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-17-07\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -881.6760210759493\n",
      "episode_reward_mean: -1161.4846647908616\n",
      "episode_reward_min: -1769.917288347496\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3540\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3662397861480713\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014100540429353714\n",
      "        model: {}\n",
      "        policy_loss: 0.016806939616799355\n",
      "        total_loss: 9.840507507324219\n",
      "        vf_explained_var: -0.01731273904442787\n",
      "        vf_loss: 9.807419776916504\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 708000\n",
      "  num_agent_steps_trained: 708000\n",
      "  num_env_steps_sampled: 708000\n",
      "  num_env_steps_trained: 708000\n",
      "iterations_since_restore: 177\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 708000\n",
      "num_agent_steps_trained: 708000\n",
      "num_env_steps_sampled: 708000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 708000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 62.123529411764714\n",
      "  ram_util_percent: 62.85294117647059\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1970640275516807\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15197534767242954\n",
      "  mean_inference_ms: 1.0054922800381194\n",
      "  mean_raw_obs_processing_ms: 0.12378057015587497\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -881.6760210759493\n",
      "  episode_reward_mean: -1161.4846647908616\n",
      "  episode_reward_min: -1769.917288347496\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -951.8698101217686\n",
      "    - -1175.7399862228212\n",
      "    - -1296.560819130166\n",
      "    - -1720.6484386226907\n",
      "    - -1002.521521896135\n",
      "    - -892.6059573164436\n",
      "    - -1487.7151886651468\n",
      "    - -994.8702425755837\n",
      "    - -885.190601649913\n",
      "    - -1339.4453119061675\n",
      "    - -966.4715287050668\n",
      "    - -931.1693340146325\n",
      "    - -1088.0664161148593\n",
      "    - -1326.2204166835436\n",
      "    - -1070.5187257429582\n",
      "    - -1077.8873104404315\n",
      "    - -1290.5589323362328\n",
      "    - -898.1377412668003\n",
      "    - -1622.747800830892\n",
      "    - -1556.1699812863633\n",
      "    - -1066.4326814802519\n",
      "    - -1295.7724385274075\n",
      "    - -1055.6080565799205\n",
      "    - -1560.3611597016925\n",
      "    - -1642.8090227986515\n",
      "    - -903.2325848941467\n",
      "    - -1207.6928812096114\n",
      "    - -974.0950836433069\n",
      "    - -1575.734153262387\n",
      "    - -1574.547963469326\n",
      "    - -1528.2453497146237\n",
      "    - -990.5477571614413\n",
      "    - -984.9056174715913\n",
      "    - -928.9086212035478\n",
      "    - -1411.274438490127\n",
      "    - -965.5660903683396\n",
      "    - -1293.1473871315175\n",
      "    - -989.9555109010518\n",
      "    - -1395.8856771219175\n",
      "    - -906.7484728993709\n",
      "    - -1514.9953789123108\n",
      "    - -890.4238607283737\n",
      "    - -1084.834755599586\n",
      "    - -1074.9239216312726\n",
      "    - -1688.903018466783\n",
      "    - -1307.8482840871854\n",
      "    - -909.5535509240301\n",
      "    - -1451.9972519300406\n",
      "    - -987.5661991022578\n",
      "    - -921.8131749598405\n",
      "    - -900.1611859573119\n",
      "    - -894.7597660910528\n",
      "    - -1005.7671174954577\n",
      "    - -1185.4192521806003\n",
      "    - -971.8141021484267\n",
      "    - -961.1916179717563\n",
      "    - -1670.3772783044894\n",
      "    - -891.7482923392632\n",
      "    - -967.5184859802789\n",
      "    - -1580.8575928880193\n",
      "    - -1769.917288347496\n",
      "    - -950.989450519108\n",
      "    - -881.6760210759493\n",
      "    - -894.6028113953142\n",
      "    - -1510.7178051137903\n",
      "    - -994.8256742210552\n",
      "    - -897.4177202805885\n",
      "    - -978.1988590024304\n",
      "    - -1462.5097065740486\n",
      "    - -988.6834064803021\n",
      "    - -1632.0620103192691\n",
      "    - -1040.6873843096935\n",
      "    - -1026.8171881936858\n",
      "    - -889.5986229708398\n",
      "    - -992.0029553116179\n",
      "    - -892.3900564217104\n",
      "    - -1018.7614627575747\n",
      "    - -988.8578325547635\n",
      "    - -1097.8835348779348\n",
      "    - -974.6751820863541\n",
      "    - -1551.3798879355518\n",
      "    - -888.7628536694473\n",
      "    - -1603.6823448710547\n",
      "    - -1309.6211597555766\n",
      "    - -1123.1570195181619\n",
      "    - -1194.5639210936577\n",
      "    - -1138.1121221460319\n",
      "    - -903.1927521204183\n",
      "    - -1015.935444292215\n",
      "    - -1719.622688029878\n",
      "    - -1538.820569171935\n",
      "    - -909.6011444219562\n",
      "    - -1175.2435275778332\n",
      "    - -920.7205436322165\n",
      "    - -1367.3872737906986\n",
      "    - -891.4072296794245\n",
      "    - -980.1242785937562\n",
      "    - -1069.2118084584956\n",
      "    - -1384.7049359304608\n",
      "    - -890.8789003267025\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1970640275516807\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15197534767242954\n",
      "    mean_inference_ms: 1.0054922800381194\n",
      "    mean_raw_obs_processing_ms: 0.12378057015587497\n",
      "time_since_restore: 1902.30535197258\n",
      "time_this_iter_s: 11.92947506904602\n",
      "time_total_s: 1902.30535197258\n",
      "timers:\n",
      "  learn_throughput: 735.874\n",
      "  learn_time_ms: 5435.716\n",
      "  load_throughput: 16127286.36\n",
      "  load_time_ms: 0.248\n",
      "  training_iteration_time_ms: 11744.393\n",
      "  update_time_ms: 2.37\n",
      "timestamp: 1660565827\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 708000\n",
      "training_iteration: 177\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 712000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 712000\n",
      "  num_agent_steps_trained: 712000\n",
      "  num_env_steps_sampled: 712000\n",
      "  num_env_steps_trained: 712000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-17-17\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -876.6836818223773\n",
      "episode_reward_mean: -1157.4651139055838\n",
      "episode_reward_min: -1769.917288347496\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3560\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1129987239837646\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006643674802035093\n",
      "        model: {}\n",
      "        policy_loss: 0.008581428788602352\n",
      "        total_loss: 9.890568733215332\n",
      "        vf_explained_var: -0.024109510704874992\n",
      "        vf_loss: 9.87431526184082\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 712000\n",
      "  num_agent_steps_trained: 712000\n",
      "  num_env_steps_sampled: 712000\n",
      "  num_env_steps_trained: 712000\n",
      "iterations_since_restore: 178\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 712000\n",
      "num_agent_steps_trained: 712000\n",
      "num_env_steps_sampled: 712000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 712000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.25\n",
      "  ram_util_percent: 62.8642857142857\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19707025035977843\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15197136313444654\n",
      "  mean_inference_ms: 1.0054854600665817\n",
      "  mean_raw_obs_processing_ms: 0.12378351016186269\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -876.6836818223773\n",
      "  episode_reward_mean: -1157.4651139055838\n",
      "  episode_reward_min: -1769.917288347496\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1066.4326814802519\n",
      "    - -1295.7724385274075\n",
      "    - -1055.6080565799205\n",
      "    - -1560.3611597016925\n",
      "    - -1642.8090227986515\n",
      "    - -903.2325848941467\n",
      "    - -1207.6928812096114\n",
      "    - -974.0950836433069\n",
      "    - -1575.734153262387\n",
      "    - -1574.547963469326\n",
      "    - -1528.2453497146237\n",
      "    - -990.5477571614413\n",
      "    - -984.9056174715913\n",
      "    - -928.9086212035478\n",
      "    - -1411.274438490127\n",
      "    - -965.5660903683396\n",
      "    - -1293.1473871315175\n",
      "    - -989.9555109010518\n",
      "    - -1395.8856771219175\n",
      "    - -906.7484728993709\n",
      "    - -1514.9953789123108\n",
      "    - -890.4238607283737\n",
      "    - -1084.834755599586\n",
      "    - -1074.9239216312726\n",
      "    - -1688.903018466783\n",
      "    - -1307.8482840871854\n",
      "    - -909.5535509240301\n",
      "    - -1451.9972519300406\n",
      "    - -987.5661991022578\n",
      "    - -921.8131749598405\n",
      "    - -900.1611859573119\n",
      "    - -894.7597660910528\n",
      "    - -1005.7671174954577\n",
      "    - -1185.4192521806003\n",
      "    - -971.8141021484267\n",
      "    - -961.1916179717563\n",
      "    - -1670.3772783044894\n",
      "    - -891.7482923392632\n",
      "    - -967.5184859802789\n",
      "    - -1580.8575928880193\n",
      "    - -1769.917288347496\n",
      "    - -950.989450519108\n",
      "    - -881.6760210759493\n",
      "    - -894.6028113953142\n",
      "    - -1510.7178051137903\n",
      "    - -994.8256742210552\n",
      "    - -897.4177202805885\n",
      "    - -978.1988590024304\n",
      "    - -1462.5097065740486\n",
      "    - -988.6834064803021\n",
      "    - -1632.0620103192691\n",
      "    - -1040.6873843096935\n",
      "    - -1026.8171881936858\n",
      "    - -889.5986229708398\n",
      "    - -992.0029553116179\n",
      "    - -892.3900564217104\n",
      "    - -1018.7614627575747\n",
      "    - -988.8578325547635\n",
      "    - -1097.8835348779348\n",
      "    - -974.6751820863541\n",
      "    - -1551.3798879355518\n",
      "    - -888.7628536694473\n",
      "    - -1603.6823448710547\n",
      "    - -1309.6211597555766\n",
      "    - -1123.1570195181619\n",
      "    - -1194.5639210936577\n",
      "    - -1138.1121221460319\n",
      "    - -903.1927521204183\n",
      "    - -1015.935444292215\n",
      "    - -1719.622688029878\n",
      "    - -1538.820569171935\n",
      "    - -909.6011444219562\n",
      "    - -1175.2435275778332\n",
      "    - -920.7205436322165\n",
      "    - -1367.3872737906986\n",
      "    - -891.4072296794245\n",
      "    - -980.1242785937562\n",
      "    - -1069.2118084584956\n",
      "    - -1384.7049359304608\n",
      "    - -890.8789003267025\n",
      "    - -911.0553659624669\n",
      "    - -982.636884885624\n",
      "    - -895.8825915091253\n",
      "    - -893.51118633085\n",
      "    - -1705.1009105806304\n",
      "    - -989.4984967222413\n",
      "    - -1548.6915753799271\n",
      "    - -1486.576595977382\n",
      "    - -876.6836818223773\n",
      "    - -959.8315854207561\n",
      "    - -1232.0897296878047\n",
      "    - -1437.0590689760838\n",
      "    - -1481.4798506496415\n",
      "    - -878.1839847043017\n",
      "    - -986.038195876965\n",
      "    - -1276.7624352117948\n",
      "    - -1402.0577863158414\n",
      "    - -962.4446889049309\n",
      "    - -1006.5560129845795\n",
      "    - -1261.0203490974923\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19707025035977843\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15197136313444654\n",
      "    mean_inference_ms: 1.0054854600665817\n",
      "    mean_raw_obs_processing_ms: 0.12378351016186269\n",
      "time_since_restore: 1912.1577599048615\n",
      "time_this_iter_s: 9.852407932281494\n",
      "time_total_s: 1912.1577599048615\n",
      "timers:\n",
      "  learn_throughput: 753.323\n",
      "  learn_time_ms: 5309.804\n",
      "  load_throughput: 16326601.791\n",
      "  load_time_ms: 0.245\n",
      "  training_iteration_time_ms: 11558.512\n",
      "  update_time_ms: 2.392\n",
      "timestamp: 1660565837\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 712000\n",
      "training_iteration: 178\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 716000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 716000\n",
      "  num_agent_steps_trained: 716000\n",
      "  num_env_steps_sampled: 716000\n",
      "  num_env_steps_trained: 716000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-17-27\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -876.6836818223773\n",
      "episode_reward_mean: -1143.0814605845328\n",
      "episode_reward_min: -1769.917288347496\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3580\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4954675436019897\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01182795874774456\n",
      "        model: {}\n",
      "        policy_loss: 0.012659368105232716\n",
      "        total_loss: 9.862921714782715\n",
      "        vf_explained_var: -0.022479049861431122\n",
      "        vf_loss: 9.836605072021484\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 716000\n",
      "  num_agent_steps_trained: 716000\n",
      "  num_env_steps_sampled: 716000\n",
      "  num_env_steps_trained: 716000\n",
      "iterations_since_restore: 179\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 716000\n",
      "num_agent_steps_trained: 716000\n",
      "num_env_steps_sampled: 716000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 716000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.00714285714286\n",
      "  ram_util_percent: 62.764285714285734\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1970335059674759\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15194359791038303\n",
      "  mean_inference_ms: 1.0052812768249084\n",
      "  mean_raw_obs_processing_ms: 0.12376648983082449\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -876.6836818223773\n",
      "  episode_reward_mean: -1143.0814605845328\n",
      "  episode_reward_min: -1769.917288347496\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1514.9953789123108\n",
      "    - -890.4238607283737\n",
      "    - -1084.834755599586\n",
      "    - -1074.9239216312726\n",
      "    - -1688.903018466783\n",
      "    - -1307.8482840871854\n",
      "    - -909.5535509240301\n",
      "    - -1451.9972519300406\n",
      "    - -987.5661991022578\n",
      "    - -921.8131749598405\n",
      "    - -900.1611859573119\n",
      "    - -894.7597660910528\n",
      "    - -1005.7671174954577\n",
      "    - -1185.4192521806003\n",
      "    - -971.8141021484267\n",
      "    - -961.1916179717563\n",
      "    - -1670.3772783044894\n",
      "    - -891.7482923392632\n",
      "    - -967.5184859802789\n",
      "    - -1580.8575928880193\n",
      "    - -1769.917288347496\n",
      "    - -950.989450519108\n",
      "    - -881.6760210759493\n",
      "    - -894.6028113953142\n",
      "    - -1510.7178051137903\n",
      "    - -994.8256742210552\n",
      "    - -897.4177202805885\n",
      "    - -978.1988590024304\n",
      "    - -1462.5097065740486\n",
      "    - -988.6834064803021\n",
      "    - -1632.0620103192691\n",
      "    - -1040.6873843096935\n",
      "    - -1026.8171881936858\n",
      "    - -889.5986229708398\n",
      "    - -992.0029553116179\n",
      "    - -892.3900564217104\n",
      "    - -1018.7614627575747\n",
      "    - -988.8578325547635\n",
      "    - -1097.8835348779348\n",
      "    - -974.6751820863541\n",
      "    - -1551.3798879355518\n",
      "    - -888.7628536694473\n",
      "    - -1603.6823448710547\n",
      "    - -1309.6211597555766\n",
      "    - -1123.1570195181619\n",
      "    - -1194.5639210936577\n",
      "    - -1138.1121221460319\n",
      "    - -903.1927521204183\n",
      "    - -1015.935444292215\n",
      "    - -1719.622688029878\n",
      "    - -1538.820569171935\n",
      "    - -909.6011444219562\n",
      "    - -1175.2435275778332\n",
      "    - -920.7205436322165\n",
      "    - -1367.3872737906986\n",
      "    - -891.4072296794245\n",
      "    - -980.1242785937562\n",
      "    - -1069.2118084584956\n",
      "    - -1384.7049359304608\n",
      "    - -890.8789003267025\n",
      "    - -911.0553659624669\n",
      "    - -982.636884885624\n",
      "    - -895.8825915091253\n",
      "    - -893.51118633085\n",
      "    - -1705.1009105806304\n",
      "    - -989.4984967222413\n",
      "    - -1548.6915753799271\n",
      "    - -1486.576595977382\n",
      "    - -876.6836818223773\n",
      "    - -959.8315854207561\n",
      "    - -1232.0897296878047\n",
      "    - -1437.0590689760838\n",
      "    - -1481.4798506496415\n",
      "    - -878.1839847043017\n",
      "    - -986.038195876965\n",
      "    - -1276.7624352117948\n",
      "    - -1402.0577863158414\n",
      "    - -962.4446889049309\n",
      "    - -1006.5560129845795\n",
      "    - -1261.0203490974923\n",
      "    - -1555.272837425509\n",
      "    - -981.5350314494108\n",
      "    - -1174.8116521396598\n",
      "    - -890.2420140479837\n",
      "    - -1104.9245961036793\n",
      "    - -1628.706574240265\n",
      "    - -889.9051027948746\n",
      "    - -902.9511298649561\n",
      "    - -898.9982260805772\n",
      "    - -1129.2623747787136\n",
      "    - -1024.8384841159846\n",
      "    - -918.1717805340335\n",
      "    - -1589.129803445557\n",
      "    - -1429.0016220881591\n",
      "    - -1183.7326398898608\n",
      "    - -894.595399022608\n",
      "    - -1055.9601546105882\n",
      "    - -1010.6474905267963\n",
      "    - -1588.699222530013\n",
      "    - -961.7194802359164\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1970335059674759\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15194359791038303\n",
      "    mean_inference_ms: 1.0052812768249084\n",
      "    mean_raw_obs_processing_ms: 0.12376648983082449\n",
      "time_since_restore: 1922.1016869544983\n",
      "time_this_iter_s: 9.94392704963684\n",
      "time_total_s: 1922.1016869544983\n",
      "timers:\n",
      "  learn_throughput: 760.101\n",
      "  learn_time_ms: 5262.462\n",
      "  load_throughput: 16293304.846\n",
      "  load_time_ms: 0.245\n",
      "  training_iteration_time_ms: 11488.382\n",
      "  update_time_ms: 2.412\n",
      "timestamp: 1660565847\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 716000\n",
      "training_iteration: 179\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 720000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 720000\n",
      "  num_agent_steps_trained: 720000\n",
      "  num_env_steps_sampled: 720000\n",
      "  num_env_steps_trained: 720000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-17-37\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -876.6836818223773\n",
      "episode_reward_mean: -1154.527154657344\n",
      "episode_reward_min: -1769.917288347496\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3600\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.518615961074829\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012488359585404396\n",
      "        model: {}\n",
      "        policy_loss: 0.01749434880912304\n",
      "        total_loss: 9.938919067382812\n",
      "        vf_explained_var: -0.017194325104355812\n",
      "        vf_loss: 9.907005310058594\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 720000\n",
      "  num_agent_steps_trained: 720000\n",
      "  num_env_steps_sampled: 720000\n",
      "  num_env_steps_trained: 720000\n",
      "iterations_since_restore: 180\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 720000\n",
      "num_agent_steps_trained: 720000\n",
      "num_env_steps_sampled: 720000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 720000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.68571428571429\n",
      "  ram_util_percent: 62.77857142857141\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19698598249870844\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1519048793198301\n",
      "  mean_inference_ms: 1.0050098377268475\n",
      "  mean_raw_obs_processing_ms: 0.12373987047761646\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -876.6836818223773\n",
      "  episode_reward_mean: -1154.527154657344\n",
      "  episode_reward_min: -1769.917288347496\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1769.917288347496\n",
      "    - -950.989450519108\n",
      "    - -881.6760210759493\n",
      "    - -894.6028113953142\n",
      "    - -1510.7178051137903\n",
      "    - -994.8256742210552\n",
      "    - -897.4177202805885\n",
      "    - -978.1988590024304\n",
      "    - -1462.5097065740486\n",
      "    - -988.6834064803021\n",
      "    - -1632.0620103192691\n",
      "    - -1040.6873843096935\n",
      "    - -1026.8171881936858\n",
      "    - -889.5986229708398\n",
      "    - -992.0029553116179\n",
      "    - -892.3900564217104\n",
      "    - -1018.7614627575747\n",
      "    - -988.8578325547635\n",
      "    - -1097.8835348779348\n",
      "    - -974.6751820863541\n",
      "    - -1551.3798879355518\n",
      "    - -888.7628536694473\n",
      "    - -1603.6823448710547\n",
      "    - -1309.6211597555766\n",
      "    - -1123.1570195181619\n",
      "    - -1194.5639210936577\n",
      "    - -1138.1121221460319\n",
      "    - -903.1927521204183\n",
      "    - -1015.935444292215\n",
      "    - -1719.622688029878\n",
      "    - -1538.820569171935\n",
      "    - -909.6011444219562\n",
      "    - -1175.2435275778332\n",
      "    - -920.7205436322165\n",
      "    - -1367.3872737906986\n",
      "    - -891.4072296794245\n",
      "    - -980.1242785937562\n",
      "    - -1069.2118084584956\n",
      "    - -1384.7049359304608\n",
      "    - -890.8789003267025\n",
      "    - -911.0553659624669\n",
      "    - -982.636884885624\n",
      "    - -895.8825915091253\n",
      "    - -893.51118633085\n",
      "    - -1705.1009105806304\n",
      "    - -989.4984967222413\n",
      "    - -1548.6915753799271\n",
      "    - -1486.576595977382\n",
      "    - -876.6836818223773\n",
      "    - -959.8315854207561\n",
      "    - -1232.0897296878047\n",
      "    - -1437.0590689760838\n",
      "    - -1481.4798506496415\n",
      "    - -878.1839847043017\n",
      "    - -986.038195876965\n",
      "    - -1276.7624352117948\n",
      "    - -1402.0577863158414\n",
      "    - -962.4446889049309\n",
      "    - -1006.5560129845795\n",
      "    - -1261.0203490974923\n",
      "    - -1555.272837425509\n",
      "    - -981.5350314494108\n",
      "    - -1174.8116521396598\n",
      "    - -890.2420140479837\n",
      "    - -1104.9245961036793\n",
      "    - -1628.706574240265\n",
      "    - -889.9051027948746\n",
      "    - -902.9511298649561\n",
      "    - -898.9982260805772\n",
      "    - -1129.2623747787136\n",
      "    - -1024.8384841159846\n",
      "    - -918.1717805340335\n",
      "    - -1589.129803445557\n",
      "    - -1429.0016220881591\n",
      "    - -1183.7326398898608\n",
      "    - -894.595399022608\n",
      "    - -1055.9601546105882\n",
      "    - -1010.6474905267963\n",
      "    - -1588.699222530013\n",
      "    - -961.7194802359164\n",
      "    - -1041.3654195905656\n",
      "    - -1541.0541421781008\n",
      "    - -1414.7301997124775\n",
      "    - -1064.0601134828316\n",
      "    - -899.4167375279516\n",
      "    - -1159.3762583979842\n",
      "    - -983.8508389015138\n",
      "    - -1464.4763464632604\n",
      "    - -1041.0969577866917\n",
      "    - -1088.5885879594546\n",
      "    - -897.5019511840914\n",
      "    - -1330.3177825882954\n",
      "    - -1464.5963527332028\n",
      "    - -1504.2762503583308\n",
      "    - -1252.350261135775\n",
      "    - -1035.8026612675474\n",
      "    - -1373.164795819956\n",
      "    - -1580.6661696537597\n",
      "    - -984.3106441170795\n",
      "    - -886.041024120556\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19698598249870844\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1519048793198301\n",
      "    mean_inference_ms: 1.0050098377268475\n",
      "    mean_raw_obs_processing_ms: 0.12373987047761646\n",
      "time_since_restore: 1932.067096233368\n",
      "time_this_iter_s: 9.965409278869629\n",
      "time_total_s: 1932.067096233368\n",
      "timers:\n",
      "  learn_throughput: 774.529\n",
      "  learn_time_ms: 5164.428\n",
      "  load_throughput: 15914642.383\n",
      "  load_time_ms: 0.251\n",
      "  training_iteration_time_ms: 11341.628\n",
      "  update_time_ms: 2.428\n",
      "timestamp: 1660565857\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 720000\n",
      "training_iteration: 180\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 724000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 724000\n",
      "  num_agent_steps_trained: 724000\n",
      "  num_env_steps_sampled: 724000\n",
      "  num_env_steps_trained: 724000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-17-47\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -876.6836818223773\n",
      "episode_reward_mean: -1166.000874038039\n",
      "episode_reward_min: -1738.873566175719\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3620\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.761527419090271\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009568407200276852\n",
      "        model: {}\n",
      "        policy_loss: 0.011980582028627396\n",
      "        total_loss: 9.822604179382324\n",
      "        vf_explained_var: -0.02152136154472828\n",
      "        vf_loss: 9.799576759338379\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 724000\n",
      "  num_agent_steps_trained: 724000\n",
      "  num_env_steps_sampled: 724000\n",
      "  num_env_steps_trained: 724000\n",
      "iterations_since_restore: 181\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 724000\n",
      "num_agent_steps_trained: 724000\n",
      "num_env_steps_sampled: 724000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 724000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 58.16666666666667\n",
      "  ram_util_percent: 62.87333333333331\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19694076644323766\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1518708343441591\n",
      "  mean_inference_ms: 1.0047634770047222\n",
      "  mean_raw_obs_processing_ms: 0.12371724295823378\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -876.6836818223773\n",
      "  episode_reward_mean: -1166.000874038039\n",
      "  episode_reward_min: -1738.873566175719\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1551.3798879355518\n",
      "    - -888.7628536694473\n",
      "    - -1603.6823448710547\n",
      "    - -1309.6211597555766\n",
      "    - -1123.1570195181619\n",
      "    - -1194.5639210936577\n",
      "    - -1138.1121221460319\n",
      "    - -903.1927521204183\n",
      "    - -1015.935444292215\n",
      "    - -1719.622688029878\n",
      "    - -1538.820569171935\n",
      "    - -909.6011444219562\n",
      "    - -1175.2435275778332\n",
      "    - -920.7205436322165\n",
      "    - -1367.3872737906986\n",
      "    - -891.4072296794245\n",
      "    - -980.1242785937562\n",
      "    - -1069.2118084584956\n",
      "    - -1384.7049359304608\n",
      "    - -890.8789003267025\n",
      "    - -911.0553659624669\n",
      "    - -982.636884885624\n",
      "    - -895.8825915091253\n",
      "    - -893.51118633085\n",
      "    - -1705.1009105806304\n",
      "    - -989.4984967222413\n",
      "    - -1548.6915753799271\n",
      "    - -1486.576595977382\n",
      "    - -876.6836818223773\n",
      "    - -959.8315854207561\n",
      "    - -1232.0897296878047\n",
      "    - -1437.0590689760838\n",
      "    - -1481.4798506496415\n",
      "    - -878.1839847043017\n",
      "    - -986.038195876965\n",
      "    - -1276.7624352117948\n",
      "    - -1402.0577863158414\n",
      "    - -962.4446889049309\n",
      "    - -1006.5560129845795\n",
      "    - -1261.0203490974923\n",
      "    - -1555.272837425509\n",
      "    - -981.5350314494108\n",
      "    - -1174.8116521396598\n",
      "    - -890.2420140479837\n",
      "    - -1104.9245961036793\n",
      "    - -1628.706574240265\n",
      "    - -889.9051027948746\n",
      "    - -902.9511298649561\n",
      "    - -898.9982260805772\n",
      "    - -1129.2623747787136\n",
      "    - -1024.8384841159846\n",
      "    - -918.1717805340335\n",
      "    - -1589.129803445557\n",
      "    - -1429.0016220881591\n",
      "    - -1183.7326398898608\n",
      "    - -894.595399022608\n",
      "    - -1055.9601546105882\n",
      "    - -1010.6474905267963\n",
      "    - -1588.699222530013\n",
      "    - -961.7194802359164\n",
      "    - -1041.3654195905656\n",
      "    - -1541.0541421781008\n",
      "    - -1414.7301997124775\n",
      "    - -1064.0601134828316\n",
      "    - -899.4167375279516\n",
      "    - -1159.3762583979842\n",
      "    - -983.8508389015138\n",
      "    - -1464.4763464632604\n",
      "    - -1041.0969577866917\n",
      "    - -1088.5885879594546\n",
      "    - -897.5019511840914\n",
      "    - -1330.3177825882954\n",
      "    - -1464.5963527332028\n",
      "    - -1504.2762503583308\n",
      "    - -1252.350261135775\n",
      "    - -1035.8026612675474\n",
      "    - -1373.164795819956\n",
      "    - -1580.6661696537597\n",
      "    - -984.3106441170795\n",
      "    - -886.041024120556\n",
      "    - -980.6906794307532\n",
      "    - -1194.6035650060273\n",
      "    - -1182.860332615098\n",
      "    - -895.8055605830198\n",
      "    - -1738.873566175719\n",
      "    - -892.5986739704113\n",
      "    - -894.0446354937527\n",
      "    - -934.5439328781066\n",
      "    - -1321.9153426037797\n",
      "    - -1500.6757644963532\n",
      "    - -1466.5289253962042\n",
      "    - -890.2829868316003\n",
      "    - -1005.9145681742913\n",
      "    - -1515.7001225743938\n",
      "    - -1452.9071349846677\n",
      "    - -1154.6104721503098\n",
      "    - -992.3675148129596\n",
      "    - -1010.8231310448841\n",
      "    - -981.4721591557272\n",
      "    - -1023.4278425049804\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19694076644323766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1518708343441591\n",
      "    mean_inference_ms: 1.0047634770047222\n",
      "    mean_raw_obs_processing_ms: 0.12371724295823378\n",
      "time_since_restore: 1942.0685169696808\n",
      "time_this_iter_s: 10.001420736312866\n",
      "time_total_s: 1942.0685169696808\n",
      "timers:\n",
      "  learn_throughput: 785.612\n",
      "  learn_time_ms: 5091.57\n",
      "  load_throughput: 16755433.936\n",
      "  load_time_ms: 0.239\n",
      "  training_iteration_time_ms: 11205.035\n",
      "  update_time_ms: 2.461\n",
      "timestamp: 1660565867\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 724000\n",
      "training_iteration: 181\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 728000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 728000\n",
      "  num_agent_steps_trained: 728000\n",
      "  num_env_steps_sampled: 728000\n",
      "  num_env_steps_trained: 728000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-17-57\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -854.7549932217595\n",
      "episode_reward_mean: -1175.1890843019503\n",
      "episode_reward_min: -1738.873566175719\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3640\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3038508892059326\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012360538356006145\n",
      "        model: {}\n",
      "        policy_loss: 0.016738349571824074\n",
      "        total_loss: 9.911689758300781\n",
      "        vf_explained_var: -0.02067725919187069\n",
      "        vf_loss: 9.8806791305542\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 728000\n",
      "  num_agent_steps_trained: 728000\n",
      "  num_env_steps_sampled: 728000\n",
      "  num_env_steps_trained: 728000\n",
      "iterations_since_restore: 182\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 728000\n",
      "num_agent_steps_trained: 728000\n",
      "num_env_steps_sampled: 728000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 728000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.67142857142857\n",
      "  ram_util_percent: 62.72857142857145\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19689936614668269\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15183494951738127\n",
      "  mean_inference_ms: 1.0045184587133043\n",
      "  mean_raw_obs_processing_ms: 0.12369178352796624\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -854.7549932217595\n",
      "  episode_reward_mean: -1175.1890843019503\n",
      "  episode_reward_min: -1738.873566175719\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -911.0553659624669\n",
      "    - -982.636884885624\n",
      "    - -895.8825915091253\n",
      "    - -893.51118633085\n",
      "    - -1705.1009105806304\n",
      "    - -989.4984967222413\n",
      "    - -1548.6915753799271\n",
      "    - -1486.576595977382\n",
      "    - -876.6836818223773\n",
      "    - -959.8315854207561\n",
      "    - -1232.0897296878047\n",
      "    - -1437.0590689760838\n",
      "    - -1481.4798506496415\n",
      "    - -878.1839847043017\n",
      "    - -986.038195876965\n",
      "    - -1276.7624352117948\n",
      "    - -1402.0577863158414\n",
      "    - -962.4446889049309\n",
      "    - -1006.5560129845795\n",
      "    - -1261.0203490974923\n",
      "    - -1555.272837425509\n",
      "    - -981.5350314494108\n",
      "    - -1174.8116521396598\n",
      "    - -890.2420140479837\n",
      "    - -1104.9245961036793\n",
      "    - -1628.706574240265\n",
      "    - -889.9051027948746\n",
      "    - -902.9511298649561\n",
      "    - -898.9982260805772\n",
      "    - -1129.2623747787136\n",
      "    - -1024.8384841159846\n",
      "    - -918.1717805340335\n",
      "    - -1589.129803445557\n",
      "    - -1429.0016220881591\n",
      "    - -1183.7326398898608\n",
      "    - -894.595399022608\n",
      "    - -1055.9601546105882\n",
      "    - -1010.6474905267963\n",
      "    - -1588.699222530013\n",
      "    - -961.7194802359164\n",
      "    - -1041.3654195905656\n",
      "    - -1541.0541421781008\n",
      "    - -1414.7301997124775\n",
      "    - -1064.0601134828316\n",
      "    - -899.4167375279516\n",
      "    - -1159.3762583979842\n",
      "    - -983.8508389015138\n",
      "    - -1464.4763464632604\n",
      "    - -1041.0969577866917\n",
      "    - -1088.5885879594546\n",
      "    - -897.5019511840914\n",
      "    - -1330.3177825882954\n",
      "    - -1464.5963527332028\n",
      "    - -1504.2762503583308\n",
      "    - -1252.350261135775\n",
      "    - -1035.8026612675474\n",
      "    - -1373.164795819956\n",
      "    - -1580.6661696537597\n",
      "    - -984.3106441170795\n",
      "    - -886.041024120556\n",
      "    - -980.6906794307532\n",
      "    - -1194.6035650060273\n",
      "    - -1182.860332615098\n",
      "    - -895.8055605830198\n",
      "    - -1738.873566175719\n",
      "    - -892.5986739704113\n",
      "    - -894.0446354937527\n",
      "    - -934.5439328781066\n",
      "    - -1321.9153426037797\n",
      "    - -1500.6757644963532\n",
      "    - -1466.5289253962042\n",
      "    - -890.2829868316003\n",
      "    - -1005.9145681742913\n",
      "    - -1515.7001225743938\n",
      "    - -1452.9071349846677\n",
      "    - -1154.6104721503098\n",
      "    - -992.3675148129596\n",
      "    - -1010.8231310448841\n",
      "    - -981.4721591557272\n",
      "    - -1023.4278425049804\n",
      "    - -854.7549932217595\n",
      "    - -983.5390778395612\n",
      "    - -901.1543313917937\n",
      "    - -1506.7461680125302\n",
      "    - -1397.0016322414976\n",
      "    - -1415.6152518488443\n",
      "    - -1241.4007733067422\n",
      "    - -1601.7613538269486\n",
      "    - -1087.794923668535\n",
      "    - -1386.0412971134963\n",
      "    - -1587.198722571958\n",
      "    - -1386.4972805713592\n",
      "    - -1102.4033663194714\n",
      "    - -1625.0398429247527\n",
      "    - -1471.3576815789515\n",
      "    - -994.7970839799568\n",
      "    - -1174.917323380171\n",
      "    - -892.147268468653\n",
      "    - -887.7088289989684\n",
      "    - -997.0742301406456\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19689936614668269\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15183494951738127\n",
      "    mean_inference_ms: 1.0045184587133043\n",
      "    mean_raw_obs_processing_ms: 0.12369178352796624\n",
      "time_since_restore: 1952.122466802597\n",
      "time_this_iter_s: 10.05394983291626\n",
      "time_total_s: 1952.122466802597\n",
      "timers:\n",
      "  learn_throughput: 850.927\n",
      "  learn_time_ms: 4700.757\n",
      "  load_throughput: 16893783.103\n",
      "  load_time_ms: 0.237\n",
      "  training_iteration_time_ms: 10660.131\n",
      "  update_time_ms: 2.322\n",
      "timestamp: 1660565877\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 728000\n",
      "training_iteration: 182\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 732000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 732000\n",
      "  num_agent_steps_trained: 732000\n",
      "  num_env_steps_sampled: 732000\n",
      "  num_env_steps_trained: 732000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-18-07\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -854.7549932217595\n",
      "episode_reward_mean: -1172.7388672986485\n",
      "episode_reward_min: -1738.873566175719\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3660\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1375858783721924\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014548973180353642\n",
      "        model: {}\n",
      "        policy_loss: 0.016299813985824585\n",
      "        total_loss: 9.861546516418457\n",
      "        vf_explained_var: -0.019255097955465317\n",
      "        vf_loss: 9.828449249267578\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 732000\n",
      "  num_agent_steps_trained: 732000\n",
      "  num_env_steps_sampled: 732000\n",
      "  num_env_steps_trained: 732000\n",
      "iterations_since_restore: 183\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 732000\n",
      "num_agent_steps_trained: 732000\n",
      "num_env_steps_sampled: 732000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 732000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.628571428571426\n",
      "  ram_util_percent: 62.78571428571428\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19685808202871488\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15180369360993337\n",
      "  mean_inference_ms: 1.0042903164455559\n",
      "  mean_raw_obs_processing_ms: 0.12367090605491113\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -854.7549932217595\n",
      "  episode_reward_mean: -1172.7388672986485\n",
      "  episode_reward_min: -1738.873566175719\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1555.272837425509\n",
      "    - -981.5350314494108\n",
      "    - -1174.8116521396598\n",
      "    - -890.2420140479837\n",
      "    - -1104.9245961036793\n",
      "    - -1628.706574240265\n",
      "    - -889.9051027948746\n",
      "    - -902.9511298649561\n",
      "    - -898.9982260805772\n",
      "    - -1129.2623747787136\n",
      "    - -1024.8384841159846\n",
      "    - -918.1717805340335\n",
      "    - -1589.129803445557\n",
      "    - -1429.0016220881591\n",
      "    - -1183.7326398898608\n",
      "    - -894.595399022608\n",
      "    - -1055.9601546105882\n",
      "    - -1010.6474905267963\n",
      "    - -1588.699222530013\n",
      "    - -961.7194802359164\n",
      "    - -1041.3654195905656\n",
      "    - -1541.0541421781008\n",
      "    - -1414.7301997124775\n",
      "    - -1064.0601134828316\n",
      "    - -899.4167375279516\n",
      "    - -1159.3762583979842\n",
      "    - -983.8508389015138\n",
      "    - -1464.4763464632604\n",
      "    - -1041.0969577866917\n",
      "    - -1088.5885879594546\n",
      "    - -897.5019511840914\n",
      "    - -1330.3177825882954\n",
      "    - -1464.5963527332028\n",
      "    - -1504.2762503583308\n",
      "    - -1252.350261135775\n",
      "    - -1035.8026612675474\n",
      "    - -1373.164795819956\n",
      "    - -1580.6661696537597\n",
      "    - -984.3106441170795\n",
      "    - -886.041024120556\n",
      "    - -980.6906794307532\n",
      "    - -1194.6035650060273\n",
      "    - -1182.860332615098\n",
      "    - -895.8055605830198\n",
      "    - -1738.873566175719\n",
      "    - -892.5986739704113\n",
      "    - -894.0446354937527\n",
      "    - -934.5439328781066\n",
      "    - -1321.9153426037797\n",
      "    - -1500.6757644963532\n",
      "    - -1466.5289253962042\n",
      "    - -890.2829868316003\n",
      "    - -1005.9145681742913\n",
      "    - -1515.7001225743938\n",
      "    - -1452.9071349846677\n",
      "    - -1154.6104721503098\n",
      "    - -992.3675148129596\n",
      "    - -1010.8231310448841\n",
      "    - -981.4721591557272\n",
      "    - -1023.4278425049804\n",
      "    - -854.7549932217595\n",
      "    - -983.5390778395612\n",
      "    - -901.1543313917937\n",
      "    - -1506.7461680125302\n",
      "    - -1397.0016322414976\n",
      "    - -1415.6152518488443\n",
      "    - -1241.4007733067422\n",
      "    - -1601.7613538269486\n",
      "    - -1087.794923668535\n",
      "    - -1386.0412971134963\n",
      "    - -1587.198722571958\n",
      "    - -1386.4972805713592\n",
      "    - -1102.4033663194714\n",
      "    - -1625.0398429247527\n",
      "    - -1471.3576815789515\n",
      "    - -994.7970839799568\n",
      "    - -1174.917323380171\n",
      "    - -892.147268468653\n",
      "    - -887.7088289989684\n",
      "    - -997.0742301406456\n",
      "    - -976.8344489170728\n",
      "    - -1604.9755757204475\n",
      "    - -1228.3521402171184\n",
      "    - -1573.1435626383425\n",
      "    - -1602.9547286883571\n",
      "    - -892.9112718378658\n",
      "    - -989.1429432025682\n",
      "    - -906.9719900169765\n",
      "    - -917.9837608808563\n",
      "    - -1635.6509283884698\n",
      "    - -976.668208751245\n",
      "    - -893.2813623547206\n",
      "    - -1381.9103466314675\n",
      "    - -1017.1966232452845\n",
      "    - -1007.1815632746213\n",
      "    - -905.5742439760962\n",
      "    - -978.7158267374989\n",
      "    - -919.0534707507776\n",
      "    - -1621.0405417463128\n",
      "    - -898.5957386945289\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19685808202871488\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15180369360993337\n",
      "    mean_inference_ms: 1.0042903164455559\n",
      "    mean_raw_obs_processing_ms: 0.12367090605491113\n",
      "time_since_restore: 1962.2148790359497\n",
      "time_this_iter_s: 10.092412233352661\n",
      "time_total_s: 1962.2148790359497\n",
      "timers:\n",
      "  learn_throughput: 867.702\n",
      "  learn_time_ms: 4609.879\n",
      "  load_throughput: 17018884.155\n",
      "  load_time_ms: 0.235\n",
      "  training_iteration_time_ms: 10469.982\n",
      "  update_time_ms: 2.25\n",
      "timestamp: 1660565887\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 732000\n",
      "training_iteration: 183\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 736000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 736000\n",
      "  num_agent_steps_trained: 736000\n",
      "  num_env_steps_sampled: 736000\n",
      "  num_env_steps_trained: 736000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-18-17\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -854.7549932217595\n",
      "episode_reward_mean: -1175.7358620189198\n",
      "episode_reward_min: -1738.873566175719\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3680\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9694817066192627\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012225920334458351\n",
      "        model: {}\n",
      "        policy_loss: 0.011909974738955498\n",
      "        total_loss: 9.796072006225586\n",
      "        vf_explained_var: -0.013653465546667576\n",
      "        vf_loss: 9.77004623413086\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 736000\n",
      "  num_agent_steps_trained: 736000\n",
      "  num_env_steps_sampled: 736000\n",
      "  num_env_steps_trained: 736000\n",
      "iterations_since_restore: 184\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 736000\n",
      "num_agent_steps_trained: 736000\n",
      "num_env_steps_sampled: 736000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 736000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.67999999999999\n",
      "  ram_util_percent: 62.77999999999999\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19681952834184321\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.151773660960759\n",
      "  mean_inference_ms: 1.004071769172452\n",
      "  mean_raw_obs_processing_ms: 0.12364867095223628\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -854.7549932217595\n",
      "  episode_reward_mean: -1175.7358620189198\n",
      "  episode_reward_min: -1738.873566175719\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1041.3654195905656\n",
      "    - -1541.0541421781008\n",
      "    - -1414.7301997124775\n",
      "    - -1064.0601134828316\n",
      "    - -899.4167375279516\n",
      "    - -1159.3762583979842\n",
      "    - -983.8508389015138\n",
      "    - -1464.4763464632604\n",
      "    - -1041.0969577866917\n",
      "    - -1088.5885879594546\n",
      "    - -897.5019511840914\n",
      "    - -1330.3177825882954\n",
      "    - -1464.5963527332028\n",
      "    - -1504.2762503583308\n",
      "    - -1252.350261135775\n",
      "    - -1035.8026612675474\n",
      "    - -1373.164795819956\n",
      "    - -1580.6661696537597\n",
      "    - -984.3106441170795\n",
      "    - -886.041024120556\n",
      "    - -980.6906794307532\n",
      "    - -1194.6035650060273\n",
      "    - -1182.860332615098\n",
      "    - -895.8055605830198\n",
      "    - -1738.873566175719\n",
      "    - -892.5986739704113\n",
      "    - -894.0446354937527\n",
      "    - -934.5439328781066\n",
      "    - -1321.9153426037797\n",
      "    - -1500.6757644963532\n",
      "    - -1466.5289253962042\n",
      "    - -890.2829868316003\n",
      "    - -1005.9145681742913\n",
      "    - -1515.7001225743938\n",
      "    - -1452.9071349846677\n",
      "    - -1154.6104721503098\n",
      "    - -992.3675148129596\n",
      "    - -1010.8231310448841\n",
      "    - -981.4721591557272\n",
      "    - -1023.4278425049804\n",
      "    - -854.7549932217595\n",
      "    - -983.5390778395612\n",
      "    - -901.1543313917937\n",
      "    - -1506.7461680125302\n",
      "    - -1397.0016322414976\n",
      "    - -1415.6152518488443\n",
      "    - -1241.4007733067422\n",
      "    - -1601.7613538269486\n",
      "    - -1087.794923668535\n",
      "    - -1386.0412971134963\n",
      "    - -1587.198722571958\n",
      "    - -1386.4972805713592\n",
      "    - -1102.4033663194714\n",
      "    - -1625.0398429247527\n",
      "    - -1471.3576815789515\n",
      "    - -994.7970839799568\n",
      "    - -1174.917323380171\n",
      "    - -892.147268468653\n",
      "    - -887.7088289989684\n",
      "    - -997.0742301406456\n",
      "    - -976.8344489170728\n",
      "    - -1604.9755757204475\n",
      "    - -1228.3521402171184\n",
      "    - -1573.1435626383425\n",
      "    - -1602.9547286883571\n",
      "    - -892.9112718378658\n",
      "    - -989.1429432025682\n",
      "    - -906.9719900169765\n",
      "    - -917.9837608808563\n",
      "    - -1635.6509283884698\n",
      "    - -976.668208751245\n",
      "    - -893.2813623547206\n",
      "    - -1381.9103466314675\n",
      "    - -1017.1966232452845\n",
      "    - -1007.1815632746213\n",
      "    - -905.5742439760962\n",
      "    - -978.7158267374989\n",
      "    - -919.0534707507776\n",
      "    - -1621.0405417463128\n",
      "    - -898.5957386945289\n",
      "    - -1162.649936725773\n",
      "    - -898.7684894684555\n",
      "    - -1169.2349378039921\n",
      "    - -1389.302433954774\n",
      "    - -893.5854716879325\n",
      "    - -1360.9310479608662\n",
      "    - -913.1409737497518\n",
      "    - -903.5321655497899\n",
      "    - -1078.3865584997468\n",
      "    - -1643.6307879123792\n",
      "    - -889.5092071131791\n",
      "    - -997.367314181235\n",
      "    - -976.8647740394529\n",
      "    - -879.5780163007772\n",
      "    - -891.9462441394002\n",
      "    - -1677.3458305733066\n",
      "    - -1675.8278950523616\n",
      "    - -1646.2348597936625\n",
      "    - -972.0912710844597\n",
      "    - -1092.876872360992\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19681952834184321\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.151773660960759\n",
      "    mean_inference_ms: 1.004071769172452\n",
      "    mean_raw_obs_processing_ms: 0.12364867095223628\n",
      "time_since_restore: 1972.4430131912231\n",
      "time_this_iter_s: 10.228134155273438\n",
      "time_total_s: 1972.4430131912231\n",
      "timers:\n",
      "  learn_throughput: 882.713\n",
      "  learn_time_ms: 4531.482\n",
      "  load_throughput: 17086481.312\n",
      "  load_time_ms: 0.234\n",
      "  training_iteration_time_ms: 10295.243\n",
      "  update_time_ms: 2.232\n",
      "timestamp: 1660565897\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 736000\n",
      "training_iteration: 184\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 740000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 740000\n",
      "  num_agent_steps_trained: 740000\n",
      "  num_env_steps_sampled: 740000\n",
      "  num_env_steps_trained: 740000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-18-32\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -854.7549932217595\n",
      "episode_reward_mean: -1170.910801794701\n",
      "episode_reward_min: -1738.873566175719\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3700\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.2639658451080322\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006387545261532068\n",
      "        model: {}\n",
      "        policy_loss: 0.011384625919163227\n",
      "        total_loss: 9.80850887298584\n",
      "        vf_explained_var: -0.024015894159674644\n",
      "        vf_loss: 9.789749145507812\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 740000\n",
      "  num_agent_steps_trained: 740000\n",
      "  num_env_steps_sampled: 740000\n",
      "  num_env_steps_trained: 740000\n",
      "iterations_since_restore: 185\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 740000\n",
      "num_agent_steps_trained: 740000\n",
      "num_env_steps_sampled: 740000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 740000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 68.70476190476191\n",
      "  ram_util_percent: 67.59999999999998\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19687862387924973\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15181725817481384\n",
      "  mean_inference_ms: 1.0046536836342224\n",
      "  mean_raw_obs_processing_ms: 0.1236930682098312\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -854.7549932217595\n",
      "  episode_reward_mean: -1170.910801794701\n",
      "  episode_reward_min: -1738.873566175719\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -980.6906794307532\n",
      "    - -1194.6035650060273\n",
      "    - -1182.860332615098\n",
      "    - -895.8055605830198\n",
      "    - -1738.873566175719\n",
      "    - -892.5986739704113\n",
      "    - -894.0446354937527\n",
      "    - -934.5439328781066\n",
      "    - -1321.9153426037797\n",
      "    - -1500.6757644963532\n",
      "    - -1466.5289253962042\n",
      "    - -890.2829868316003\n",
      "    - -1005.9145681742913\n",
      "    - -1515.7001225743938\n",
      "    - -1452.9071349846677\n",
      "    - -1154.6104721503098\n",
      "    - -992.3675148129596\n",
      "    - -1010.8231310448841\n",
      "    - -981.4721591557272\n",
      "    - -1023.4278425049804\n",
      "    - -854.7549932217595\n",
      "    - -983.5390778395612\n",
      "    - -901.1543313917937\n",
      "    - -1506.7461680125302\n",
      "    - -1397.0016322414976\n",
      "    - -1415.6152518488443\n",
      "    - -1241.4007733067422\n",
      "    - -1601.7613538269486\n",
      "    - -1087.794923668535\n",
      "    - -1386.0412971134963\n",
      "    - -1587.198722571958\n",
      "    - -1386.4972805713592\n",
      "    - -1102.4033663194714\n",
      "    - -1625.0398429247527\n",
      "    - -1471.3576815789515\n",
      "    - -994.7970839799568\n",
      "    - -1174.917323380171\n",
      "    - -892.147268468653\n",
      "    - -887.7088289989684\n",
      "    - -997.0742301406456\n",
      "    - -976.8344489170728\n",
      "    - -1604.9755757204475\n",
      "    - -1228.3521402171184\n",
      "    - -1573.1435626383425\n",
      "    - -1602.9547286883571\n",
      "    - -892.9112718378658\n",
      "    - -989.1429432025682\n",
      "    - -906.9719900169765\n",
      "    - -917.9837608808563\n",
      "    - -1635.6509283884698\n",
      "    - -976.668208751245\n",
      "    - -893.2813623547206\n",
      "    - -1381.9103466314675\n",
      "    - -1017.1966232452845\n",
      "    - -1007.1815632746213\n",
      "    - -905.5742439760962\n",
      "    - -978.7158267374989\n",
      "    - -919.0534707507776\n",
      "    - -1621.0405417463128\n",
      "    - -898.5957386945289\n",
      "    - -1162.649936725773\n",
      "    - -898.7684894684555\n",
      "    - -1169.2349378039921\n",
      "    - -1389.302433954774\n",
      "    - -893.5854716879325\n",
      "    - -1360.9310479608662\n",
      "    - -913.1409737497518\n",
      "    - -903.5321655497899\n",
      "    - -1078.3865584997468\n",
      "    - -1643.6307879123792\n",
      "    - -889.5092071131791\n",
      "    - -997.367314181235\n",
      "    - -976.8647740394529\n",
      "    - -879.5780163007772\n",
      "    - -891.9462441394002\n",
      "    - -1677.3458305733066\n",
      "    - -1675.8278950523616\n",
      "    - -1646.2348597936625\n",
      "    - -972.0912710844597\n",
      "    - -1092.876872360992\n",
      "    - -881.9298125502846\n",
      "    - -1085.4408323936339\n",
      "    - -1196.9878582227996\n",
      "    - -1192.7765906710433\n",
      "    - -977.9751611072953\n",
      "    - -1599.8825823251225\n",
      "    - -1546.9767828150923\n",
      "    - -895.4408186125494\n",
      "    - -978.6433745479337\n",
      "    - -1702.9920164527816\n",
      "    - -978.9762250064781\n",
      "    - -973.405910422875\n",
      "    - -1077.7026131921475\n",
      "    - -1218.7894935772356\n",
      "    - -1573.578828130713\n",
      "    - -991.8702922638819\n",
      "    - -889.7009125998907\n",
      "    - -1596.0359459560443\n",
      "    - -991.2026144264111\n",
      "    - -1174.2288072833344\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19687862387924973\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15181725817481384\n",
      "    mean_inference_ms: 1.0046536836342224\n",
      "    mean_raw_obs_processing_ms: 0.1236930682098312\n",
      "time_since_restore: 1987.2308812141418\n",
      "time_this_iter_s: 14.787868022918701\n",
      "time_total_s: 1987.2308812141418\n",
      "timers:\n",
      "  learn_throughput: 870.975\n",
      "  learn_time_ms: 4592.557\n",
      "  load_throughput: 16374405.622\n",
      "  load_time_ms: 0.244\n",
      "  training_iteration_time_ms: 10706.067\n",
      "  update_time_ms: 2.22\n",
      "timestamp: 1660565912\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 740000\n",
      "training_iteration: 185\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 744000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 744000\n",
      "  num_agent_steps_trained: 744000\n",
      "  num_env_steps_sampled: 744000\n",
      "  num_env_steps_trained: 744000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-18-47\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -854.7549932217595\n",
      "episode_reward_mean: -1182.1701830226364\n",
      "episode_reward_min: -1702.9920164527816\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3720\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.2249350547790527\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017852282151579857\n",
      "        model: {}\n",
      "        policy_loss: 0.01697038672864437\n",
      "        total_loss: 9.767714500427246\n",
      "        vf_explained_var: -0.012549045495688915\n",
      "        vf_loss: 9.730133056640625\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 744000\n",
      "  num_agent_steps_trained: 744000\n",
      "  num_env_steps_sampled: 744000\n",
      "  num_env_steps_trained: 744000\n",
      "iterations_since_restore: 186\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 744000\n",
      "num_agent_steps_trained: 744000\n",
      "num_env_steps_sampled: 744000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 744000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 45.135000000000005\n",
      "  ram_util_percent: 69.465\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19695881582631894\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15186996309226544\n",
      "  mean_inference_ms: 1.0053359957639447\n",
      "  mean_raw_obs_processing_ms: 0.12374587028875551\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -854.7549932217595\n",
      "  episode_reward_mean: -1182.1701830226364\n",
      "  episode_reward_min: -1702.9920164527816\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -854.7549932217595\n",
      "    - -983.5390778395612\n",
      "    - -901.1543313917937\n",
      "    - -1506.7461680125302\n",
      "    - -1397.0016322414976\n",
      "    - -1415.6152518488443\n",
      "    - -1241.4007733067422\n",
      "    - -1601.7613538269486\n",
      "    - -1087.794923668535\n",
      "    - -1386.0412971134963\n",
      "    - -1587.198722571958\n",
      "    - -1386.4972805713592\n",
      "    - -1102.4033663194714\n",
      "    - -1625.0398429247527\n",
      "    - -1471.3576815789515\n",
      "    - -994.7970839799568\n",
      "    - -1174.917323380171\n",
      "    - -892.147268468653\n",
      "    - -887.7088289989684\n",
      "    - -997.0742301406456\n",
      "    - -976.8344489170728\n",
      "    - -1604.9755757204475\n",
      "    - -1228.3521402171184\n",
      "    - -1573.1435626383425\n",
      "    - -1602.9547286883571\n",
      "    - -892.9112718378658\n",
      "    - -989.1429432025682\n",
      "    - -906.9719900169765\n",
      "    - -917.9837608808563\n",
      "    - -1635.6509283884698\n",
      "    - -976.668208751245\n",
      "    - -893.2813623547206\n",
      "    - -1381.9103466314675\n",
      "    - -1017.1966232452845\n",
      "    - -1007.1815632746213\n",
      "    - -905.5742439760962\n",
      "    - -978.7158267374989\n",
      "    - -919.0534707507776\n",
      "    - -1621.0405417463128\n",
      "    - -898.5957386945289\n",
      "    - -1162.649936725773\n",
      "    - -898.7684894684555\n",
      "    - -1169.2349378039921\n",
      "    - -1389.302433954774\n",
      "    - -893.5854716879325\n",
      "    - -1360.9310479608662\n",
      "    - -913.1409737497518\n",
      "    - -903.5321655497899\n",
      "    - -1078.3865584997468\n",
      "    - -1643.6307879123792\n",
      "    - -889.5092071131791\n",
      "    - -997.367314181235\n",
      "    - -976.8647740394529\n",
      "    - -879.5780163007772\n",
      "    - -891.9462441394002\n",
      "    - -1677.3458305733066\n",
      "    - -1675.8278950523616\n",
      "    - -1646.2348597936625\n",
      "    - -972.0912710844597\n",
      "    - -1092.876872360992\n",
      "    - -881.9298125502846\n",
      "    - -1085.4408323936339\n",
      "    - -1196.9878582227996\n",
      "    - -1192.7765906710433\n",
      "    - -977.9751611072953\n",
      "    - -1599.8825823251225\n",
      "    - -1546.9767828150923\n",
      "    - -895.4408186125494\n",
      "    - -978.6433745479337\n",
      "    - -1702.9920164527816\n",
      "    - -978.9762250064781\n",
      "    - -973.405910422875\n",
      "    - -1077.7026131921475\n",
      "    - -1218.7894935772356\n",
      "    - -1573.578828130713\n",
      "    - -991.8702922638819\n",
      "    - -889.7009125998907\n",
      "    - -1596.0359459560443\n",
      "    - -991.2026144264111\n",
      "    - -1174.2288072833344\n",
      "    - -1058.8583092245447\n",
      "    - -1336.1333351790342\n",
      "    - -994.9121802443835\n",
      "    - -979.332724529968\n",
      "    - -918.9385782803904\n",
      "    - -1595.2210751335826\n",
      "    - -1067.2076908649065\n",
      "    - -1644.1987732160069\n",
      "    - -1481.8455631994943\n",
      "    - -1418.0063538374845\n",
      "    - -1039.019441015357\n",
      "    - -1093.2705387455162\n",
      "    - -1064.4086564635736\n",
      "    - -887.320912935638\n",
      "    - -922.3544025556113\n",
      "    - -896.8388844276989\n",
      "    - -1009.7972555785836\n",
      "    - -1473.0615614062408\n",
      "    - -1607.216056590205\n",
      "    - -1668.6427402483648\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19695881582631894\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15186996309226544\n",
      "    mean_inference_ms: 1.0053359957639447\n",
      "    mean_raw_obs_processing_ms: 0.12374587028875551\n",
      "time_since_restore: 2001.5058183670044\n",
      "time_this_iter_s: 14.274937152862549\n",
      "time_total_s: 2001.5058183670044\n",
      "timers:\n",
      "  learn_throughput: 811.649\n",
      "  learn_time_ms: 4928.239\n",
      "  load_throughput: 16253842.279\n",
      "  load_time_ms: 0.246\n",
      "  training_iteration_time_ms: 11106.077\n",
      "  update_time_ms: 2.228\n",
      "timestamp: 1660565927\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 744000\n",
      "training_iteration: 186\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 748000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 748000\n",
      "  num_agent_steps_trained: 748000\n",
      "  num_env_steps_sampled: 748000\n",
      "  num_env_steps_trained: 748000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-19-02\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -878.1553580557503\n",
      "episode_reward_mean: -1201.0462926197722\n",
      "episode_reward_min: -1760.3604291262138\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3740\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.667336940765381\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013187666423618793\n",
      "        model: {}\n",
      "        policy_loss: 0.014345923438668251\n",
      "        total_loss: 9.911601066589355\n",
      "        vf_explained_var: -0.022407371550798416\n",
      "        vf_loss: 9.88202953338623\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 748000\n",
      "  num_agent_steps_trained: 748000\n",
      "  num_env_steps_sampled: 748000\n",
      "  num_env_steps_trained: 748000\n",
      "iterations_since_restore: 187\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 748000\n",
      "num_agent_steps_trained: 748000\n",
      "num_env_steps_sampled: 748000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 748000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.75909090909091\n",
      "  ram_util_percent: 66.31818181818181\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1971558253170894\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15200513178899785\n",
      "  mean_inference_ms: 1.0066450632504682\n",
      "  mean_raw_obs_processing_ms: 0.12386621885529304\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -878.1553580557503\n",
      "  episode_reward_mean: -1201.0462926197722\n",
      "  episode_reward_min: -1760.3604291262138\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -976.8344489170728\n",
      "    - -1604.9755757204475\n",
      "    - -1228.3521402171184\n",
      "    - -1573.1435626383425\n",
      "    - -1602.9547286883571\n",
      "    - -892.9112718378658\n",
      "    - -989.1429432025682\n",
      "    - -906.9719900169765\n",
      "    - -917.9837608808563\n",
      "    - -1635.6509283884698\n",
      "    - -976.668208751245\n",
      "    - -893.2813623547206\n",
      "    - -1381.9103466314675\n",
      "    - -1017.1966232452845\n",
      "    - -1007.1815632746213\n",
      "    - -905.5742439760962\n",
      "    - -978.7158267374989\n",
      "    - -919.0534707507776\n",
      "    - -1621.0405417463128\n",
      "    - -898.5957386945289\n",
      "    - -1162.649936725773\n",
      "    - -898.7684894684555\n",
      "    - -1169.2349378039921\n",
      "    - -1389.302433954774\n",
      "    - -893.5854716879325\n",
      "    - -1360.9310479608662\n",
      "    - -913.1409737497518\n",
      "    - -903.5321655497899\n",
      "    - -1078.3865584997468\n",
      "    - -1643.6307879123792\n",
      "    - -889.5092071131791\n",
      "    - -997.367314181235\n",
      "    - -976.8647740394529\n",
      "    - -879.5780163007772\n",
      "    - -891.9462441394002\n",
      "    - -1677.3458305733066\n",
      "    - -1675.8278950523616\n",
      "    - -1646.2348597936625\n",
      "    - -972.0912710844597\n",
      "    - -1092.876872360992\n",
      "    - -881.9298125502846\n",
      "    - -1085.4408323936339\n",
      "    - -1196.9878582227996\n",
      "    - -1192.7765906710433\n",
      "    - -977.9751611072953\n",
      "    - -1599.8825823251225\n",
      "    - -1546.9767828150923\n",
      "    - -895.4408186125494\n",
      "    - -978.6433745479337\n",
      "    - -1702.9920164527816\n",
      "    - -978.9762250064781\n",
      "    - -973.405910422875\n",
      "    - -1077.7026131921475\n",
      "    - -1218.7894935772356\n",
      "    - -1573.578828130713\n",
      "    - -991.8702922638819\n",
      "    - -889.7009125998907\n",
      "    - -1596.0359459560443\n",
      "    - -991.2026144264111\n",
      "    - -1174.2288072833344\n",
      "    - -1058.8583092245447\n",
      "    - -1336.1333351790342\n",
      "    - -994.9121802443835\n",
      "    - -979.332724529968\n",
      "    - -918.9385782803904\n",
      "    - -1595.2210751335826\n",
      "    - -1067.2076908649065\n",
      "    - -1644.1987732160069\n",
      "    - -1481.8455631994943\n",
      "    - -1418.0063538374845\n",
      "    - -1039.019441015357\n",
      "    - -1093.2705387455162\n",
      "    - -1064.4086564635736\n",
      "    - -887.320912935638\n",
      "    - -922.3544025556113\n",
      "    - -896.8388844276989\n",
      "    - -1009.7972555785836\n",
      "    - -1473.0615614062408\n",
      "    - -1607.216056590205\n",
      "    - -1668.6427402483648\n",
      "    - -984.6425681387026\n",
      "    - -878.1553580557503\n",
      "    - -1176.01734706895\n",
      "    - -924.971424992065\n",
      "    - -1713.4174885169596\n",
      "    - -1346.1183582435947\n",
      "    - -1092.417470473636\n",
      "    - -1646.3948907266513\n",
      "    - -1760.3604291262138\n",
      "    - -1608.9600137641016\n",
      "    - -1645.3330356249298\n",
      "    - -1011.4701514928411\n",
      "    - -1517.0667804857871\n",
      "    - -1610.6352364548957\n",
      "    - -1029.0980691634763\n",
      "    - -1063.587960259489\n",
      "    - -1681.1366842542716\n",
      "    - -896.4440224717763\n",
      "    - -1069.8016930704953\n",
      "    - -1726.5334087355989\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1971558253170894\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15200513178899785\n",
      "    mean_inference_ms: 1.0066450632504682\n",
      "    mean_raw_obs_processing_ms: 0.12386621885529304\n",
      "time_since_restore: 2016.6009426116943\n",
      "time_this_iter_s: 15.095124244689941\n",
      "time_total_s: 2016.6009426116943\n",
      "timers:\n",
      "  learn_throughput: 815.166\n",
      "  learn_time_ms: 4906.974\n",
      "  load_throughput: 15762134.536\n",
      "  load_time_ms: 0.254\n",
      "  training_iteration_time_ms: 11422.531\n",
      "  update_time_ms: 2.217\n",
      "timestamp: 1660565942\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 748000\n",
      "training_iteration: 187\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 752000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 752000\n",
      "  num_agent_steps_trained: 752000\n",
      "  num_env_steps_sampled: 752000\n",
      "  num_env_steps_trained: 752000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-19-17\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -878.1553580557503\n",
      "episode_reward_mean: -1207.2411887312205\n",
      "episode_reward_min: -1760.3604291262138\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3760\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.542616605758667\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012558315880596638\n",
      "        model: {}\n",
      "        policy_loss: 0.01396174542605877\n",
      "        total_loss: 9.911466598510742\n",
      "        vf_explained_var: -0.021805616095662117\n",
      "        vf_loss: 9.88300609588623\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 752000\n",
      "  num_agent_steps_trained: 752000\n",
      "  num_env_steps_sampled: 752000\n",
      "  num_env_steps_trained: 752000\n",
      "iterations_since_restore: 188\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 752000\n",
      "num_agent_steps_trained: 752000\n",
      "num_env_steps_sampled: 752000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 752000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 67.12272727272727\n",
      "  ram_util_percent: 65.31363636363636\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.197376296590685\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1521522442150311\n",
      "  mean_inference_ms: 1.0080516847233105\n",
      "  mean_raw_obs_processing_ms: 0.1239959583967806\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -878.1553580557503\n",
      "  episode_reward_mean: -1207.2411887312205\n",
      "  episode_reward_min: -1760.3604291262138\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1162.649936725773\n",
      "    - -898.7684894684555\n",
      "    - -1169.2349378039921\n",
      "    - -1389.302433954774\n",
      "    - -893.5854716879325\n",
      "    - -1360.9310479608662\n",
      "    - -913.1409737497518\n",
      "    - -903.5321655497899\n",
      "    - -1078.3865584997468\n",
      "    - -1643.6307879123792\n",
      "    - -889.5092071131791\n",
      "    - -997.367314181235\n",
      "    - -976.8647740394529\n",
      "    - -879.5780163007772\n",
      "    - -891.9462441394002\n",
      "    - -1677.3458305733066\n",
      "    - -1675.8278950523616\n",
      "    - -1646.2348597936625\n",
      "    - -972.0912710844597\n",
      "    - -1092.876872360992\n",
      "    - -881.9298125502846\n",
      "    - -1085.4408323936339\n",
      "    - -1196.9878582227996\n",
      "    - -1192.7765906710433\n",
      "    - -977.9751611072953\n",
      "    - -1599.8825823251225\n",
      "    - -1546.9767828150923\n",
      "    - -895.4408186125494\n",
      "    - -978.6433745479337\n",
      "    - -1702.9920164527816\n",
      "    - -978.9762250064781\n",
      "    - -973.405910422875\n",
      "    - -1077.7026131921475\n",
      "    - -1218.7894935772356\n",
      "    - -1573.578828130713\n",
      "    - -991.8702922638819\n",
      "    - -889.7009125998907\n",
      "    - -1596.0359459560443\n",
      "    - -991.2026144264111\n",
      "    - -1174.2288072833344\n",
      "    - -1058.8583092245447\n",
      "    - -1336.1333351790342\n",
      "    - -994.9121802443835\n",
      "    - -979.332724529968\n",
      "    - -918.9385782803904\n",
      "    - -1595.2210751335826\n",
      "    - -1067.2076908649065\n",
      "    - -1644.1987732160069\n",
      "    - -1481.8455631994943\n",
      "    - -1418.0063538374845\n",
      "    - -1039.019441015357\n",
      "    - -1093.2705387455162\n",
      "    - -1064.4086564635736\n",
      "    - -887.320912935638\n",
      "    - -922.3544025556113\n",
      "    - -896.8388844276989\n",
      "    - -1009.7972555785836\n",
      "    - -1473.0615614062408\n",
      "    - -1607.216056590205\n",
      "    - -1668.6427402483648\n",
      "    - -984.6425681387026\n",
      "    - -878.1553580557503\n",
      "    - -1176.01734706895\n",
      "    - -924.971424992065\n",
      "    - -1713.4174885169596\n",
      "    - -1346.1183582435947\n",
      "    - -1092.417470473636\n",
      "    - -1646.3948907266513\n",
      "    - -1760.3604291262138\n",
      "    - -1608.9600137641016\n",
      "    - -1645.3330356249298\n",
      "    - -1011.4701514928411\n",
      "    - -1517.0667804857871\n",
      "    - -1610.6352364548957\n",
      "    - -1029.0980691634763\n",
      "    - -1063.587960259489\n",
      "    - -1681.1366842542716\n",
      "    - -896.4440224717763\n",
      "    - -1069.8016930704953\n",
      "    - -1726.5334087355989\n",
      "    - -901.7309638157002\n",
      "    - -905.9677869655596\n",
      "    - -1016.8173286612305\n",
      "    - -1016.5540579840091\n",
      "    - -1610.2445753722664\n",
      "    - -984.5020941832482\n",
      "    - -962.5742729344432\n",
      "    - -1570.4943250680928\n",
      "    - -928.6560235199195\n",
      "    - -1327.9709381716439\n",
      "    - -1531.602043053955\n",
      "    - -1516.3411438458766\n",
      "    - -1065.908246457656\n",
      "    - -893.597804361671\n",
      "    - -1062.1204871110765\n",
      "    - -1172.7888814024197\n",
      "    - -1504.2281839091158\n",
      "    - -1547.8809454064049\n",
      "    - -1039.980471662971\n",
      "    - -987.668313928197\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.197376296590685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1521522442150311\n",
      "    mean_inference_ms: 1.0080516847233105\n",
      "    mean_raw_obs_processing_ms: 0.1239959583967806\n",
      "time_since_restore: 2031.9580435752869\n",
      "time_this_iter_s: 15.35710096359253\n",
      "time_total_s: 2031.9580435752869\n",
      "timers:\n",
      "  learn_throughput: 742.036\n",
      "  learn_time_ms: 5390.573\n",
      "  load_throughput: 15737000.281\n",
      "  load_time_ms: 0.254\n",
      "  training_iteration_time_ms: 11972.099\n",
      "  update_time_ms: 2.309\n",
      "timestamp: 1660565957\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 752000\n",
      "training_iteration: 188\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 756000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 756000\n",
      "  num_agent_steps_trained: 756000\n",
      "  num_env_steps_sampled: 756000\n",
      "  num_env_steps_trained: 756000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-19-30\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -878.1553580557503\n",
      "episode_reward_mean: -1213.153974088461\n",
      "episode_reward_min: -1760.3604291262138\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3780\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.594369649887085\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012732268311083317\n",
      "        model: {}\n",
      "        policy_loss: 0.015384675934910774\n",
      "        total_loss: 9.815971374511719\n",
      "        vf_explained_var: -0.008387334644794464\n",
      "        vf_loss: 9.785886764526367\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 756000\n",
      "  num_agent_steps_trained: 756000\n",
      "  num_env_steps_sampled: 756000\n",
      "  num_env_steps_trained: 756000\n",
      "iterations_since_restore: 189\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 756000\n",
      "num_agent_steps_trained: 756000\n",
      "num_env_steps_sampled: 756000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 756000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 61.56315789473685\n",
      "  ram_util_percent: 65.14210526315792\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19763949572897466\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15232988214959092\n",
      "  mean_inference_ms: 1.0096875603027002\n",
      "  mean_raw_obs_processing_ms: 0.12415265242646141\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -878.1553580557503\n",
      "  episode_reward_mean: -1213.153974088461\n",
      "  episode_reward_min: -1760.3604291262138\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -881.9298125502846\n",
      "    - -1085.4408323936339\n",
      "    - -1196.9878582227996\n",
      "    - -1192.7765906710433\n",
      "    - -977.9751611072953\n",
      "    - -1599.8825823251225\n",
      "    - -1546.9767828150923\n",
      "    - -895.4408186125494\n",
      "    - -978.6433745479337\n",
      "    - -1702.9920164527816\n",
      "    - -978.9762250064781\n",
      "    - -973.405910422875\n",
      "    - -1077.7026131921475\n",
      "    - -1218.7894935772356\n",
      "    - -1573.578828130713\n",
      "    - -991.8702922638819\n",
      "    - -889.7009125998907\n",
      "    - -1596.0359459560443\n",
      "    - -991.2026144264111\n",
      "    - -1174.2288072833344\n",
      "    - -1058.8583092245447\n",
      "    - -1336.1333351790342\n",
      "    - -994.9121802443835\n",
      "    - -979.332724529968\n",
      "    - -918.9385782803904\n",
      "    - -1595.2210751335826\n",
      "    - -1067.2076908649065\n",
      "    - -1644.1987732160069\n",
      "    - -1481.8455631994943\n",
      "    - -1418.0063538374845\n",
      "    - -1039.019441015357\n",
      "    - -1093.2705387455162\n",
      "    - -1064.4086564635736\n",
      "    - -887.320912935638\n",
      "    - -922.3544025556113\n",
      "    - -896.8388844276989\n",
      "    - -1009.7972555785836\n",
      "    - -1473.0615614062408\n",
      "    - -1607.216056590205\n",
      "    - -1668.6427402483648\n",
      "    - -984.6425681387026\n",
      "    - -878.1553580557503\n",
      "    - -1176.01734706895\n",
      "    - -924.971424992065\n",
      "    - -1713.4174885169596\n",
      "    - -1346.1183582435947\n",
      "    - -1092.417470473636\n",
      "    - -1646.3948907266513\n",
      "    - -1760.3604291262138\n",
      "    - -1608.9600137641016\n",
      "    - -1645.3330356249298\n",
      "    - -1011.4701514928411\n",
      "    - -1517.0667804857871\n",
      "    - -1610.6352364548957\n",
      "    - -1029.0980691634763\n",
      "    - -1063.587960259489\n",
      "    - -1681.1366842542716\n",
      "    - -896.4440224717763\n",
      "    - -1069.8016930704953\n",
      "    - -1726.5334087355989\n",
      "    - -901.7309638157002\n",
      "    - -905.9677869655596\n",
      "    - -1016.8173286612305\n",
      "    - -1016.5540579840091\n",
      "    - -1610.2445753722664\n",
      "    - -984.5020941832482\n",
      "    - -962.5742729344432\n",
      "    - -1570.4943250680928\n",
      "    - -928.6560235199195\n",
      "    - -1327.9709381716439\n",
      "    - -1531.602043053955\n",
      "    - -1516.3411438458766\n",
      "    - -1065.908246457656\n",
      "    - -893.597804361671\n",
      "    - -1062.1204871110765\n",
      "    - -1172.7888814024197\n",
      "    - -1504.2281839091158\n",
      "    - -1547.8809454064049\n",
      "    - -1039.980471662971\n",
      "    - -987.668313928197\n",
      "    - -935.9901448044623\n",
      "    - -886.3718179784456\n",
      "    - -1181.0161681031514\n",
      "    - -1106.386430134326\n",
      "    - -993.3900694293094\n",
      "    - -1064.7079292828232\n",
      "    - -971.5294866851863\n",
      "    - -1688.8640263259576\n",
      "    - -1490.4302377645074\n",
      "    - -1220.5105172372384\n",
      "    - -893.9056998694575\n",
      "    - -1590.5402576220356\n",
      "    - -1513.398591755453\n",
      "    - -890.0747975604423\n",
      "    - -1095.2921232763622\n",
      "    - -1341.3520458015607\n",
      "    - -1196.3633168338051\n",
      "    - -1676.6663219015106\n",
      "    - -885.6112163140879\n",
      "    - -1081.6824249961662\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19763949572897466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15232988214959092\n",
      "    mean_inference_ms: 1.0096875603027002\n",
      "    mean_raw_obs_processing_ms: 0.12415265242646141\n",
      "time_since_restore: 2044.8742978572845\n",
      "time_this_iter_s: 12.91625428199768\n",
      "time_total_s: 2044.8742978572845\n",
      "timers:\n",
      "  learn_throughput: 720.086\n",
      "  learn_time_ms: 5554.892\n",
      "  load_throughput: 15719306.662\n",
      "  load_time_ms: 0.254\n",
      "  training_iteration_time_ms: 12269.208\n",
      "  update_time_ms: 2.425\n",
      "timestamp: 1660565970\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 756000\n",
      "training_iteration: 189\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 760000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 760000\n",
      "  num_agent_steps_trained: 760000\n",
      "  num_env_steps_sampled: 760000\n",
      "  num_env_steps_trained: 760000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-19-41\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -878.1553580557503\n",
      "episode_reward_mean: -1216.214722945986\n",
      "episode_reward_min: -1760.3604291262138\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3800\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.616877317428589\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008145858533680439\n",
      "        model: {}\n",
      "        policy_loss: 0.012161406688392162\n",
      "        total_loss: 9.829610824584961\n",
      "        vf_explained_var: -0.022229503840208054\n",
      "        vf_loss: 9.80804443359375\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 760000\n",
      "  num_agent_steps_trained: 760000\n",
      "  num_env_steps_sampled: 760000\n",
      "  num_env_steps_trained: 760000\n",
      "iterations_since_restore: 190\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 760000\n",
      "num_agent_steps_trained: 760000\n",
      "num_env_steps_sampled: 760000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 760000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 61.525000000000006\n",
      "  ram_util_percent: 65.09375\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19781649029511494\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15244138759937143\n",
      "  mean_inference_ms: 1.0105897705533338\n",
      "  mean_raw_obs_processing_ms: 0.12424908465123562\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -878.1553580557503\n",
      "  episode_reward_mean: -1216.214722945986\n",
      "  episode_reward_min: -1760.3604291262138\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1058.8583092245447\n",
      "    - -1336.1333351790342\n",
      "    - -994.9121802443835\n",
      "    - -979.332724529968\n",
      "    - -918.9385782803904\n",
      "    - -1595.2210751335826\n",
      "    - -1067.2076908649065\n",
      "    - -1644.1987732160069\n",
      "    - -1481.8455631994943\n",
      "    - -1418.0063538374845\n",
      "    - -1039.019441015357\n",
      "    - -1093.2705387455162\n",
      "    - -1064.4086564635736\n",
      "    - -887.320912935638\n",
      "    - -922.3544025556113\n",
      "    - -896.8388844276989\n",
      "    - -1009.7972555785836\n",
      "    - -1473.0615614062408\n",
      "    - -1607.216056590205\n",
      "    - -1668.6427402483648\n",
      "    - -984.6425681387026\n",
      "    - -878.1553580557503\n",
      "    - -1176.01734706895\n",
      "    - -924.971424992065\n",
      "    - -1713.4174885169596\n",
      "    - -1346.1183582435947\n",
      "    - -1092.417470473636\n",
      "    - -1646.3948907266513\n",
      "    - -1760.3604291262138\n",
      "    - -1608.9600137641016\n",
      "    - -1645.3330356249298\n",
      "    - -1011.4701514928411\n",
      "    - -1517.0667804857871\n",
      "    - -1610.6352364548957\n",
      "    - -1029.0980691634763\n",
      "    - -1063.587960259489\n",
      "    - -1681.1366842542716\n",
      "    - -896.4440224717763\n",
      "    - -1069.8016930704953\n",
      "    - -1726.5334087355989\n",
      "    - -901.7309638157002\n",
      "    - -905.9677869655596\n",
      "    - -1016.8173286612305\n",
      "    - -1016.5540579840091\n",
      "    - -1610.2445753722664\n",
      "    - -984.5020941832482\n",
      "    - -962.5742729344432\n",
      "    - -1570.4943250680928\n",
      "    - -928.6560235199195\n",
      "    - -1327.9709381716439\n",
      "    - -1531.602043053955\n",
      "    - -1516.3411438458766\n",
      "    - -1065.908246457656\n",
      "    - -893.597804361671\n",
      "    - -1062.1204871110765\n",
      "    - -1172.7888814024197\n",
      "    - -1504.2281839091158\n",
      "    - -1547.8809454064049\n",
      "    - -1039.980471662971\n",
      "    - -987.668313928197\n",
      "    - -935.9901448044623\n",
      "    - -886.3718179784456\n",
      "    - -1181.0161681031514\n",
      "    - -1106.386430134326\n",
      "    - -993.3900694293094\n",
      "    - -1064.7079292828232\n",
      "    - -971.5294866851863\n",
      "    - -1688.8640263259576\n",
      "    - -1490.4302377645074\n",
      "    - -1220.5105172372384\n",
      "    - -893.9056998694575\n",
      "    - -1590.5402576220356\n",
      "    - -1513.398591755453\n",
      "    - -890.0747975604423\n",
      "    - -1095.2921232763622\n",
      "    - -1341.3520458015607\n",
      "    - -1196.3633168338051\n",
      "    - -1676.6663219015106\n",
      "    - -885.6112163140879\n",
      "    - -1081.6824249961662\n",
      "    - -966.5805365059784\n",
      "    - -1369.0708585173732\n",
      "    - -1314.5514281892072\n",
      "    - -879.1653773147469\n",
      "    - -890.7939862983777\n",
      "    - -897.7416629880984\n",
      "    - -1427.137029814911\n",
      "    - -1338.3153490548116\n",
      "    - -1081.2254126516098\n",
      "    - -1489.3988238542331\n",
      "    - -1063.4339777626594\n",
      "    - -1042.0847622759534\n",
      "    - -1429.7670373664039\n",
      "    - -888.8089937793033\n",
      "    - -981.5646747632268\n",
      "    - -1166.2522647686158\n",
      "    - -1600.0212677463874\n",
      "    - -1529.3898393147842\n",
      "    - -926.4832355846296\n",
      "    - -1548.8258397587606\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19781649029511494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15244138759937143\n",
      "    mean_inference_ms: 1.0105897705533338\n",
      "    mean_raw_obs_processing_ms: 0.12424908465123562\n",
      "time_since_restore: 2056.1615965366364\n",
      "time_this_iter_s: 11.287298679351807\n",
      "time_total_s: 2056.1615965366364\n",
      "timers:\n",
      "  learn_throughput: 708.134\n",
      "  learn_time_ms: 5648.645\n",
      "  load_throughput: 16231826.625\n",
      "  load_time_ms: 0.246\n",
      "  training_iteration_time_ms: 12401.158\n",
      "  update_time_ms: 2.452\n",
      "timestamp: 1660565981\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 760000\n",
      "training_iteration: 190\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 764000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 764000\n",
      "  num_agent_steps_trained: 764000\n",
      "  num_env_steps_sampled: 764000\n",
      "  num_env_steps_trained: 764000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-20-09\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -878.1553580557503\n",
      "episode_reward_mean: -1211.2132175477893\n",
      "episode_reward_min: -1760.3604291262138\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3820\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4634714126586914\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009121837094426155\n",
      "        model: {}\n",
      "        policy_loss: 0.01188063807785511\n",
      "        total_loss: 9.841279983520508\n",
      "        vf_explained_var: -0.013459404930472374\n",
      "        vf_loss: 9.818866729736328\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 764000\n",
      "  num_agent_steps_trained: 764000\n",
      "  num_env_steps_sampled: 764000\n",
      "  num_env_steps_trained: 764000\n",
      "iterations_since_restore: 191\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 764000\n",
      "num_agent_steps_trained: 764000\n",
      "num_env_steps_sampled: 764000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 764000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 82.8153846153846\n",
      "  ram_util_percent: 65.63076923076923\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19827579931215675\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1527555437928676\n",
      "  mean_inference_ms: 1.0130502015687533\n",
      "  mean_raw_obs_processing_ms: 0.12450883643262893\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -878.1553580557503\n",
      "  episode_reward_mean: -1211.2132175477893\n",
      "  episode_reward_min: -1760.3604291262138\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -984.6425681387026\n",
      "    - -878.1553580557503\n",
      "    - -1176.01734706895\n",
      "    - -924.971424992065\n",
      "    - -1713.4174885169596\n",
      "    - -1346.1183582435947\n",
      "    - -1092.417470473636\n",
      "    - -1646.3948907266513\n",
      "    - -1760.3604291262138\n",
      "    - -1608.9600137641016\n",
      "    - -1645.3330356249298\n",
      "    - -1011.4701514928411\n",
      "    - -1517.0667804857871\n",
      "    - -1610.6352364548957\n",
      "    - -1029.0980691634763\n",
      "    - -1063.587960259489\n",
      "    - -1681.1366842542716\n",
      "    - -896.4440224717763\n",
      "    - -1069.8016930704953\n",
      "    - -1726.5334087355989\n",
      "    - -901.7309638157002\n",
      "    - -905.9677869655596\n",
      "    - -1016.8173286612305\n",
      "    - -1016.5540579840091\n",
      "    - -1610.2445753722664\n",
      "    - -984.5020941832482\n",
      "    - -962.5742729344432\n",
      "    - -1570.4943250680928\n",
      "    - -928.6560235199195\n",
      "    - -1327.9709381716439\n",
      "    - -1531.602043053955\n",
      "    - -1516.3411438458766\n",
      "    - -1065.908246457656\n",
      "    - -893.597804361671\n",
      "    - -1062.1204871110765\n",
      "    - -1172.7888814024197\n",
      "    - -1504.2281839091158\n",
      "    - -1547.8809454064049\n",
      "    - -1039.980471662971\n",
      "    - -987.668313928197\n",
      "    - -935.9901448044623\n",
      "    - -886.3718179784456\n",
      "    - -1181.0161681031514\n",
      "    - -1106.386430134326\n",
      "    - -993.3900694293094\n",
      "    - -1064.7079292828232\n",
      "    - -971.5294866851863\n",
      "    - -1688.8640263259576\n",
      "    - -1490.4302377645074\n",
      "    - -1220.5105172372384\n",
      "    - -893.9056998694575\n",
      "    - -1590.5402576220356\n",
      "    - -1513.398591755453\n",
      "    - -890.0747975604423\n",
      "    - -1095.2921232763622\n",
      "    - -1341.3520458015607\n",
      "    - -1196.3633168338051\n",
      "    - -1676.6663219015106\n",
      "    - -885.6112163140879\n",
      "    - -1081.6824249961662\n",
      "    - -966.5805365059784\n",
      "    - -1369.0708585173732\n",
      "    - -1314.5514281892072\n",
      "    - -879.1653773147469\n",
      "    - -890.7939862983777\n",
      "    - -897.7416629880984\n",
      "    - -1427.137029814911\n",
      "    - -1338.3153490548116\n",
      "    - -1081.2254126516098\n",
      "    - -1489.3988238542331\n",
      "    - -1063.4339777626594\n",
      "    - -1042.0847622759534\n",
      "    - -1429.7670373664039\n",
      "    - -888.8089937793033\n",
      "    - -981.5646747632268\n",
      "    - -1166.2522647686158\n",
      "    - -1600.0212677463874\n",
      "    - -1529.3898393147842\n",
      "    - -926.4832355846296\n",
      "    - -1548.8258397587606\n",
      "    - -1315.8253941811\n",
      "    - -1041.7049607094893\n",
      "    - -885.1273875900291\n",
      "    - -1463.6615026953248\n",
      "    - -982.1413159653125\n",
      "    - -1731.7835888267746\n",
      "    - -1628.529062516308\n",
      "    - -1201.9379923585218\n",
      "    - -887.5700092423698\n",
      "    - -955.7504803074911\n",
      "    - -1430.774164320203\n",
      "    - -1388.3846391931168\n",
      "    - -1093.7849913644247\n",
      "    - -1058.6903272917168\n",
      "    - -892.9168232453553\n",
      "    - -885.1906636468847\n",
      "    - -981.097214691514\n",
      "    - -909.0923941231595\n",
      "    - -1587.8953869467084\n",
      "    - -1334.5761946411162\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19827579931215675\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1527555437928676\n",
      "    mean_inference_ms: 1.0130502015687533\n",
      "    mean_raw_obs_processing_ms: 0.12450883643262893\n",
      "time_since_restore: 2083.742700099945\n",
      "time_this_iter_s: 27.581103563308716\n",
      "time_total_s: 2083.742700099945\n",
      "timers:\n",
      "  learn_throughput: 615.384\n",
      "  learn_time_ms: 6500.01\n",
      "  load_throughput: 16219273.009\n",
      "  load_time_ms: 0.247\n",
      "  training_iteration_time_ms: 14159.067\n",
      "  update_time_ms: 2.739\n",
      "timestamp: 1660566009\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 764000\n",
      "training_iteration: 191\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 768000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 768000\n",
      "  num_agent_steps_trained: 768000\n",
      "  num_env_steps_sampled: 768000\n",
      "  num_env_steps_trained: 768000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-20-25\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -871.8658133602139\n",
      "episode_reward_mean: -1157.795032669685\n",
      "episode_reward_min: -1731.7835888267746\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3840\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.27950984239578247\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01010771282017231\n",
      "        model: {}\n",
      "        policy_loss: 0.011972629465162754\n",
      "        total_loss: 9.846896171569824\n",
      "        vf_explained_var: -0.017675383016467094\n",
      "        vf_loss: 9.82325267791748\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 768000\n",
      "  num_agent_steps_trained: 768000\n",
      "  num_env_steps_sampled: 768000\n",
      "  num_env_steps_trained: 768000\n",
      "iterations_since_restore: 192\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 768000\n",
      "num_agent_steps_trained: 768000\n",
      "num_env_steps_sampled: 768000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 768000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 73.4\n",
      "  ram_util_percent: 64.77391304347827\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19870072961834523\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.1530505805862998\n",
      "  mean_inference_ms: 1.015356750468605\n",
      "  mean_raw_obs_processing_ms: 0.12474992921840286\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -871.8658133602139\n",
      "  episode_reward_mean: -1157.795032669685\n",
      "  episode_reward_min: -1731.7835888267746\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -901.7309638157002\n",
      "    - -905.9677869655596\n",
      "    - -1016.8173286612305\n",
      "    - -1016.5540579840091\n",
      "    - -1610.2445753722664\n",
      "    - -984.5020941832482\n",
      "    - -962.5742729344432\n",
      "    - -1570.4943250680928\n",
      "    - -928.6560235199195\n",
      "    - -1327.9709381716439\n",
      "    - -1531.602043053955\n",
      "    - -1516.3411438458766\n",
      "    - -1065.908246457656\n",
      "    - -893.597804361671\n",
      "    - -1062.1204871110765\n",
      "    - -1172.7888814024197\n",
      "    - -1504.2281839091158\n",
      "    - -1547.8809454064049\n",
      "    - -1039.980471662971\n",
      "    - -987.668313928197\n",
      "    - -935.9901448044623\n",
      "    - -886.3718179784456\n",
      "    - -1181.0161681031514\n",
      "    - -1106.386430134326\n",
      "    - -993.3900694293094\n",
      "    - -1064.7079292828232\n",
      "    - -971.5294866851863\n",
      "    - -1688.8640263259576\n",
      "    - -1490.4302377645074\n",
      "    - -1220.5105172372384\n",
      "    - -893.9056998694575\n",
      "    - -1590.5402576220356\n",
      "    - -1513.398591755453\n",
      "    - -890.0747975604423\n",
      "    - -1095.2921232763622\n",
      "    - -1341.3520458015607\n",
      "    - -1196.3633168338051\n",
      "    - -1676.6663219015106\n",
      "    - -885.6112163140879\n",
      "    - -1081.6824249961662\n",
      "    - -966.5805365059784\n",
      "    - -1369.0708585173732\n",
      "    - -1314.5514281892072\n",
      "    - -879.1653773147469\n",
      "    - -890.7939862983777\n",
      "    - -897.7416629880984\n",
      "    - -1427.137029814911\n",
      "    - -1338.3153490548116\n",
      "    - -1081.2254126516098\n",
      "    - -1489.3988238542331\n",
      "    - -1063.4339777626594\n",
      "    - -1042.0847622759534\n",
      "    - -1429.7670373664039\n",
      "    - -888.8089937793033\n",
      "    - -981.5646747632268\n",
      "    - -1166.2522647686158\n",
      "    - -1600.0212677463874\n",
      "    - -1529.3898393147842\n",
      "    - -926.4832355846296\n",
      "    - -1548.8258397587606\n",
      "    - -1315.8253941811\n",
      "    - -1041.7049607094893\n",
      "    - -885.1273875900291\n",
      "    - -1463.6615026953248\n",
      "    - -982.1413159653125\n",
      "    - -1731.7835888267746\n",
      "    - -1628.529062516308\n",
      "    - -1201.9379923585218\n",
      "    - -887.5700092423698\n",
      "    - -955.7504803074911\n",
      "    - -1430.774164320203\n",
      "    - -1388.3846391931168\n",
      "    - -1093.7849913644247\n",
      "    - -1058.6903272917168\n",
      "    - -892.9168232453553\n",
      "    - -885.1906636468847\n",
      "    - -981.097214691514\n",
      "    - -909.0923941231595\n",
      "    - -1587.8953869467084\n",
      "    - -1334.5761946411162\n",
      "    - -891.5602800479242\n",
      "    - -1161.7216110366967\n",
      "    - -889.3009785993856\n",
      "    - -1039.922576475857\n",
      "    - -874.9479615401947\n",
      "    - -1087.4217458216528\n",
      "    - -1376.030472268543\n",
      "    - -1268.0676823144788\n",
      "    - -909.6383795367825\n",
      "    - -1099.1993810733936\n",
      "    - -970.9138977861028\n",
      "    - -1091.3785133562674\n",
      "    - -898.7120591262741\n",
      "    - -1525.6591490986928\n",
      "    - -1059.3322940709184\n",
      "    - -1080.309767454588\n",
      "    - -1090.4924082779235\n",
      "    - -871.8658133602139\n",
      "    - -886.7374590548793\n",
      "    - -967.5314730089814\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19870072961834523\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1530505805862998\n",
      "    mean_inference_ms: 1.015356750468605\n",
      "    mean_raw_obs_processing_ms: 0.12474992921840286\n",
      "time_since_restore: 2099.7874450683594\n",
      "time_this_iter_s: 16.044744968414307\n",
      "time_total_s: 2099.7874450683594\n",
      "timers:\n",
      "  learn_throughput: 585.555\n",
      "  learn_time_ms: 6831.127\n",
      "  load_throughput: 15288150.173\n",
      "  load_time_ms: 0.262\n",
      "  training_iteration_time_ms: 14757.762\n",
      "  update_time_ms: 2.768\n",
      "timestamp: 1660566025\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 768000\n",
      "training_iteration: 192\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 772000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 772000\n",
      "  num_agent_steps_trained: 772000\n",
      "  num_env_steps_sampled: 772000\n",
      "  num_env_steps_trained: 772000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-20-41\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -861.7535633483535\n",
      "episode_reward_mean: -1124.8123123545647\n",
      "episode_reward_min: -1731.7835888267746\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3860\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: -0.38954657316207886\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008795036934316158\n",
      "        model: {}\n",
      "        policy_loss: 0.01170964352786541\n",
      "        total_loss: 9.755346298217773\n",
      "        vf_explained_var: -0.03159419819712639\n",
      "        vf_loss: 9.733481407165527\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 772000\n",
      "  num_agent_steps_trained: 772000\n",
      "  num_env_steps_sampled: 772000\n",
      "  num_env_steps_trained: 772000\n",
      "iterations_since_restore: 193\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 772000\n",
      "num_agent_steps_trained: 772000\n",
      "num_env_steps_sampled: 772000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 772000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 68.55652173913043\n",
      "  ram_util_percent: 64.74782608695652\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19919769488678188\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15339277889494002\n",
      "  mean_inference_ms: 1.0180534862707362\n",
      "  mean_raw_obs_processing_ms: 0.12502810214325158\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -861.7535633483535\n",
      "  episode_reward_mean: -1124.8123123545647\n",
      "  episode_reward_min: -1731.7835888267746\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -935.9901448044623\n",
      "    - -886.3718179784456\n",
      "    - -1181.0161681031514\n",
      "    - -1106.386430134326\n",
      "    - -993.3900694293094\n",
      "    - -1064.7079292828232\n",
      "    - -971.5294866851863\n",
      "    - -1688.8640263259576\n",
      "    - -1490.4302377645074\n",
      "    - -1220.5105172372384\n",
      "    - -893.9056998694575\n",
      "    - -1590.5402576220356\n",
      "    - -1513.398591755453\n",
      "    - -890.0747975604423\n",
      "    - -1095.2921232763622\n",
      "    - -1341.3520458015607\n",
      "    - -1196.3633168338051\n",
      "    - -1676.6663219015106\n",
      "    - -885.6112163140879\n",
      "    - -1081.6824249961662\n",
      "    - -966.5805365059784\n",
      "    - -1369.0708585173732\n",
      "    - -1314.5514281892072\n",
      "    - -879.1653773147469\n",
      "    - -890.7939862983777\n",
      "    - -897.7416629880984\n",
      "    - -1427.137029814911\n",
      "    - -1338.3153490548116\n",
      "    - -1081.2254126516098\n",
      "    - -1489.3988238542331\n",
      "    - -1063.4339777626594\n",
      "    - -1042.0847622759534\n",
      "    - -1429.7670373664039\n",
      "    - -888.8089937793033\n",
      "    - -981.5646747632268\n",
      "    - -1166.2522647686158\n",
      "    - -1600.0212677463874\n",
      "    - -1529.3898393147842\n",
      "    - -926.4832355846296\n",
      "    - -1548.8258397587606\n",
      "    - -1315.8253941811\n",
      "    - -1041.7049607094893\n",
      "    - -885.1273875900291\n",
      "    - -1463.6615026953248\n",
      "    - -982.1413159653125\n",
      "    - -1731.7835888267746\n",
      "    - -1628.529062516308\n",
      "    - -1201.9379923585218\n",
      "    - -887.5700092423698\n",
      "    - -955.7504803074911\n",
      "    - -1430.774164320203\n",
      "    - -1388.3846391931168\n",
      "    - -1093.7849913644247\n",
      "    - -1058.6903272917168\n",
      "    - -892.9168232453553\n",
      "    - -885.1906636468847\n",
      "    - -981.097214691514\n",
      "    - -909.0923941231595\n",
      "    - -1587.8953869467084\n",
      "    - -1334.5761946411162\n",
      "    - -891.5602800479242\n",
      "    - -1161.7216110366967\n",
      "    - -889.3009785993856\n",
      "    - -1039.922576475857\n",
      "    - -874.9479615401947\n",
      "    - -1087.4217458216528\n",
      "    - -1376.030472268543\n",
      "    - -1268.0676823144788\n",
      "    - -909.6383795367825\n",
      "    - -1099.1993810733936\n",
      "    - -970.9138977861028\n",
      "    - -1091.3785133562674\n",
      "    - -898.7120591262741\n",
      "    - -1525.6591490986928\n",
      "    - -1059.3322940709184\n",
      "    - -1080.309767454588\n",
      "    - -1090.4924082779235\n",
      "    - -871.8658133602139\n",
      "    - -886.7374590548793\n",
      "    - -967.5314730089814\n",
      "    - -891.346765277063\n",
      "    - -962.7505811826362\n",
      "    - -885.9242802768747\n",
      "    - -1109.0856748819526\n",
      "    - -987.7035909597229\n",
      "    - -886.2807975278015\n",
      "    - -1067.4737732596227\n",
      "    - -1170.2134187668196\n",
      "    - -1444.4215286907067\n",
      "    - -1076.6470842916365\n",
      "    - -914.3542754286049\n",
      "    - -1069.5074729630403\n",
      "    - -988.9580327535876\n",
      "    - -975.4627081378549\n",
      "    - -985.1519025812771\n",
      "    - -876.9769049778906\n",
      "    - -1155.8053803966284\n",
      "    - -890.1061630729755\n",
      "    - -861.7535633483535\n",
      "    - -1049.4329575283828\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19919769488678188\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15339277889494002\n",
      "    mean_inference_ms: 1.0180534862707362\n",
      "    mean_raw_obs_processing_ms: 0.12502810214325158\n",
      "time_since_restore: 2115.9823718070984\n",
      "time_this_iter_s: 16.194926738739014\n",
      "time_total_s: 2115.9823718070984\n",
      "timers:\n",
      "  learn_throughput: 558.408\n",
      "  learn_time_ms: 7163.221\n",
      "  load_throughput: 15243699.8\n",
      "  load_time_ms: 0.262\n",
      "  training_iteration_time_ms: 15367.362\n",
      "  update_time_ms: 2.899\n",
      "timestamp: 1660566041\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 772000\n",
      "training_iteration: 193\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 776000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 776000\n",
      "  num_agent_steps_trained: 776000\n",
      "  num_env_steps_sampled: 776000\n",
      "  num_env_steps_trained: 776000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-20-58\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -810.5744379423566\n",
      "episode_reward_mean: -1127.0779153329001\n",
      "episode_reward_min: -1731.7835888267746\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3880\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.832901954650879\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015437815338373184\n",
      "        model: {}\n",
      "        policy_loss: 0.017729077488183975\n",
      "        total_loss: 9.855798721313477\n",
      "        vf_explained_var: -0.010579402558505535\n",
      "        vf_loss: 9.820245742797852\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 776000\n",
      "  num_agent_steps_trained: 776000\n",
      "  num_env_steps_sampled: 776000\n",
      "  num_env_steps_trained: 776000\n",
      "iterations_since_restore: 194\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 776000\n",
      "num_agent_steps_trained: 776000\n",
      "num_env_steps_sampled: 776000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 776000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.07083333333333\n",
      "  ram_util_percent: 64.92916666666666\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19972896394482859\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15375223709227215\n",
      "  mean_inference_ms: 1.0209451047495353\n",
      "  mean_raw_obs_processing_ms: 0.12532283208886788\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -810.5744379423566\n",
      "  episode_reward_mean: -1127.0779153329001\n",
      "  episode_reward_min: -1731.7835888267746\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -966.5805365059784\n",
      "    - -1369.0708585173732\n",
      "    - -1314.5514281892072\n",
      "    - -879.1653773147469\n",
      "    - -890.7939862983777\n",
      "    - -897.7416629880984\n",
      "    - -1427.137029814911\n",
      "    - -1338.3153490548116\n",
      "    - -1081.2254126516098\n",
      "    - -1489.3988238542331\n",
      "    - -1063.4339777626594\n",
      "    - -1042.0847622759534\n",
      "    - -1429.7670373664039\n",
      "    - -888.8089937793033\n",
      "    - -981.5646747632268\n",
      "    - -1166.2522647686158\n",
      "    - -1600.0212677463874\n",
      "    - -1529.3898393147842\n",
      "    - -926.4832355846296\n",
      "    - -1548.8258397587606\n",
      "    - -1315.8253941811\n",
      "    - -1041.7049607094893\n",
      "    - -885.1273875900291\n",
      "    - -1463.6615026953248\n",
      "    - -982.1413159653125\n",
      "    - -1731.7835888267746\n",
      "    - -1628.529062516308\n",
      "    - -1201.9379923585218\n",
      "    - -887.5700092423698\n",
      "    - -955.7504803074911\n",
      "    - -1430.774164320203\n",
      "    - -1388.3846391931168\n",
      "    - -1093.7849913644247\n",
      "    - -1058.6903272917168\n",
      "    - -892.9168232453553\n",
      "    - -885.1906636468847\n",
      "    - -981.097214691514\n",
      "    - -909.0923941231595\n",
      "    - -1587.8953869467084\n",
      "    - -1334.5761946411162\n",
      "    - -891.5602800479242\n",
      "    - -1161.7216110366967\n",
      "    - -889.3009785993856\n",
      "    - -1039.922576475857\n",
      "    - -874.9479615401947\n",
      "    - -1087.4217458216528\n",
      "    - -1376.030472268543\n",
      "    - -1268.0676823144788\n",
      "    - -909.6383795367825\n",
      "    - -1099.1993810733936\n",
      "    - -970.9138977861028\n",
      "    - -1091.3785133562674\n",
      "    - -898.7120591262741\n",
      "    - -1525.6591490986928\n",
      "    - -1059.3322940709184\n",
      "    - -1080.309767454588\n",
      "    - -1090.4924082779235\n",
      "    - -871.8658133602139\n",
      "    - -886.7374590548793\n",
      "    - -967.5314730089814\n",
      "    - -891.346765277063\n",
      "    - -962.7505811826362\n",
      "    - -885.9242802768747\n",
      "    - -1109.0856748819526\n",
      "    - -987.7035909597229\n",
      "    - -886.2807975278015\n",
      "    - -1067.4737732596227\n",
      "    - -1170.2134187668196\n",
      "    - -1444.4215286907067\n",
      "    - -1076.6470842916365\n",
      "    - -914.3542754286049\n",
      "    - -1069.5074729630403\n",
      "    - -988.9580327535876\n",
      "    - -975.4627081378549\n",
      "    - -985.1519025812771\n",
      "    - -876.9769049778906\n",
      "    - -1155.8053803966284\n",
      "    - -890.1061630729755\n",
      "    - -861.7535633483535\n",
      "    - -1049.4329575283828\n",
      "    - -1359.5309763294883\n",
      "    - -1072.9593060285715\n",
      "    - -1017.5228999531375\n",
      "    - -1166.5884027927723\n",
      "    - -1515.933429783369\n",
      "    - -884.4882608866907\n",
      "    - -1707.890387665496\n",
      "    - -810.5744379423566\n",
      "    - -1185.2479247374595\n",
      "    - -900.1307602381222\n",
      "    - -1284.242659530731\n",
      "    - -891.8255877748799\n",
      "    - -911.741006948722\n",
      "    - -1190.6851972801933\n",
      "    - -1616.8801008214548\n",
      "    - -1309.3689368487132\n",
      "    - -1694.1599916889552\n",
      "    - -1446.1379186986017\n",
      "    - -890.261700755347\n",
      "    - -1074.4740348047737\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19972896394482859\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15375223709227215\n",
      "    mean_inference_ms: 1.0209451047495353\n",
      "    mean_raw_obs_processing_ms: 0.12532283208886788\n",
      "time_since_restore: 2132.48743224144\n",
      "time_this_iter_s: 16.50506043434143\n",
      "time_total_s: 2132.48743224144\n",
      "timers:\n",
      "  learn_throughput: 529.884\n",
      "  learn_time_ms: 7548.828\n",
      "  load_throughput: 14530760.437\n",
      "  load_time_ms: 0.275\n",
      "  training_iteration_time_ms: 15994.256\n",
      "  update_time_ms: 2.896\n",
      "timestamp: 1660566058\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 776000\n",
      "training_iteration: 194\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 780000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 780000\n",
      "  num_agent_steps_trained: 780000\n",
      "  num_env_steps_sampled: 780000\n",
      "  num_env_steps_trained: 780000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-21-16\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -810.5744379423566\n",
      "episode_reward_mean: -1142.4543842985624\n",
      "episode_reward_min: -1731.7835888267746\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3900\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.702758550643921\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014202053658664227\n",
      "        model: {}\n",
      "        policy_loss: 0.013910653069615364\n",
      "        total_loss: 9.931720733642578\n",
      "        vf_explained_var: -0.018125997856259346\n",
      "        vf_loss: 9.901412963867188\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 780000\n",
      "  num_agent_steps_trained: 780000\n",
      "  num_env_steps_sampled: 780000\n",
      "  num_env_steps_trained: 780000\n",
      "iterations_since_restore: 195\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 780000\n",
      "num_agent_steps_trained: 780000\n",
      "num_env_steps_sampled: 780000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 780000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 72.736\n",
      "  ram_util_percent: 64.972\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.2003361995194978\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15416192612967894\n",
      "  mean_inference_ms: 1.0242765973745294\n",
      "  mean_raw_obs_processing_ms: 0.1256607875135593\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -810.5744379423566\n",
      "  episode_reward_mean: -1142.4543842985624\n",
      "  episode_reward_min: -1731.7835888267746\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -1315.8253941811\n",
      "    - -1041.7049607094893\n",
      "    - -885.1273875900291\n",
      "    - -1463.6615026953248\n",
      "    - -982.1413159653125\n",
      "    - -1731.7835888267746\n",
      "    - -1628.529062516308\n",
      "    - -1201.9379923585218\n",
      "    - -887.5700092423698\n",
      "    - -955.7504803074911\n",
      "    - -1430.774164320203\n",
      "    - -1388.3846391931168\n",
      "    - -1093.7849913644247\n",
      "    - -1058.6903272917168\n",
      "    - -892.9168232453553\n",
      "    - -885.1906636468847\n",
      "    - -981.097214691514\n",
      "    - -909.0923941231595\n",
      "    - -1587.8953869467084\n",
      "    - -1334.5761946411162\n",
      "    - -891.5602800479242\n",
      "    - -1161.7216110366967\n",
      "    - -889.3009785993856\n",
      "    - -1039.922576475857\n",
      "    - -874.9479615401947\n",
      "    - -1087.4217458216528\n",
      "    - -1376.030472268543\n",
      "    - -1268.0676823144788\n",
      "    - -909.6383795367825\n",
      "    - -1099.1993810733936\n",
      "    - -970.9138977861028\n",
      "    - -1091.3785133562674\n",
      "    - -898.7120591262741\n",
      "    - -1525.6591490986928\n",
      "    - -1059.3322940709184\n",
      "    - -1080.309767454588\n",
      "    - -1090.4924082779235\n",
      "    - -871.8658133602139\n",
      "    - -886.7374590548793\n",
      "    - -967.5314730089814\n",
      "    - -891.346765277063\n",
      "    - -962.7505811826362\n",
      "    - -885.9242802768747\n",
      "    - -1109.0856748819526\n",
      "    - -987.7035909597229\n",
      "    - -886.2807975278015\n",
      "    - -1067.4737732596227\n",
      "    - -1170.2134187668196\n",
      "    - -1444.4215286907067\n",
      "    - -1076.6470842916365\n",
      "    - -914.3542754286049\n",
      "    - -1069.5074729630403\n",
      "    - -988.9580327535876\n",
      "    - -975.4627081378549\n",
      "    - -985.1519025812771\n",
      "    - -876.9769049778906\n",
      "    - -1155.8053803966284\n",
      "    - -890.1061630729755\n",
      "    - -861.7535633483535\n",
      "    - -1049.4329575283828\n",
      "    - -1359.5309763294883\n",
      "    - -1072.9593060285715\n",
      "    - -1017.5228999531375\n",
      "    - -1166.5884027927723\n",
      "    - -1515.933429783369\n",
      "    - -884.4882608866907\n",
      "    - -1707.890387665496\n",
      "    - -810.5744379423566\n",
      "    - -1185.2479247374595\n",
      "    - -900.1307602381222\n",
      "    - -1284.242659530731\n",
      "    - -891.8255877748799\n",
      "    - -911.741006948722\n",
      "    - -1190.6851972801933\n",
      "    - -1616.8801008214548\n",
      "    - -1309.3689368487132\n",
      "    - -1694.1599916889552\n",
      "    - -1446.1379186986017\n",
      "    - -890.261700755347\n",
      "    - -1074.4740348047737\n",
      "    - -891.3097663924796\n",
      "    - -970.0763637419791\n",
      "    - -1279.986333706904\n",
      "    - -1642.6399879828912\n",
      "    - -1019.5837255238087\n",
      "    - -1368.8553865572126\n",
      "    - -1474.7842076690085\n",
      "    - -1369.7768665555857\n",
      "    - -1126.1419759340897\n",
      "    - -1237.748207712123\n",
      "    - -1658.0694276510285\n",
      "    - -1650.2365990724031\n",
      "    - -984.8141911675006\n",
      "    - -1661.739184813638\n",
      "    - -1090.1358469392258\n",
      "    - -1356.2947349390136\n",
      "    - -918.619758356486\n",
      "    - -1191.9893994188749\n",
      "    - -891.2594274583084\n",
      "    - -1584.197863283754\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2003361995194978\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15416192612967894\n",
      "    mean_inference_ms: 1.0242765973745294\n",
      "    mean_raw_obs_processing_ms: 0.1256607875135593\n",
      "time_since_restore: 2150.1420266628265\n",
      "time_this_iter_s: 17.65459442138672\n",
      "time_total_s: 2150.1420266628265\n",
      "timers:\n",
      "  learn_throughput: 504.08\n",
      "  learn_time_ms: 7935.253\n",
      "  load_throughput: 14451904.557\n",
      "  load_time_ms: 0.277\n",
      "  training_iteration_time_ms: 16280.164\n",
      "  update_time_ms: 3.494\n",
      "timestamp: 1660566076\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 780000\n",
      "training_iteration: 195\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n",
      "agent_timesteps_total: 784000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 784000\n",
      "  num_agent_steps_trained: 784000\n",
      "  num_env_steps_sampled: 784000\n",
      "  num_env_steps_trained: 784000\n",
      "custom_metrics: {}\n",
      "date: 2022-08-15_14-21-29\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -810.5744379423566\n",
      "episode_reward_mean: -1119.6041127783124\n",
      "episode_reward_min: -1707.890387665496\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 3920\n",
      "experiment_id: d4e2c07358434fcfb1bffa31989c0df7\n",
      "hostname: zet4\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5216729640960693\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008882205002009869\n",
      "        model: {}\n",
      "        policy_loss: 0.011491194367408752\n",
      "        total_loss: 9.8053560256958\n",
      "        vf_explained_var: -0.024706590920686722\n",
      "        vf_loss: 9.783610343933105\n",
      "      num_agent_steps_trained: 128.0\n",
      "  num_agent_steps_sampled: 784000\n",
      "  num_agent_steps_trained: 784000\n",
      "  num_env_steps_sampled: 784000\n",
      "  num_env_steps_trained: 784000\n",
      "iterations_since_restore: 196\n",
      "node_ip: 192.168.70.36\n",
      "num_agent_steps_sampled: 784000\n",
      "num_agent_steps_trained: 784000\n",
      "num_env_steps_sampled: 784000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 784000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 71.0578947368421\n",
      "  ram_util_percent: 65.15789473684211\n",
      "pid: 34627\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.20070346312725248\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.15439740469896585\n",
      "  mean_inference_ms: 1.0262385410806645\n",
      "  mean_raw_obs_processing_ms: 0.12585698400236262\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -810.5744379423566\n",
      "  episode_reward_mean: -1119.6041127783124\n",
      "  episode_reward_min: -1707.890387665496\n",
      "  episodes_this_iter: 20\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -891.5602800479242\n",
      "    - -1161.7216110366967\n",
      "    - -889.3009785993856\n",
      "    - -1039.922576475857\n",
      "    - -874.9479615401947\n",
      "    - -1087.4217458216528\n",
      "    - -1376.030472268543\n",
      "    - -1268.0676823144788\n",
      "    - -909.6383795367825\n",
      "    - -1099.1993810733936\n",
      "    - -970.9138977861028\n",
      "    - -1091.3785133562674\n",
      "    - -898.7120591262741\n",
      "    - -1525.6591490986928\n",
      "    - -1059.3322940709184\n",
      "    - -1080.309767454588\n",
      "    - -1090.4924082779235\n",
      "    - -871.8658133602139\n",
      "    - -886.7374590548793\n",
      "    - -967.5314730089814\n",
      "    - -891.346765277063\n",
      "    - -962.7505811826362\n",
      "    - -885.9242802768747\n",
      "    - -1109.0856748819526\n",
      "    - -987.7035909597229\n",
      "    - -886.2807975278015\n",
      "    - -1067.4737732596227\n",
      "    - -1170.2134187668196\n",
      "    - -1444.4215286907067\n",
      "    - -1076.6470842916365\n",
      "    - -914.3542754286049\n",
      "    - -1069.5074729630403\n",
      "    - -988.9580327535876\n",
      "    - -975.4627081378549\n",
      "    - -985.1519025812771\n",
      "    - -876.9769049778906\n",
      "    - -1155.8053803966284\n",
      "    - -890.1061630729755\n",
      "    - -861.7535633483535\n",
      "    - -1049.4329575283828\n",
      "    - -1359.5309763294883\n",
      "    - -1072.9593060285715\n",
      "    - -1017.5228999531375\n",
      "    - -1166.5884027927723\n",
      "    - -1515.933429783369\n",
      "    - -884.4882608866907\n",
      "    - -1707.890387665496\n",
      "    - -810.5744379423566\n",
      "    - -1185.2479247374595\n",
      "    - -900.1307602381222\n",
      "    - -1284.242659530731\n",
      "    - -891.8255877748799\n",
      "    - -911.741006948722\n",
      "    - -1190.6851972801933\n",
      "    - -1616.8801008214548\n",
      "    - -1309.3689368487132\n",
      "    - -1694.1599916889552\n",
      "    - -1446.1379186986017\n",
      "    - -890.261700755347\n",
      "    - -1074.4740348047737\n",
      "    - -891.3097663924796\n",
      "    - -970.0763637419791\n",
      "    - -1279.986333706904\n",
      "    - -1642.6399879828912\n",
      "    - -1019.5837255238087\n",
      "    - -1368.8553865572126\n",
      "    - -1474.7842076690085\n",
      "    - -1369.7768665555857\n",
      "    - -1126.1419759340897\n",
      "    - -1237.748207712123\n",
      "    - -1658.0694276510285\n",
      "    - -1650.2365990724031\n",
      "    - -984.8141911675006\n",
      "    - -1661.739184813638\n",
      "    - -1090.1358469392258\n",
      "    - -1356.2947349390136\n",
      "    - -918.619758356486\n",
      "    - -1191.9893994188749\n",
      "    - -891.2594274583084\n",
      "    - -1584.197863283754\n",
      "    - -1126.8762080152867\n",
      "    - -1433.683349201933\n",
      "    - -1325.691259059472\n",
      "    - -1481.8196302570022\n",
      "    - -890.8698710857384\n",
      "    - -891.8138231582346\n",
      "    - -904.4086256935528\n",
      "    - -1533.7862836121503\n",
      "    - -978.9113776156751\n",
      "    - -1040.3760973769156\n",
      "    - -890.169272109134\n",
      "    - -1522.6376991835075\n",
      "    - -894.1476725492145\n",
      "    - -880.4856997930498\n",
      "    - -976.1769292836723\n",
      "    - -979.02741603465\n",
      "    - -900.9917670405094\n",
      "    - -890.47283722678\n",
      "    - -893.7069658361022\n",
      "    - -935.3545576993126\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.20070346312725248\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15439740469896585\n",
      "    mean_inference_ms: 1.0262385410806645\n",
      "    mean_raw_obs_processing_ms: 0.12585698400236262\n",
      "time_since_restore: 2163.082482099533\n",
      "time_this_iter_s: 12.940455436706543\n",
      "time_total_s: 2163.082482099533\n",
      "timers:\n",
      "  learn_throughput: 521.029\n",
      "  learn_time_ms: 7677.111\n",
      "  load_throughput: 14436981.327\n",
      "  load_time_ms: 0.277\n",
      "  training_iteration_time_ms: 16146.892\n",
      "  update_time_ms: 3.509\n",
      "timestamp: 1660566089\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 784000\n",
      "training_iteration: 196\n",
      "trial_id: default\n",
      "warmup_time: 6.6556315422058105\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/zet4/Documents/finrl/basic_training.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zet4/Documents/finrl/basic_training.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m trainer \u001b[39m=\u001b[39m ppo\u001b[39m.\u001b[39mPPOTrainer(config\u001b[39m=\u001b[39mconfig, env\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPendulum-v0\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zet4/Documents/finrl/basic_training.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/zet4/Documents/finrl/basic_training.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zet4/Documents/finrl/basic_training.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(pretty_print(result))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zet4/Documents/finrl/basic_training.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.virtualenvs/rayrl/lib/python3.10/site-packages/ray/tune/trainable.py:360\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warmup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n\u001b[1;32m    359\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 360\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    361\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mdict\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mstep() needs to return a dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m \u001b[39m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/rayrl/lib/python3.10/site-packages/ray/rllib/agents/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m step_ctx\u001b[39m.\u001b[39mshould_stop(step_attempt_results):\n\u001b[1;32m   1110\u001b[0m     \u001b[39m# Try to train one step.\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1112\u001b[0m         step_attempt_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_attempt()\n\u001b[1;32m   1113\u001b[0m     \u001b[39m# @ray.remote RolloutWorker failure.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[39mexcept\u001b[39;00m RayError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1115\u001b[0m         \u001b[39m# Try to recover w/o the failed worker.\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/rayrl/lib/python3.10/site-packages/ray/rllib/agents/trainer.py:1214\u001b[0m, in \u001b[0;36mTrainer.step_attempt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[39m# No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m evaluate_this_iter:\n\u001b[0;32m-> 1214\u001b[0m     step_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_exec_plan_or_training_iteration_fn()\n\u001b[1;32m   1215\u001b[0m \u001b[39m# We have to evaluate in this training iteration.\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1217\u001b[0m     \u001b[39m# No parallelism.\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mevaluation_parallel_to_training\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.virtualenvs/rayrl/lib/python3.10/site-packages/ray/rllib/agents/trainer.py:2209\u001b[0m, in \u001b[0;36mTrainer._exec_plan_or_training_iteration_fn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2207\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[1;32m   2208\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39m_disable_execution_plan_api\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m-> 2209\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_iteration()\n\u001b[1;32m   2210\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2211\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[0;32m~/.virtualenvs/rayrl/lib/python3.10/site-packages/ray/rllib/agents/ppo/ppo.py:450\u001b[0m, in \u001b[0;36mPPOTrainer.training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    448\u001b[0m     train_results \u001b[39m=\u001b[39m train_one_step(\u001b[39mself\u001b[39m, train_batch)\n\u001b[1;32m    449\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 450\u001b[0m     train_results \u001b[39m=\u001b[39m multi_gpu_train_one_step(\u001b[39mself\u001b[39;49m, train_batch)\n\u001b[1;32m    452\u001b[0m global_vars \u001b[39m=\u001b[39m {\n\u001b[1;32m    453\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimestep\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_counters[NUM_AGENT_STEPS_SAMPLED],\n\u001b[1;32m    454\u001b[0m }\n\u001b[1;32m    456\u001b[0m \u001b[39m# Update weights - after learning on the local worker - on all remote\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[39m# workers.\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/rayrl/lib/python3.10/site-packages/ray/rllib/execution/train_ops.py:167\u001b[0m, in \u001b[0;36mmulti_gpu_train_one_step\u001b[0;34m(trainer, train_batch)\u001b[0m\n\u001b[1;32m    162\u001b[0m         permutation \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mpermutation(num_batches)\n\u001b[1;32m    163\u001b[0m         \u001b[39mfor\u001b[39;00m batch_index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_batches):\n\u001b[1;32m    164\u001b[0m             \u001b[39m# Learn on the pre-loaded data in the buffer.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m             \u001b[39m# Note: For minibatch SGD, the data is an offset into\u001b[39;00m\n\u001b[1;32m    166\u001b[0m             \u001b[39m# the pre-loaded entire train batch.\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m             results \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39;49mlearn_on_loaded_batch(\n\u001b[1;32m    168\u001b[0m                 permutation[batch_index] \u001b[39m*\u001b[39;49m per_device_batch_size, buffer_index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[1;32m    169\u001b[0m             )\n\u001b[1;32m    171\u001b[0m             learner_info_builder\u001b[39m.\u001b[39madd_learn_on_batch_results(results, policy_id)\n\u001b[1;32m    173\u001b[0m \u001b[39m# Tower reduce and finalize results.\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/rayrl/lib/python3.10/site-packages/ray/rllib/policy/dynamic_tf_policy.py:627\u001b[0m, in \u001b[0;36mDynamicTFPolicy.learn_on_loaded_batch\u001b[0;34m(self, offset, buffer_index)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    624\u001b[0m         sliced_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loaded_single_cpu_batch\u001b[39m.\u001b[39mslice(\n\u001b[1;32m    625\u001b[0m             start\u001b[39m=\u001b[39moffset, end\u001b[39m=\u001b[39moffset \u001b[39m+\u001b[39m batch_size\n\u001b[1;32m    626\u001b[0m         )\n\u001b[0;32m--> 627\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearn_on_batch(sliced_batch)\n\u001b[1;32m    629\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmulti_gpu_tower_stacks[buffer_index]\u001b[39m.\u001b[39moptimize(\n\u001b[1;32m    630\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_session(), offset\n\u001b[1;32m    631\u001b[0m )\n",
      "File \u001b[0;32m~/.virtualenvs/rayrl/lib/python3.10/site-packages/ray/rllib/policy/tf_policy.py:453\u001b[0m, in \u001b[0;36mTFPolicy.learn_on_batch\u001b[0;34m(self, postprocessed_batch)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mon_learn_on_batch(\n\u001b[1;32m    449\u001b[0m     policy\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, train_batch\u001b[39m=\u001b[39mpostprocessed_batch, result\u001b[39m=\u001b[39mlearn_stats\n\u001b[1;32m    450\u001b[0m )\n\u001b[1;32m    452\u001b[0m fetches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_learn_on_batch(builder, postprocessed_batch)\n\u001b[0;32m--> 453\u001b[0m stats \u001b[39m=\u001b[39m builder\u001b[39m.\u001b[39;49mget(fetches)\n\u001b[1;32m    454\u001b[0m stats\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m    455\u001b[0m     {\n\u001b[1;32m    456\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcustom_metrics\u001b[39m\u001b[39m\"\u001b[39m: learn_stats,\n\u001b[1;32m    457\u001b[0m         NUM_AGENT_STEPS_TRAINED: postprocessed_batch\u001b[39m.\u001b[39mcount,\n\u001b[1;32m    458\u001b[0m     }\n\u001b[1;32m    459\u001b[0m )\n\u001b[1;32m    460\u001b[0m \u001b[39mreturn\u001b[39;00m stats\n",
      "File \u001b[0;32m~/.virtualenvs/rayrl/lib/python3.10/site-packages/ray/rllib/utils/tf_run_builder.py:42\u001b[0m, in \u001b[0;36mTFRunBuilder.get\u001b[0;34m(self, to_fetch)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executed \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executed \u001b[39m=\u001b[39m run_timeline(\n\u001b[1;32m     43\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msession,\n\u001b[1;32m     44\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetches,\n\u001b[1;32m     45\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdebug_name,\n\u001b[1;32m     46\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_dict,\n\u001b[1;32m     47\u001b[0m             os\u001b[39m.\u001b[39;49menviron\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mTF_TIMELINE_DIR\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     48\u001b[0m         )\n\u001b[1;32m     49\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     50\u001b[0m         logger\u001b[39m.\u001b[39mexception(\n\u001b[1;32m     51\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mError fetching: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, feed_dict=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     52\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfetches, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_dict\n\u001b[1;32m     53\u001b[0m             )\n\u001b[1;32m     54\u001b[0m         )\n",
      "File \u001b[0;32m~/.virtualenvs/rayrl/lib/python3.10/site-packages/ray/rllib/utils/tf_run_builder.py:102\u001b[0m, in \u001b[0;36mrun_timeline\u001b[0;34m(sess, ops, debug_name, feed_dict, timeline_dir)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[39mif\u001b[39;00m log_once(\u001b[39m\"\u001b[39m\u001b[39mtf_timeline\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m         logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m     99\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExecuting TF run without tracing. To dump TF timeline traces \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mto disk, set the TF_TIMELINE_DIR environment variable.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m         )\n\u001b[0;32m--> 102\u001b[0m     fetches \u001b[39m=\u001b[39m sess\u001b[39m.\u001b[39;49mrun(ops, feed_dict\u001b[39m=\u001b[39;49mfeed_dict)\n\u001b[1;32m    103\u001b[0m \u001b[39mreturn\u001b[39;00m fetches\n",
      "File \u001b[0;32m~/.virtualenvs/rayrl/lib/python3.10/site-packages/tensorflow/python/client/session.py:967\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    964\u001b[0m run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    966\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 967\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39mNone\u001b[39;49;00m, fetches, feed_dict, options_ptr,\n\u001b[1;32m    968\u001b[0m                      run_metadata_ptr)\n\u001b[1;32m    969\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[1;32m    970\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m~/.virtualenvs/rayrl/lib/python3.10/site-packages/tensorflow/python/client/session.py:1172\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1169\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTensor \u001b[39m\u001b[39m{\u001b[39;00msubfeed_t\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m may not be fed.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1171\u001b[0m       feed_dict_tensor[subfeed_t\u001b[39m.\u001b[39mref()] \u001b[39m=\u001b[39m np_val\n\u001b[0;32m-> 1172\u001b[0m       feed_map[compat\u001b[39m.\u001b[39;49mas_bytes(subfeed_t\u001b[39m.\u001b[39;49mname)] \u001b[39m=\u001b[39m (subfeed_t, subfeed_val)\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Create a fetch handler to take care of the structure of fetches.\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m fetch_handler \u001b[39m=\u001b[39m _FetchHandler(\n\u001b[1;32m   1176\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph, fetches, feed_dict_tensor, feed_handles\u001b[39m=\u001b[39mfeed_handles)\n",
      "File \u001b[0;32m~/.virtualenvs/rayrl/lib/python3.10/site-packages/tensorflow/python/util/compat.py:78\u001b[0m, in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m# Validate encoding, a LookupError will be raised if invalid.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m encoding \u001b[39m=\u001b[39m codecs\u001b[39m.\u001b[39mlookup(encoding)\u001b[39m.\u001b[39mname\n\u001b[0;32m---> 78\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39;49m(bytes_or_text, \u001b[39mbytearray\u001b[39;49m):\n\u001b[1;32m     79\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mbytes\u001b[39m(bytes_or_text)\n\u001b[1;32m     80\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(bytes_or_text, _six\u001b[39m.\u001b[39mtext_type):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ray.init(ignore_reinit_error=True)\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config['num_gpus'] = 0\n",
    "config['num_workers'] = 1\n",
    "trainer = ppo.PPOTrainer(config=config, env='Pendulum-v0')\n",
    "\n",
    "for i in range(1000):\n",
    "    result = trainer.train()\n",
    "    print(pretty_print(result))\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        checkpoint = trainer.save()\n",
    "        print('Iteration ', i)\n",
    "        print('checkpoint saved at ', checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('rayrl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74e0a97186b63debfb3936693e25014f03e3d231f3563a137539dc61528fa20b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
