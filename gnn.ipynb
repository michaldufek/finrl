{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Convolutional Network\n",
    "# https://www.youtube.com/watch?v=8qTnNXdkF1Q&list=PLSgGvve8UweGx4_6hhrF3n4wpHf_RV76_&index=6\n",
    "\n",
    "# Karate club: 34 nodes which are people\n",
    "# edges represent social interactions that occured outside of the context of the Karate club\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community.modularity_max import greedy_modularity_communities\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from scipy.linalg import sqrtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.karate_club_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 78)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.number_of_nodes(), g.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = nx.to_numpy_matrix(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 4., 5., ..., 2., 0., 0.],\n",
       "        [4., 0., 6., ..., 0., 0., 0.],\n",
       "        [5., 6., 0., ..., 0., 2., 0.],\n",
       "        ...,\n",
       "        [2., 0., 0., ..., 0., 4., 4.],\n",
       "        [0., 0., 2., ..., 4., 0., 5.],\n",
       "        [0., 0., 0., ..., 4., 5., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_mod = A + np.eye(g.number_of_nodes()) # self-connections\n",
    "\n",
    "# Normalization\n",
    "D_mod = np.zeros_like(A_mod) # Degree Matrix\n",
    "np.fill_diagonal(D_mod, np.asarray(A_mod.sum(axis=1)).flatten()) # number of connections for each node\n",
    "\n",
    "D_mod_invroot = np.linalg.inv(sqrtm(D_mod))\n",
    "\n",
    "# Note: @ -- is used for matrix multiplication, matrix * matrix brings a element-wise multiplication known as a Hadamard product\n",
    "A_hat = D_mod_invroot @ A_mod @ D_mod_invroot # A_hat_i_j = 1/sqrt(d_i * d_j) * A_hat_i_j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features Matrix -- since we have no node features, we just use the identity matrix\n",
    "X = np.eye(g.number_of_nodes()) # identity matrix effectively map each graph to a column of learnable parameters resulting in a full learnable embeddings\n",
    "\n",
    "# Now we have a labels, normalized Adjacency matrix and input features => GCN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN layer is implemented in GCN layer class which can be able to stack into a larger GCN model\n",
    "from turtle import forward\n",
    "\n",
    "\n",
    "class GCN():\n",
    "    def __init__(self, n_inputs, n_outputs, activation=None, name=''):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.w = glorot_init(self.n_outputs, self.n_inputs)\n",
    "        self.activation = activation\n",
    "        self.name = name\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"GCN: W({'_'+self.name if self.name else ''} ({self.n_inputs}, {self.n_outputs}\"\n",
    "\n",
    "    def forward(self, A, X, W=None):\n",
    "        '''\n",
    "        Assumes A is (bs, bs) adjacency matrix and X is (bs, D)\n",
    "        where bs = 'batch size' and D = input features length\n",
    "        '''\n",
    "        self._X = (A @ X).T # for calculating gradients (D, bs)\n",
    "\n",
    "        if W is None:\n",
    "            W = self.w\n",
    "        \n",
    "        H = W @ self._X # (h, D)*(D, bs) -> (h, bs)\n",
    "        if self.activation is not None:\n",
    "            H = self.activation()\n",
    "        self._H = H # (h, bs)\n",
    "        return self._H.T # (bs, h)\n",
    "\n",
    "    def backward(self, optim, update=True):\n",
    "        dtanh = 1 - np.asarray(self._H.T)**2 # (bs, out_dim)\n",
    "        d2 = np.multiply(optim.out, dtanh) # (bs, out_dim) * element-wise *(bs, out_dim)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zet4/.virtualenvs/rayrl/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Deep Graph Library\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "Number of categories: 7\n"
     ]
    }
   ],
   "source": [
    "import dgl.data\n",
    "\n",
    "dataset = dgl.data.CoraGraphDataset()\n",
    "print(f'Number of categories: {dataset.num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.edata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv, ChebConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes, k=2):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, h_feats)\n",
    "        self.specConv = ChebConv(h_feats, num_classes, k)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        h = self.specConv(g, h)\n",
    "        h = F.relu(h)\n",
    "        return h\n",
    "\n",
    "model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv1): GraphConv(in=1433, out=16, normalization=both, activation=None)\n",
       "  (conv2): GraphConv(in=16, out=16, normalization=both, activation=None)\n",
       "  (specConv): ChebConv(\n",
       "    (linear): Linear(in_features=32, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss 1.9472980499267578, vall_acc: 0.057999998331069946, best acc: 0.057999998331069946, test acc: 0.06400000303983688, best test acc: 0.06400000303983688\n",
      "In epoch 5, loss 1.9188668727874756, vall_acc: 0.20600000023841858, best acc: 0.20600000023841858, test acc: 0.19599999487400055, best test acc: 0.19599999487400055\n",
      "In epoch 10, loss 1.8453208208084106, vall_acc: 0.28200000524520874, best acc: 0.28200000524520874, test acc: 0.2639999985694885, best test acc: 0.2639999985694885\n",
      "In epoch 15, loss 1.7023981809616089, vall_acc: 0.29600000381469727, best acc: 0.29600000381469727, test acc: 0.27900001406669617, best test acc: 0.27900001406669617\n",
      "In epoch 20, loss 1.5109180212020874, vall_acc: 0.29600000381469727, best acc: 0.29600000381469727, test acc: 0.2800000011920929, best test acc: 0.27900001406669617\n",
      "In epoch 25, loss 1.3367013931274414, vall_acc: 0.32199999690055847, best acc: 0.32199999690055847, test acc: 0.31299999356269836, best test acc: 0.31299999356269836\n",
      "In epoch 30, loss 1.2174791097640991, vall_acc: 0.3700000047683716, best acc: 0.3700000047683716, test acc: 0.36899998784065247, best test acc: 0.36899998784065247\n",
      "In epoch 35, loss 1.1470063924789429, vall_acc: 0.3799999952316284, best acc: 0.3799999952316284, test acc: 0.36899998784065247, best test acc: 0.3709999918937683\n",
      "In epoch 40, loss 1.0836690664291382, vall_acc: 0.37599998712539673, best acc: 0.3799999952316284, test acc: 0.36399999260902405, best test acc: 0.3709999918937683\n",
      "In epoch 45, loss 0.9784247279167175, vall_acc: 0.4779999852180481, best acc: 0.4779999852180481, test acc: 0.49300000071525574, best test acc: 0.49300000071525574\n",
      "In epoch 50, loss 0.8958940505981445, vall_acc: 0.5239999890327454, best acc: 0.5239999890327454, test acc: 0.5320000052452087, best test acc: 0.5320000052452087\n",
      "In epoch 55, loss 0.8115035891532898, vall_acc: 0.527999997138977, best acc: 0.527999997138977, test acc: 0.5429999828338623, best test acc: 0.5429999828338623\n",
      "In epoch 60, loss 0.7329880595207214, vall_acc: 0.5339999794960022, best acc: 0.5360000133514404, test acc: 0.5419999957084656, best test acc: 0.546999990940094\n",
      "In epoch 65, loss 0.662456214427948, vall_acc: 0.5519999861717224, best acc: 0.5519999861717224, test acc: 0.5590000152587891, best test acc: 0.5540000200271606\n",
      "In epoch 70, loss 0.6119827628135681, vall_acc: 0.5519999861717224, best acc: 0.5519999861717224, test acc: 0.5690000057220459, best test acc: 0.5540000200271606\n",
      "In epoch 75, loss 0.5881551504135132, vall_acc: 0.5720000267028809, best acc: 0.5720000267028809, test acc: 0.5809999704360962, best test acc: 0.5809999704360962\n",
      "In epoch 80, loss 0.5753339529037476, vall_acc: 0.5699999928474426, best acc: 0.5740000009536743, test acc: 0.5759999752044678, best test acc: 0.5799999833106995\n",
      "In epoch 85, loss 0.5677884817123413, vall_acc: 0.578000009059906, best acc: 0.578000009059906, test acc: 0.5770000219345093, best test acc: 0.5770000219345093\n",
      "In epoch 90, loss 0.5636643171310425, vall_acc: 0.578000009059906, best acc: 0.5799999833106995, test acc: 0.578000009059906, best test acc: 0.5789999961853027\n",
      "In epoch 95, loss 0.5615304708480835, vall_acc: 0.5699999928474426, best acc: 0.5799999833106995, test acc: 0.5770000219345093, best test acc: 0.5789999961853027\n",
      "In epoch 100, loss 0.5602660775184631, vall_acc: 0.5759999752044678, best acc: 0.5799999833106995, test acc: 0.5799999833106995, best test acc: 0.5789999961853027\n",
      "In epoch 105, loss 0.5593274235725403, vall_acc: 0.578000009059906, best acc: 0.5799999833106995, test acc: 0.5740000009536743, best test acc: 0.5789999961853027\n",
      "In epoch 110, loss 0.5587172508239746, vall_acc: 0.578000009059906, best acc: 0.5799999833106995, test acc: 0.5830000042915344, best test acc: 0.5789999961853027\n",
      "In epoch 115, loss 0.5582864284515381, vall_acc: 0.578000009059906, best acc: 0.5799999833106995, test acc: 0.5789999961853027, best test acc: 0.5789999961853027\n",
      "In epoch 120, loss 0.5580045580863953, vall_acc: 0.5799999833106995, best acc: 0.5799999833106995, test acc: 0.5789999961853027, best test acc: 0.5789999961853027\n",
      "In epoch 125, loss 0.557754635810852, vall_acc: 0.5759999752044678, best acc: 0.5799999833106995, test acc: 0.5799999833106995, best test acc: 0.5789999961853027\n",
      "In epoch 130, loss 0.5575720071792603, vall_acc: 0.578000009059906, best acc: 0.5799999833106995, test acc: 0.5789999961853027, best test acc: 0.5789999961853027\n",
      "In epoch 135, loss 0.5574340224266052, vall_acc: 0.5820000171661377, best acc: 0.5820000171661377, test acc: 0.5789999961853027, best test acc: 0.5789999961853027\n",
      "In epoch 140, loss 0.5573179721832275, vall_acc: 0.578000009059906, best acc: 0.5820000171661377, test acc: 0.5770000219345093, best test acc: 0.5789999961853027\n",
      "In epoch 145, loss 0.5572167038917542, vall_acc: 0.578000009059906, best acc: 0.5820000171661377, test acc: 0.5799999833106995, best test acc: 0.5789999961853027\n",
      "In epoch 150, loss 0.5571507215499878, vall_acc: 0.5820000171661377, best acc: 0.5820000171661377, test acc: 0.5820000171661377, best test acc: 0.5789999961853027\n",
      "In epoch 155, loss 0.5570765137672424, vall_acc: 0.5759999752044678, best acc: 0.5820000171661377, test acc: 0.5820000171661377, best test acc: 0.5789999961853027\n",
      "In epoch 160, loss 0.5570277571678162, vall_acc: 0.5740000009536743, best acc: 0.5820000171661377, test acc: 0.5809999704360962, best test acc: 0.5789999961853027\n",
      "In epoch 165, loss 0.5569708943367004, vall_acc: 0.5759999752044678, best acc: 0.5820000171661377, test acc: 0.578000009059906, best test acc: 0.5789999961853027\n",
      "In epoch 170, loss 0.5569043755531311, vall_acc: 0.5740000009536743, best acc: 0.5820000171661377, test acc: 0.5770000219345093, best test acc: 0.5789999961853027\n",
      "In epoch 175, loss 0.556872546672821, vall_acc: 0.578000009059906, best acc: 0.5820000171661377, test acc: 0.5830000042915344, best test acc: 0.5789999961853027\n",
      "In epoch 180, loss 0.5568085312843323, vall_acc: 0.5799999833106995, best acc: 0.5820000171661377, test acc: 0.5820000171661377, best test acc: 0.5789999961853027\n",
      "In epoch 185, loss 0.5567531585693359, vall_acc: 0.5759999752044678, best acc: 0.5820000171661377, test acc: 0.5820000171661377, best test acc: 0.5789999961853027\n",
      "In epoch 190, loss 0.5568060278892517, vall_acc: 0.5759999752044678, best acc: 0.5820000171661377, test acc: 0.5799999833106995, best test acc: 0.5789999961853027\n",
      "In epoch 195, loss 0.5567098259925842, vall_acc: 0.5759999752044678, best acc: 0.5820000171661377, test acc: 0.5770000219345093, best test acc: 0.5789999961853027\n",
      "In epoch 200, loss 0.5566702485084534, vall_acc: 0.578000009059906, best acc: 0.5820000171661377, test acc: 0.5789999961853027, best test acc: 0.5789999961853027\n",
      "In epoch 205, loss 0.5566353797912598, vall_acc: 0.578000009059906, best acc: 0.5820000171661377, test acc: 0.5799999833106995, best test acc: 0.5789999961853027\n",
      "In epoch 210, loss 0.5566021800041199, vall_acc: 0.5759999752044678, best acc: 0.5820000171661377, test acc: 0.5789999961853027, best test acc: 0.5789999961853027\n",
      "In epoch 215, loss 0.5565700531005859, vall_acc: 0.5799999833106995, best acc: 0.5820000171661377, test acc: 0.5799999833106995, best test acc: 0.5789999961853027\n",
      "In epoch 220, loss 0.556553840637207, vall_acc: 0.578000009059906, best acc: 0.5820000171661377, test acc: 0.5789999961853027, best test acc: 0.5789999961853027\n",
      "In epoch 225, loss 0.5565252900123596, vall_acc: 0.5820000171661377, best acc: 0.5820000171661377, test acc: 0.5799999833106995, best test acc: 0.5789999961853027\n",
      "In epoch 230, loss 0.5565060973167419, vall_acc: 0.5799999833106995, best acc: 0.5839999914169312, test acc: 0.5820000171661377, best test acc: 0.5799999833106995\n",
      "In epoch 235, loss 0.5565053224563599, vall_acc: 0.5820000171661377, best acc: 0.5839999914169312, test acc: 0.5830000042915344, best test acc: 0.5799999833106995\n",
      "In epoch 240, loss 0.5564737319946289, vall_acc: 0.5759999752044678, best acc: 0.5839999914169312, test acc: 0.574999988079071, best test acc: 0.5799999833106995\n",
      "In epoch 245, loss 0.556563138961792, vall_acc: 0.5740000009536743, best acc: 0.5839999914169312, test acc: 0.5759999752044678, best test acc: 0.5799999833106995\n",
      "In epoch 250, loss 0.556467592716217, vall_acc: 0.578000009059906, best acc: 0.5839999914169312, test acc: 0.5789999961853027, best test acc: 0.5799999833106995\n",
      "In epoch 255, loss 0.5564420819282532, vall_acc: 0.578000009059906, best acc: 0.5839999914169312, test acc: 0.5809999704360962, best test acc: 0.5799999833106995\n",
      "In epoch 260, loss 0.5564222931861877, vall_acc: 0.5759999752044678, best acc: 0.5839999914169312, test acc: 0.5799999833106995, best test acc: 0.5799999833106995\n",
      "In epoch 265, loss 0.5564072728157043, vall_acc: 0.5799999833106995, best acc: 0.5839999914169312, test acc: 0.5799999833106995, best test acc: 0.5799999833106995\n",
      "In epoch 270, loss 0.5563926696777344, vall_acc: 0.578000009059906, best acc: 0.5839999914169312, test acc: 0.578000009059906, best test acc: 0.5799999833106995\n",
      "In epoch 275, loss 0.5563719272613525, vall_acc: 0.578000009059906, best acc: 0.5839999914169312, test acc: 0.5770000219345093, best test acc: 0.5799999833106995\n",
      "In epoch 280, loss 0.5563528537750244, vall_acc: 0.5820000171661377, best acc: 0.5839999914169312, test acc: 0.578000009059906, best test acc: 0.5799999833106995\n",
      "In epoch 285, loss 0.5563417077064514, vall_acc: 0.5799999833106995, best acc: 0.5839999914169312, test acc: 0.5809999704360962, best test acc: 0.5799999833106995\n",
      "In epoch 290, loss 0.556326687335968, vall_acc: 0.5799999833106995, best acc: 0.5839999914169312, test acc: 0.5789999961853027, best test acc: 0.5799999833106995\n",
      "In epoch 295, loss 0.5563106536865234, vall_acc: 0.5799999833106995, best acc: 0.5839999914169312, test acc: 0.5799999833106995, best test acc: 0.5799999833106995\n",
      "In epoch 300, loss 0.5562995672225952, vall_acc: 0.5820000171661377, best acc: 0.5839999914169312, test acc: 0.5799999833106995, best test acc: 0.5799999833106995\n",
      "In epoch 305, loss 0.5562897324562073, vall_acc: 0.5820000171661377, best acc: 0.5839999914169312, test acc: 0.5789999961853027, best test acc: 0.5799999833106995\n",
      "In epoch 310, loss 0.5562837719917297, vall_acc: 0.5820000171661377, best acc: 0.5839999914169312, test acc: 0.5799999833106995, best test acc: 0.5799999833106995\n",
      "In epoch 315, loss 0.5562791228294373, vall_acc: 0.5799999833106995, best acc: 0.5839999914169312, test acc: 0.5770000219345093, best test acc: 0.5799999833106995\n",
      "In epoch 320, loss 0.5562877058982849, vall_acc: 0.5759999752044678, best acc: 0.5839999914169312, test acc: 0.5740000009536743, best test acc: 0.5799999833106995\n",
      "In epoch 325, loss 0.5570761561393738, vall_acc: 0.5759999752044678, best acc: 0.5839999914169312, test acc: 0.578000009059906, best test acc: 0.5799999833106995\n",
      "In epoch 330, loss 0.5564272999763489, vall_acc: 0.5699999928474426, best acc: 0.5839999914169312, test acc: 0.5820000171661377, best test acc: 0.5799999833106995\n",
      "In epoch 335, loss 0.5564931631088257, vall_acc: 0.5699999928474426, best acc: 0.5839999914169312, test acc: 0.5860000252723694, best test acc: 0.5799999833106995\n",
      "In epoch 340, loss 0.5564970374107361, vall_acc: 0.5699999928474426, best acc: 0.5839999914169312, test acc: 0.5879999995231628, best test acc: 0.5799999833106995\n",
      "In epoch 345, loss 0.5564205646514893, vall_acc: 0.5699999928474426, best acc: 0.5839999914169312, test acc: 0.5820000171661377, best test acc: 0.5799999833106995\n",
      "In epoch 350, loss 0.5563597083091736, vall_acc: 0.5699999928474426, best acc: 0.5839999914169312, test acc: 0.5789999961853027, best test acc: 0.5799999833106995\n",
      "In epoch 355, loss 0.5563271641731262, vall_acc: 0.5699999928474426, best acc: 0.5839999914169312, test acc: 0.5799999833106995, best test acc: 0.5799999833106995\n",
      "In epoch 360, loss 0.5563055872917175, vall_acc: 0.5740000009536743, best acc: 0.5839999914169312, test acc: 0.5789999961853027, best test acc: 0.5799999833106995\n",
      "In epoch 365, loss 0.5562859773635864, vall_acc: 0.5759999752044678, best acc: 0.5839999914169312, test acc: 0.5770000219345093, best test acc: 0.5799999833106995\n",
      "In epoch 370, loss 0.5562678575515747, vall_acc: 0.5759999752044678, best acc: 0.5839999914169312, test acc: 0.578000009059906, best test acc: 0.5799999833106995\n",
      "In epoch 375, loss 0.556252121925354, vall_acc: 0.5759999752044678, best acc: 0.5839999914169312, test acc: 0.5770000219345093, best test acc: 0.5799999833106995\n",
      "In epoch 380, loss 0.5562384128570557, vall_acc: 0.5759999752044678, best acc: 0.5839999914169312, test acc: 0.5770000219345093, best test acc: 0.5799999833106995\n",
      "In epoch 385, loss 0.556226372718811, vall_acc: 0.578000009059906, best acc: 0.5839999914169312, test acc: 0.5799999833106995, best test acc: 0.5799999833106995\n",
      "In epoch 390, loss 0.5562156438827515, vall_acc: 0.578000009059906, best acc: 0.5839999914169312, test acc: 0.5799999833106995, best test acc: 0.5799999833106995\n",
      "In epoch 395, loss 0.5562060475349426, vall_acc: 0.578000009059906, best acc: 0.5839999914169312, test acc: 0.5809999704360962, best test acc: 0.5799999833106995\n",
      "In epoch 400, loss 0.5561972260475159, vall_acc: 0.578000009059906, best acc: 0.5839999914169312, test acc: 0.5820000171661377, best test acc: 0.5799999833106995\n",
      "In epoch 405, loss 0.5561890602111816, vall_acc: 0.5799999833106995, best acc: 0.5839999914169312, test acc: 0.5820000171661377, best test acc: 0.5799999833106995\n",
      "In epoch 410, loss 0.5561814308166504, vall_acc: 0.5799999833106995, best acc: 0.5839999914169312, test acc: 0.5830000042915344, best test acc: 0.5799999833106995\n",
      "In epoch 415, loss 0.5561743378639221, vall_acc: 0.5799999833106995, best acc: 0.5839999914169312, test acc: 0.5839999914169312, best test acc: 0.5799999833106995\n",
      "In epoch 420, loss 0.5561677813529968, vall_acc: 0.578000009059906, best acc: 0.5839999914169312, test acc: 0.5839999914169312, best test acc: 0.5799999833106995\n",
      "In epoch 425, loss 0.5561615228652954, vall_acc: 0.578000009059906, best acc: 0.5839999914169312, test acc: 0.5839999914169312, best test acc: 0.5799999833106995\n",
      "In epoch 430, loss 0.5561556816101074, vall_acc: 0.578000009059906, best acc: 0.5839999914169312, test acc: 0.5839999914169312, best test acc: 0.5799999833106995\n",
      "In epoch 435, loss 0.5561501383781433, vall_acc: 0.578000009059906, best acc: 0.5839999914169312, test acc: 0.5839999914169312, best test acc: 0.5799999833106995\n",
      "In epoch 440, loss 0.5561449527740479, vall_acc: 0.578000009059906, best acc: 0.5839999914169312, test acc: 0.5839999914169312, best test acc: 0.5799999833106995\n",
      "In epoch 445, loss 0.5561400055885315, vall_acc: 0.5759999752044678, best acc: 0.5839999914169312, test acc: 0.5839999914169312, best test acc: 0.5799999833106995\n",
      "In epoch 450, loss 0.556135356426239, vall_acc: 0.5759999752044678, best acc: 0.5839999914169312, test acc: 0.5839999914169312, best test acc: 0.5799999833106995\n",
      "In epoch 455, loss 0.5561308860778809, vall_acc: 0.5759999752044678, best acc: 0.5839999914169312, test acc: 0.5839999914169312, best test acc: 0.5799999833106995\n",
      "In epoch 460, loss 0.5561304688453674, vall_acc: 0.578000009059906, best acc: 0.5839999914169312, test acc: 0.578000009059906, best test acc: 0.5799999833106995\n",
      "In epoch 465, loss 0.556143581867218, vall_acc: 0.5759999752044678, best acc: 0.5839999914169312, test acc: 0.5709999799728394, best test acc: 0.5799999833106995\n",
      "In epoch 470, loss 0.5561481714248657, vall_acc: 0.5720000267028809, best acc: 0.5839999914169312, test acc: 0.5680000185966492, best test acc: 0.5799999833106995\n",
      "In epoch 475, loss 0.5561408996582031, vall_acc: 0.5740000009536743, best acc: 0.5839999914169312, test acc: 0.5709999799728394, best test acc: 0.5799999833106995\n",
      "In epoch 480, loss 0.5561302304267883, vall_acc: 0.578000009059906, best acc: 0.5839999914169312, test acc: 0.5720000267028809, best test acc: 0.5799999833106995\n",
      "In epoch 485, loss 0.5561214685440063, vall_acc: 0.5799999833106995, best acc: 0.5839999914169312, test acc: 0.5720000267028809, best test acc: 0.5799999833106995\n",
      "In epoch 490, loss 0.5561144351959229, vall_acc: 0.578000009059906, best acc: 0.5839999914169312, test acc: 0.5730000138282776, best test acc: 0.5799999833106995\n",
      "In epoch 495, loss 0.5561087131500244, vall_acc: 0.5759999752044678, best acc: 0.5839999914169312, test acc: 0.5740000009536743, best test acc: 0.5799999833106995\n"
     ]
    }
   ],
   "source": [
    "def train(g, model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "\n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    val_mask = g.ndata['val_mask']\n",
    "    test_mask = g.ndata['test_mask']\n",
    "    for e in range(500):\n",
    "        # Forward\n",
    "        logits = model(g, features)\n",
    "        #print(f'Output from forward pass: {logits.shape}')\n",
    "        # Compute predictions -- i.e. gather the maximal\n",
    "        pred = logits.argmax(1)\n",
    "\n",
    "        # Compute loss -- you should compute the losses of the nodes in the training set\n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "\n",
    "        # Compute accuracy on training/validation/test\n",
    "        train_acc = ( pred[train_mask] == labels[train_mask] ).float().mean()\n",
    "        val_acc = ( pred[val_mask] == labels[val_mask] ).float().mean()\n",
    "        test_acc = ( pred[test_mask] == labels[test_mask] ).float().mean()\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            print(f'In epoch {e}, loss {loss}, vall_acc: {val_acc}, best acc: {best_val_acc}, test acc: {test_acc}, best test acc: {best_test_acc}')\n",
    "\n",
    "model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)\n",
    "train(g, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How Does DGL Represent a Graph\n",
    "import dgl\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "g = dgl.graph(([0, 0, 0, 0, 0], [1, 2, 3, 4, 5]), num_nodes=6)\n",
    "# Equivalently LongTensor also works\n",
    "g = dgl.graph((torch.LongTensor([0, 0, 0, 0, 0]), torch.LongTensor([1, 2, 3, 4, 5])), num_nodes=6)\n",
    "\n",
    "# You can omit the number of nodes argument if you can tell the number of nodes from the edge list alone\n",
    "g = dgl.graph(([0, 0, 0, 0, 0], [1, 2, 3, 4, 5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.edges()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3221, 0.6773, 0.2418, 0.0775],\n",
      "        [0.1706, 0.8468, 0.5743, 0.0085],\n",
      "        [0.2340, 0.3074, 0.9905, 0.7342],\n",
      "        [0.6691, 0.9219, 0.1042, 0.6407],\n",
      "        [0.3158, 0.3419, 0.0140, 0.7359]])\n"
     ]
    }
   ],
   "source": [
    "# Assign a 3-dimensional\n",
    "g.ndata['x'] = torch.rand(6, 3)\n",
    "# Assign a 4-dimensional edge feature vector for each edge\n",
    "g.edata['a'] = torch.rand(5, 4)\n",
    "# Assign a 5x4 node feature matrix for each node. Node and edge features in DGL can be multi-dimensional\n",
    "g.ndata['y'] = torch.rand(6, 5, 4)\n",
    "\n",
    "print(g.edata['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "5\n",
      "5\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Querying Graph Structures\n",
    "print(g.num_nodes())\n",
    "print(g.num_edges())\n",
    "# Out degrees of the center node\n",
    "print(g.out_degrees(0))\n",
    "# In degress of the center node - note that the graph is directed so the in degree should be zero\n",
    "print(g.in_degrees(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Transofrmations \n",
    "# Introduce a subgraph from node 0. node 1 and node 3 from the original graph\n",
    "sg1 = g.subgraph([0, 1, 3])\n",
    "# Introduce a sobgraph from edge, edge 1 and edge 3 from the original graph\n",
    "sg2 = g.edge_subgraph([0, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 3])\n",
      "tensor([0, 2])\n",
      "tensor([0, 1, 2, 4])\n",
      "tensor([0, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# The original IDs of each node in sg1\n",
    "print(sg1.ndata[dgl.NID])\n",
    "# The original IDs of each edge in sg1\n",
    "print(sg1.edata[dgl.EID])\n",
    "# The original IDs of each node in sg2\n",
    "print(sg2.ndata[dgl.NID])\n",
    "# The original IDs of each edge in sg2\n",
    "print(sg2.edata[dgl.EID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9325, 0.1277, 0.3511],\n",
      "        [0.7269, 0.4787, 0.5622],\n",
      "        [0.0375, 0.8109, 0.0934],\n",
      "        [0.6426, 0.2974, 0.7814]])\n"
     ]
    }
   ],
   "source": [
    "# subgraph and edge_subgraph also copies the original features to the subgraph\n",
    "# The original node feature of each node in sg1\n",
    "print(sg1.ndata['x'])\n",
    "# The original edge feature of each node in sq1\n",
    "print(sg1.edata['a'])\n",
    "# The original node feature of each node in sg2\n",
    "print(sg2.ndata['x'])\n",
    "# The original edge feature of each node in sg2\n",
    "print(sg2.edata['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0, 1, 2, 3, 4, 5]),\n",
       " tensor([1, 2, 3, 4, 5, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another common transformation is to add a reverse edge for each edge in the original graph with dgl.add_reverse_edges\n",
    "# But if you have an undirected graph (have edges that do not have a direction and indicates two-way relationship) it is better to convert it into \n",
    "# a bidirectional graph first via adding reverse edges (bidirectional might have different edge weights between two adjacent vertices depending on the direction)\n",
    "\n",
    "newg = dgl.add_reverse_edges(g)\n",
    "newg.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=6, num_edges=5,\n",
      "      ndata_schemes={'x': Scheme(shape=(3,), dtype=torch.float32), 'y': Scheme(shape=(5, 4), dtype=torch.float32)}\n",
      "      edata_schemes={'a': Scheme(shape=(4,), dtype=torch.float32)})\n"
     ]
    }
   ],
   "source": [
    "# Loading and Saving Graphs\n",
    "# You can save a graph or a list of graphs via dgl.save_graphs and load them back with dgl.load_graphs\n",
    "# Save Graphs\n",
    "dgl.save_graphs('graph.dgl', g)\n",
    "dgl.save_graphs('graphs.dgl', [g, sg1, sg2])\n",
    "\n",
    "# Load Graphs\n",
    "(g,), _ = dgl.load_graphs('graph.dgl')\n",
    "print(g)\n",
    "(g, sg1, sg2), _ = dgl.load_graphs('graphs.dgl')\n",
    "print(g)\n",
    "print(sg1)\n",
    "print(sg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Your Own GNN module\n",
    "# Sometimes your model goes beyond simply stacking existing GNN modules. E.g. you would like to invent a new way  of aggregating neighbor information by considering node importance or edge weights.\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message Passing and GNNs\n",
    "import dgl.function as fn\n",
    "\n",
    "class SAGEConv(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat):\n",
    "        super(SAGEConv, self).__init__()\n",
    "        # A linear submodule for projecting the input and neighbor feature to the output\n",
    "        self.linear = nn.Linear(in_feat*2, out_feat)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            # update_all is a mesage passing API\n",
    "            g.update_all(message_func=fn.copy_u('h', 'm'), reduce_func=fn.mean('m', 'h_N'))\n",
    "            h_N = g.ndata['h_N']\n",
    "            h_total = torch.cat([h, h_N], dim=1)\n",
    "            return self.linear(h_total)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zet4/.virtualenvs/rayrl/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 1.9515306949615479, vall acc: 0.15600000321865082, best: 0.15600000321865082, test acc: 0.14399999380111694, best: 0.14399999380111694\n",
      "In epoch 5, loss: 1.8879996538162231, vall acc: 0.17800000309944153, best: 0.17800000309944153, test acc: 0.15800000727176666, best: 0.15800000727176666\n",
      "In epoch 10, loss: 1.7564455270767212, vall acc: 0.5220000147819519, best: 0.5220000147819519, test acc: 0.49300000071525574, best: 0.49300000071525574\n",
      "In epoch 15, loss: 1.5496182441711426, vall acc: 0.5460000038146973, best: 0.550000011920929, test acc: 0.5479999780654907, best: 0.5249999761581421\n",
      "In epoch 20, loss: 1.2737606763839722, vall acc: 0.5879999995231628, best: 0.5879999995231628, test acc: 0.5789999961853027, best: 0.5789999961853027\n",
      "In epoch 25, loss: 0.9600477814674377, vall acc: 0.6340000033378601, best: 0.6340000033378601, test acc: 0.6309999823570251, best: 0.6309999823570251\n",
      "In epoch 30, loss: 0.6582165956497192, vall acc: 0.6800000071525574, best: 0.6800000071525574, test acc: 0.6899999976158142, best: 0.6899999976158142\n",
      "In epoch 35, loss: 0.4142754077911377, vall acc: 0.7300000190734863, best: 0.7300000190734863, test acc: 0.7250000238418579, best: 0.7250000238418579\n",
      "In epoch 40, loss: 0.2467499077320099, vall acc: 0.75, best: 0.75, test acc: 0.7329999804496765, best: 0.7329999804496765\n",
      "In epoch 45, loss: 0.14495906233787537, vall acc: 0.7480000257492065, best: 0.75, test acc: 0.7440000176429749, best: 0.7329999804496765\n",
      "In epoch 50, loss: 0.08706821501255035, vall acc: 0.75, best: 0.75, test acc: 0.75, best: 0.7329999804496765\n",
      "In epoch 55, loss: 0.054896045476198196, vall acc: 0.7540000081062317, best: 0.7540000081062317, test acc: 0.7540000081062317, best: 0.7540000081062317\n",
      "In epoch 60, loss: 0.03686859831213951, vall acc: 0.7459999918937683, best: 0.7540000081062317, test acc: 0.7549999952316284, best: 0.7540000081062317\n",
      "In epoch 65, loss: 0.02645057812333107, vall acc: 0.7459999918937683, best: 0.7540000081062317, test acc: 0.7490000128746033, best: 0.7540000081062317\n",
      "In epoch 70, loss: 0.020168976858258247, vall acc: 0.7419999837875366, best: 0.7540000081062317, test acc: 0.753000020980835, best: 0.7540000081062317\n",
      "In epoch 75, loss: 0.016187123954296112, vall acc: 0.7440000176429749, best: 0.7540000081062317, test acc: 0.7509999871253967, best: 0.7540000081062317\n",
      "In epoch 80, loss: 0.013520157895982265, vall acc: 0.7459999918937683, best: 0.7540000081062317, test acc: 0.746999979019165, best: 0.7540000081062317\n",
      "In epoch 85, loss: 0.011641508899629116, vall acc: 0.7459999918937683, best: 0.7540000081062317, test acc: 0.7459999918937683, best: 0.7540000081062317\n",
      "In epoch 90, loss: 0.010251828469336033, vall acc: 0.7459999918937683, best: 0.7540000081062317, test acc: 0.746999979019165, best: 0.7540000081062317\n",
      "In epoch 95, loss: 0.009182373061776161, vall acc: 0.7440000176429749, best: 0.7540000081062317, test acc: 0.7459999918937683, best: 0.7540000081062317\n"
     ]
    }
   ],
   "source": [
    "# Afterwards, you can stack your own GraphSAGE convolution layers to form a multi-layer GraphSAGE network\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats)\n",
    "        self.conv2 = SAGEConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "# Training Loop\n",
    "import dgl.data\n",
    "\n",
    "dataset = dgl.data.CoraGraphDataset()\n",
    "g = dataset[0]\n",
    "\n",
    "def train(g, model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    all_logits = []\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "\n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    val_mask = g.ndata['val_mask']\n",
    "    test_mask = g.ndata['test_mask']\n",
    "    for e in range(100):\n",
    "        # Forward\n",
    "        logits = model(g, features)\n",
    "\n",
    "        # Compute prediction\n",
    "        pred = logits.argmax(1)\n",
    "\n",
    "        #  Compute loss -- one should only compute the losses of the nodes in the trainng set, i.e. with train_mask = 1\n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "\n",
    "        # Compute accuracy on training / validation / test\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean() \n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        all_logits.append(logits.detach())\n",
    "\n",
    "        if e %  5 == 0:\n",
    "            print(f'In epoch {e}, loss: {loss}, vall acc: {val_acc}, best: {best_val_acc}, test acc: {test_acc}, best: {best_test_acc}')\n",
    "\n",
    "model = Model(g.ndata['feat'].shape[1], 16, dataset.num_classes)\n",
    "train(g, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zet4/.virtualenvs/rayrl/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 1.951414942741394, vall acc: 0.3160000145435333, best: 0.3160000145435333, test acc: 0.3190000057220459, best: 0.3190000057220459\n",
      "In epoch 5, loss: 1.8783222436904907, vall acc: 0.5680000185966492, best: 0.5680000185966492, test acc: 0.5559999942779541, best: 0.5559999942779541\n",
      "In epoch 10, loss: 1.7482794523239136, vall acc: 0.2639999985694885, best: 0.5680000185966492, test acc: 0.26100000739097595, best: 0.5559999942779541\n",
      "In epoch 15, loss: 1.5574053525924683, vall acc: 0.2980000078678131, best: 0.5680000185966492, test acc: 0.2939999997615814, best: 0.5559999942779541\n",
      "In epoch 20, loss: 1.3111858367919922, vall acc: 0.4000000059604645, best: 0.5680000185966492, test acc: 0.39100000262260437, best: 0.5559999942779541\n",
      "In epoch 25, loss: 1.0292094945907593, vall acc: 0.5199999809265137, best: 0.5680000185966492, test acc: 0.492000013589859, best: 0.5559999942779541\n",
      "In epoch 30, loss: 0.7458023428916931, vall acc: 0.5820000171661377, best: 0.5820000171661377, test acc: 0.5830000042915344, best: 0.5830000042915344\n",
      "In epoch 35, loss: 0.4989318251609802, vall acc: 0.6359999775886536, best: 0.6359999775886536, test acc: 0.656000018119812, best: 0.656000018119812\n",
      "In epoch 40, loss: 0.31351038813591003, vall acc: 0.6740000247955322, best: 0.6740000247955322, test acc: 0.703000009059906, best: 0.703000009059906\n",
      "In epoch 45, loss: 0.1910027116537094, vall acc: 0.6919999718666077, best: 0.6919999718666077, test acc: 0.7300000190734863, best: 0.7300000190734863\n",
      "In epoch 50, loss: 0.11673200130462646, vall acc: 0.6980000138282776, best: 0.699999988079071, test acc: 0.7329999804496765, best: 0.7319999933242798\n",
      "In epoch 55, loss: 0.07379880547523499, vall acc: 0.7059999704360962, best: 0.7059999704360962, test acc: 0.7279999852180481, best: 0.7279999852180481\n",
      "In epoch 60, loss: 0.04922422021627426, vall acc: 0.7099999785423279, best: 0.7120000123977661, test acc: 0.7310000061988831, best: 0.7289999723434448\n",
      "In epoch 65, loss: 0.034873202443122864, vall acc: 0.7139999866485596, best: 0.7139999866485596, test acc: 0.7329999804496765, best: 0.7310000061988831\n",
      "In epoch 70, loss: 0.026171356439590454, vall acc: 0.7179999947547913, best: 0.7179999947547913, test acc: 0.734000027179718, best: 0.7319999933242798\n",
      "In epoch 75, loss: 0.020645633339881897, vall acc: 0.7160000205039978, best: 0.7179999947547913, test acc: 0.7379999756813049, best: 0.7319999933242798\n",
      "In epoch 80, loss: 0.016964592039585114, vall acc: 0.7200000286102295, best: 0.7200000286102295, test acc: 0.7390000224113464, best: 0.7390000224113464\n",
      "In epoch 85, loss: 0.014396769925951958, vall acc: 0.7200000286102295, best: 0.7200000286102295, test acc: 0.7429999709129333, best: 0.7390000224113464\n",
      "In epoch 90, loss: 0.012521225027740002, vall acc: 0.7200000286102295, best: 0.7200000286102295, test acc: 0.7450000047683716, best: 0.7390000224113464\n",
      "In epoch 95, loss: 0.011091056279838085, vall acc: 0.7200000286102295, best: 0.7200000286102295, test acc: 0.7450000047683716, best: 0.7390000224113464\n"
     ]
    }
   ],
   "source": [
    "# More Customization: DGL has got many built-in message and reduce functions under the dgl.function package\n",
    "# https://docs.dgl.ai/api/python/dgl.function.html#apifunction\n",
    "# E.g. Add Edge Weights\n",
    "class WeightedSAGEConv(nn.Module):\n",
    "    def __init__(self,in_feat, out_feat):\n",
    "        super(WeightedSAGEConv, self).__init__()\n",
    "        # a linear submodule for projecting the input and neighbor feature to rhe output\n",
    "        self.linear = nn.Linear(in_feat * 2, out_feat)\n",
    "\n",
    "    def forward(self, g, h, w):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            g.edata['w'] = w\n",
    "            g.update_all(message_func=fn.u_mul_e('h', 'w', 'm'), reduce_func=fn.mean('m', 'h_N'))\n",
    "            h_N = g.ndata['h_N']\n",
    "            h_total = torch.cat([h, h_N], dim=1)\n",
    "            return self.linear(h_total)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = WeightedSAGEConv(in_feats, h_feats)\n",
    "        self.conv2 = WeightedSAGEConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat, torch.ones(g.num_edges(), 1).to(g.device))\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h, torch.ones(g.num_edges(), 1).to(g.device))\n",
    "        return h\n",
    "\n",
    "model = Model(g.ndata['feat'].shape[1], 16, dataset.num_classes)\n",
    "train(g, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even More Customization by user-defined Function\n",
    "# DGL allows user-defined message and reduce functio for the maximal expresivness. Here is a user-defined message funcion that is equivalent to fn.u_mul_e('h', 'w', 'm')\n",
    "def u_mul_e_udf(edges):\n",
    "    return {'m': edges.src['h'] * edges.data['w']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link Prediction using GNN, i.e. predicting the existence of an edge between two arbitrary nodes in a graph\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "# Many applications such as soical recommendation, item recommendation, knowledge graph competion can be formulated as link prediction, which predicts\n",
    "# whether an edge exists between two particular nodes.\n",
    "# Example: prediciton whether a citation raltionship, either citing or being cited, between two papers exists in a citation network\n",
    "#    - binary classification\n",
    "#    - treat the edgess in the graph as positive examples\n",
    "#    - sample a number of non-existing edges (i.e. node pairs with no edges between them) as negative examples\n",
    "#    - divide the positive examples and negative examples into a training set and a test set\n",
    "#    - evaluate the model with any binary classification metric such as AUC\n",
    "\n",
    "import dgl.data\n",
    "\n",
    "dataset = dgl.data.CoraGraphDataset()\n",
    "g = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split edge set for training and testing\n",
    "u,v = g.edges()\n",
    "\n",
    "eids = np.arange(g.number_of_edges())\n",
    "eids = np.random.permutation(eids)\n",
    "test_size = int(len(eids) * 0.1) # 10 % for testing purposes\n",
    "train_size = g.number_of_edges() - test_size\n",
    "test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
    "train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]\n",
    "\n",
    "# Find all negative edges and split them fo training and testing\n",
    "adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\n",
    "adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\n",
    "neg_u, neg_v = np.where(adj_neg != 0)\n",
    "\n",
    "neg_eids = np.random.choice(len(neg_u), g.number_of_edges())\n",
    "test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]\n",
    "train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove edges in the test set from the original graph\n",
    "train_g = dgl.remove_edges(g, eids[:test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a GrapSAGE Model\n",
    "from dgl.nn import SAGEConv\n",
    "\n",
    "#----------------2. create model -----------------------------#\n",
    "# build a two-layer GraphSAGE model\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, 'mean')\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, the model predicts probability of existence of an edge by computing a score between the representations of both incident nodes with a function (e.g. an MLP or a dot product)\n",
    "\n",
    "# Positive graph, negative graph\n",
    "train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())\n",
    "train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes()) \n",
    "\n",
    "test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n",
    "test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "\n",
    "class DotPredictor(nn.Module):\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            # Compute a new edge feature names 'score' by a dot-product between the source node feature 'h' and destination node feature 'h'\n",
    "            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "            # u_dot_v returns a 1-element vector for each edge so you need to squeeze it\n",
    "            return g.edata['score'][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One can also write own function if it is complex, e.g. the following module produces a scalar score on each edge by concatenating the incident nodes features and passing it to MLP\n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, h_feats):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(h_feats * 2, h_feats)\n",
    "        self.W2 = nn.Linera(h_feats, 1) # output is a scalar\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        '''\n",
    "        Computes a scalar score for each edge of the given graph.\n",
    "        Parameters\n",
    "        ----------\n",
    "        edges:\n",
    "            Has three members ''src'', ''dst'' and ''data'', each of which is a dictionary representing the features of the source nodes, the destination nodes and the edges themselves.\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary of new edge features\n",
    "        '''\n",
    "        h = torch.cat([edges.src['h'], edges.dst['h']], 1)\n",
    "        return {'score': self.W2(F.relu(self.W1(h))).squeeze(1)}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            g.apply_edges(self.apply_edges)\n",
    "            return g.edata['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "# loss function: binary cross-entropy\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model = GraphSAGE(train_g.ndata['feat'].shape[1], 16)\n",
    "# One can replace DotPredictor with MLPPredictor\n",
    "# pred = MLPPredictor(16)\n",
    "pred = DotPredictor()\n",
    "\n",
    "def compute_loss(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score])\n",
    "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
    "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "def compute_auc(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score]).numpy()\n",
    "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
    "    return roc_auc_score(labels, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zet4/.virtualenvs/rayrl/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 0.6929958462715149\n",
      "In epoch 5, loss: 0.664452314376831\n",
      "In epoch 10, loss: 0.5835233330726624\n",
      "In epoch 15, loss: 0.5399207472801208\n",
      "In epoch 20, loss: 0.5070629119873047\n",
      "In epoch 25, loss: 0.48428264260292053\n",
      "In epoch 30, loss: 0.4538520574569702\n",
      "In epoch 35, loss: 0.4277080297470093\n",
      "In epoch 40, loss: 0.4002706706523895\n",
      "In epoch 45, loss: 0.37552323937416077\n",
      "In epoch 50, loss: 0.35206684470176697\n",
      "In epoch 55, loss: 0.32759401202201843\n",
      "In epoch 60, loss: 0.30359822511672974\n",
      "In epoch 65, loss: 0.27989843487739563\n",
      "In epoch 70, loss: 0.25594303011894226\n",
      "In epoch 75, loss: 0.23273035883903503\n",
      "In epoch 80, loss: 0.20979833602905273\n",
      "In epoch 85, loss: 0.18734630942344666\n",
      "In epoch 90, loss: 0.16602729260921478\n",
      "In epoch 95, loss: 0.145292729139328\n",
      "AUC 0.8608620650928774\n"
     ]
    }
   ],
   "source": [
    "# -------------- 3. set up loss and optimizer -------------------#\n",
    "# in this case, loss will in training loop\n",
    "optimizer = torch.optim.Adam(itertools.chain(model.parameters(), pred.parameters()), lr=0.01)\n",
    "\n",
    "# -------------- 4. training ---------------------------------- #\n",
    "all_logits = []\n",
    "for e in range(100):\n",
    "    # forward pass\n",
    "    h = model(train_g, train_g.ndata['feat'])\n",
    "    pos_score = pred(train_pos_g, h)\n",
    "    neg_score = pred(train_neg_g, h)\n",
    "    loss = compute_loss(pos_score, neg_score)\n",
    "\n",
    "    # backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 5 == 0:\n",
    "        print(f'In epoch {e}, loss: {loss}')\n",
    "\n",
    "# -------------5. check the results -------------------------- #\n",
    "with torch.no_grad():\n",
    "    pos_score = pred(test_pos_g, h)\n",
    "    neg_score = pred(test_neg_g, h)\n",
    "    print('AUC', compute_auc(pos_score, neg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('rayrl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74e0a97186b63debfb3936693e25014f03e3d231f3563a137539dc61528fa20b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
